{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try it with the Actor outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm as tqdm\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "import random\n",
    "import matplotlib\n",
    "from environment import Environment\n",
    "from plots import just_plot\n",
    "# from plots import *\n",
    "from nets import *\n",
    "from buffer import ReplayBuffer\n",
    "from misc import *\n",
    "import numpy as np\n",
    "\n",
    "dolinar_layers=2\n",
    "amplitude = 0.4\n",
    "number_phases=2\n",
    "total_episodes = 10**3\n",
    "buffer_size=500\n",
    "batch_size=64\n",
    "ep_guess=0.01\n",
    "noise_displacement=0.5\n",
    "lr_actor=0.01\n",
    "lr_critic=0.001\n",
    "tau=0.005\n",
    "\n",
    "\n",
    "\n",
    "class PolicyEvaluator(Basics):\n",
    "    def __init__(self, **kwargs):\n",
    "        amplitude= kwargs.get(\"amplitude\", .4)\n",
    "        dolinar_layers=kwargs.get(\"dolinar_layers\", 2)\n",
    "        number_phases=kwargs.get(\"number_phases\", 2)\n",
    "        super().__init__(amplitude=amplitude, dolinar_layers=dolinar_layers, number_phases=number_phases)\n",
    "        \n",
    "        displacement_tree = {}\n",
    "        #self.at = make_attenuations(self.number_layers)\n",
    "        for layer in range(self.dolinar_layers+1):\n",
    "            displacement_tree[str(layer)] = {}\n",
    "\n",
    "        for k in outcomes_universe(self.dolinar_layers):\n",
    "            for layer in range(self.dolinar_layers+1):\n",
    "                displacement_tree[str(layer)][str(k[:layer])] = 0\n",
    "                \n",
    "        self.displacement_tree = displacement_tree\n",
    "        \n",
    "    def random_tree(self):\n",
    "        actions = self.displacement_tree.copy()\n",
    "        for k in outcomes_universe(self.dolinar_layers):\n",
    "            for layer in range(self.dolinar_layers+1):\n",
    "                actions[str(layer)][str(k[:layer])] = np.random.random()\n",
    "        return actions\n",
    "    \n",
    "    def success_probability(self, displacements_tree):\n",
    "        \"\"\"\n",
    "        Given a tree of conditional actions (on the outcomes history), computes\n",
    "        the success probability. Notice the final action is the guess\n",
    "        for the phase of the state given a given branch.\n",
    "        \"\"\"\n",
    "        p=0\n",
    "        for ot in outcomes_universe(self.dolinar_layers):\n",
    "            c=1\n",
    "            for layer in range(self.dolinar_layers):\n",
    "                eff_at = np.prod(np.sin(self.at[:layer]))*np.cos(self.at[layer])\n",
    "                c*=P(displacements_tree[str(self.dolinar_layers)][str(ot)]*self.amplitude,\n",
    "                     displacements_tree[str(layer)][str(ot[:(layer)])], eff_at, ot[self.dolinar_layers-1-layer] ) #notice i respect that the columns of the outcomes_universe\n",
    "                #correspond to the layer: the last column is the first layer.\n",
    "            p += c\n",
    "        return p/self.number_phases\n",
    "    \n",
    "    \n",
    "    def greedy_strategy(self, actor, critic):\n",
    "        \"\"\"Assuming actor, critic and self have the same dolinar_layers.\n",
    "            self.possible_phases are the possible phases of the coherent states\n",
    "\n",
    "        \"\"\"\n",
    "        rr = np.ones((2**(self.dolinar_layers-1), self.dolinar_layers, 1))*actor.pad_value\n",
    "        rr[:,1:] = np.reshape(outcomes_universe(self.dolinar_layers-1),(2**(self.dolinar_layers-1), self.dolinar_layers-1,1))\n",
    "        preds = np.squeeze(actor(rr))\n",
    "        \n",
    "        for ot, seqot in zip(outcomes_universe(self.dolinar_layers-1), preds):\n",
    "            for layer in range(self.dolinar_layers):\n",
    "                self.displacement_tree[str(layer)][str(ot[:layer])] = seqot[layer]\n",
    "\n",
    "            history = []\n",
    "            index_seqot, index_ot= 0, 0\n",
    "            for index_history in range(2*self.dolinar_layers-1):\n",
    "                if index_history%2==0:\n",
    "                    history.append(seqot[index_seqot])\n",
    "                    index_seqot+=1\n",
    "                else:\n",
    "                    history.append(ot[index_ot])\n",
    "                    index_ot+=1   \n",
    "            for final_outcome in [0,1]:\n",
    "                final_history = np.append(history, final_outcome)\n",
    "                self.displacement_tree[str(self.dolinar_layers)][str(np.append(ot,final_outcome))] = self.possible_phases[critic.give_favourite_guess(final_history)[0]]\n",
    "\n",
    "        return self.success_probability(self.displacement_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PolicyEvaluator(Basics):\n",
    "    def __init__(self, **kwargs):\n",
    "        amplitude= kwargs.get(\"amplitude\", .4)\n",
    "        dolinar_layers=kwargs.get(\"dolinar_layers\", 2)\n",
    "        number_phases=kwargs.get(\"number_phases\", 2)\n",
    "        super().__init__(amplitude=amplitude, dolinar_layers=dolinar_layers, number_phases=number_phases)\n",
    "        \n",
    "        displacement_tree = {}\n",
    "        #self.at = make_attenuations(self.number_layers)\n",
    "        for layer in range(self.dolinar_layers+1):\n",
    "            displacement_tree[str(layer)] = {}\n",
    "\n",
    "        for k in outcomes_universe(self.dolinar_layers):\n",
    "            for layer in range(self.dolinar_layers+1):\n",
    "                displacement_tree[str(layer)][str(k[:layer])] = 0\n",
    "                \n",
    "        self.displacement_tree = displacement_tree\n",
    "        \n",
    "    def random_tree(self):\n",
    "        actions = self.displacement_tree.copy()\n",
    "        for k in outcomes_universe(self.dolinar_layers):\n",
    "            for layer in range(self.dolinar_layers+1):\n",
    "                actions[str(layer)][str(k[:layer])] = np.random.random()\n",
    "        return actions\n",
    "    \n",
    "    def success_probability(self, displacements_tree):\n",
    "        \"\"\"\n",
    "        Given a tree of conditional actions (on the outcomes history), computes\n",
    "        the success probability. Notice the final action is the guess\n",
    "        for the phase of the state given a given branch.\n",
    "        \"\"\"\n",
    "        p=0\n",
    "        for ot in outcomes_universe(self.dolinar_layers):\n",
    "            c=1\n",
    "            for layer in range(self.dolinar_layers):\n",
    "                eff_at = np.prod(np.sin(self.at[:layer]))*np.cos(self.at[layer])\n",
    "                c*=P(displacements_tree[str(self.dolinar_layers)][str(ot)]*self.amplitude,\n",
    "                     displacements_tree[str(layer)][str(ot[:(layer)])], eff_at, ot[self.dolinar_layers-1-layer] ) #notice i respect that the columns of the outcomes_universe\n",
    "                #correspond to the layer: the last column is the first layer.\n",
    "            p += c\n",
    "        return p/self.number_phases\n",
    "    \n",
    "    \n",
    "    def greedy_strategy(self, actor, critic):\n",
    "        \"\"\"Assuming actor, critic and self have the same dolinar_layers.\n",
    "            self.possible_phases are the possible phases of the coherent states\n",
    "\n",
    "        \"\"\"\n",
    "        rr = np.ones((2**(self.dolinar_layers-1), self.dolinar_layers, 1))*actor.pad_value\n",
    "        rr[:,1:] = np.reshape(outcomes_universe(self.dolinar_layers-1),(2**(self.dolinar_layers-1), self.dolinar_layers-1,1))\n",
    "        preds = np.squeeze(actor(rr))\n",
    "        \n",
    "        for ot, seqot in zip(outcomes_universe(self.dolinar_layers-1), preds):\n",
    "            for layer in range(self.dolinar_layers):\n",
    "                self.displacement_tree[str(layer)][str(ot[:layer])] = seqot[layer]\n",
    "\n",
    "            history = []\n",
    "            index_seqot, index_ot= 0, 0\n",
    "            for index_history in range(2*self.dolinar_layers-1):\n",
    "                if index_history%2==0:\n",
    "                    history.append(seqot[index_seqot])\n",
    "                    index_seqot+=1\n",
    "                else:\n",
    "                    history.append(ot[index_ot])\n",
    "                    index_ot+=1   \n",
    "            for final_outcome in [0,1]:\n",
    "                final_history = np.append(history, final_outcome)\n",
    "                self.displacement_tree[str(self.dolinar_layers)][str(np.append(ot,final_outcome))] = self.possible_phases[critic.give_favourite_guess(final_history)[0]]\n",
    "\n",
    "        return self.success_probability(self.displacement_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcomes_universe(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dolinar_layers=2\n",
    "\n",
    "policy_evaluator = PolicyEvaluator(amplitude = amplitude, dolinar_layers=dolinar_layers, number_phases = number_phases)\n",
    "env = Environment(amplitude=0.4, dolinar_layers = dolinar_layers)\n",
    "buffer = ReplayBuffer(buffer_size=buffer_size)\n",
    "\n",
    "critic = Critic(nature=\"primary\",valreg=0.01, dolinar_layers = dolinar_layers, number_phases=number_phases)\n",
    "critic_target = Critic(nature=\"target\", dolinar_layers = dolinar_layers, number_phases=number_phases)\n",
    "actor = Actor(nature=\"primary\", dolinar_layers = dolinar_layers)\n",
    "actor_target = Actor(nature=\"target\", dolinar_layers = dolinar_layers)\n",
    "\n",
    "\n",
    "rt = []\n",
    "pt = []\n",
    "new_loss=0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer actor is casting an input tensor from dtype float32 to the layer's dtype of float64, which is new behavior in TensorFlow 2.  The layer has dtype float64 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float64, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float32 by default, call `tf.keras.backend.set_floatx('float32')`. To change just this layer, pass dtype='float32' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  5.67it/s]\n"
     ]
    }
   ],
   "source": [
    "for episode in tqdm(range(10)):\n",
    "\n",
    "    env.pick_phase()\n",
    "    experiences=[] #where the current history of the current episode is stored\n",
    "    context_outcome_actor = np.reshape(np.array([actor.pad_value]),(1,1,1)).astype(np.float32)\n",
    "    for layer in range(actor.dolinar_layers):\n",
    "        beta_would_do = np.squeeze(actor(context_outcome_actor))\n",
    "        beta =  beta_would_do + np.random.uniform(-noise_displacement, noise_displacement)\n",
    "        outcome = env.give_outcome(beta,layer)\n",
    "        experiences.append(beta)\n",
    "        experiences.append(outcome)\n",
    "\n",
    "        context_outcome_actor = np.reshape(np.array([outcome]),(1,1,1)).astype(np.float32)\n",
    "\n",
    "    ### ep-gredy guessing of the phase###\n",
    "    if np.random.random()< ep_guess:\n",
    "        val = np.random.choice(range(number_phases),1)[0]\n",
    "        guess_index, guess_input_network = val, val/critic.number_phases\n",
    "    else:\n",
    "        guess_index, guess_input_network = critic.give_favourite_guess(experiences) #experiences is the branch of the current tree of actions + outcomes.\n",
    "    experiences.append(guess_input_network)\n",
    "\n",
    "    reward = env.give_reward(guess_input_network)\n",
    "    experiences.append(reward)\n",
    "    buffer.add(tuple(experiences))\n",
    "    \n",
    "    actor.lstm.stateful=False\n",
    "    pt.append(policy_evaluator.greedy_strategy(actor = actor, critic = critic))\n",
    "    actor.lstm.stateful=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to put some tree which is more or less meaningful..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': {'[]': 0.033168748},\n",
       " '1': {'[0]': 0.033168748, '[1]': 0.033349838},\n",
       " '2': {'[0 0]': (-1+0j), '[0 1]': (1+0j), '[1 0]': (-1+0j), '[1 1]': (1+0j)}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_evaluator.displacement_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_evaluator.displacement_tree = {\"0\":{\"[]\":-.7}, \"1\":{\"[0]\": -.6, \"[1]\": .6}, \"2\":{\"[0 0]\": 1, \"[0 1]\":1, \"[1 0]\":1, \"[1 1]\":1}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': {'[]': -0.7},\n",
       " '1': {'[0]': -0.6, '[1]': 0.6},\n",
       " '2': {'[0 0]': 1, '[0 1]': 1, '[1 0]': 1, '[1 1]': 1}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_evaluator.displacement_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7228144406306147"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_evaluator.success_probability(policy_evaluator.displacement_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_evaluator.dolinar_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_strategy(self, actor, critic):\n",
    "    \"\"\"Assuming actor, critic and self have the same dolinar_layers.\n",
    "        self.possible_phases are the possible phases of the coherent states\n",
    "    \n",
    "    \"\"\"\n",
    "    rr = np.ones((2**(self.dolinar_layers-1), self.dolinar_layers, 1))*actor.pad_value\n",
    "    rr[:,1:] = np.reshape(outcomes_universe(actor.dolinar_layers-1),(2**(self.dolinar_layers-1), self.dolinar_layers-1,1))\n",
    "    preds = np.squeeze(actor(rr))\n",
    "\n",
    "    for ot, seqot in zip(outcomes_universe(actor.dolinar_layers-1), preds):\n",
    "        for layer in range(actor.dolinar_layers):\n",
    "            self.displacement_tree[str(layer)][str(ot[:layer])] = seqot[layer]\n",
    "\n",
    "        history = []\n",
    "        index_seqot, index_ot= 0, 0\n",
    "        for index_history in range(2*actor.dolinar_layers-1):\n",
    "            if index_history%2==0:\n",
    "                history.append(seqot[index_seqot])\n",
    "                index_seqot+=1\n",
    "            else:\n",
    "                history.append(ot[index_ot])\n",
    "                index_ot+=1   \n",
    "        for final_outcome in [0,1]:\n",
    "            final_history = np.append(history, final_outcome)\n",
    "            self.displacement_tree[str(actor.dolinar_layers)][str(np.append(ot,final_outcome))] = self.possible_phases[critic.give_favourite_guess(final_history)[0]]\n",
    "            \n",
    "    return self.success_probability(displacement_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = Actions(number_layers=3, number_phases=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions.success_probability(displacement_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic.dolinar_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_phases = env.possible_phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
