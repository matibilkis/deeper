import tensorflow as tf
import pandas as pd
from tensorflow.keras.layers import Dense
import numpy as np
import matplotlib.pyplot as plt
import os
from tqdm import tqdm as tqdm
tf.keras.backend.set_floatx('float64')
from collections import deque
from datetime import datetime
import random
import matplotlib
import cmath





def P(a,b,et,n):

    p0=np.exp(-abs((et*a)+b)**2)

    if n ==0:
        return p0
    else:
        return 1-(p0)

def outcomes_universe(L):
    """
    Takes L (# of photodetections in the experiment) and returns
    all possible outcomes in a matrix of 2**L rows by L columns,
    which are all possible sequence of outcomes you can ever get.
    """
    a = np.array([0,1])
    two_outcomes = np.array([[0,0],[0,1],[1,0],[1,1]]).astype(int)
    if L<2:
        return np.array([0,1]).astype(int)
    elif L==2:
        return two_outcomes
    else:
        x = insert(a,two_outcomes)
        for i in range(L-3):
            x = insert(a,x)
        return x.astype(int)

def make_attenuations(layers):
    if layers == 1:
        return [0]
    else:
        ats=[0]
        for i in range(layers-1):
            ats.append(np.arctan(1/np.cos(ats[i])))
        return np.flip(ats)


def prob_2L(actions_tree, at): #
    #at = make_attenuations(2)
    p=0
    for ot in outcomes_universe(2):
        p += P(actions_tree["2"][str(ot[:2])]*0.4, actions_tree["0"]["[]"], np.sin(at[0]), ot[0])*P(actions_tree["2"][str(ot[:2])]*0.4,
                                                                                                    actions_tree["1"][str(ot[:1])], np.cos(at[0]), ot[1])
    return p/2

def record():
    if not os.path.exists("results/number_rune.txt"):
        with open("results/number_rune.txt", "w+") as f:
            f.write("0")
            f.close()
        a=0
        number_run=0
    else:
        with open("results/number_rune.txt", "r") as f:
            a = f.readlines()[0]
            f.close()
        with open("results/number_rune.txt", "w") as f:
            f.truncate(0)
            f.write(str(int(a)+1))
            f.close()
        number_run = int(a)+1
    if not os.path.exists("run_"+str(number_run)):
        os.makedirs("results/run_"+str(number_run))
        for net in ["actor_primary", "actor_target", "critic_primary", "critic_target"]:
            os.makedirs("results/run_"+str(number_run)+"/networks/"+net)

    #
    # if not os.path.exists("results/run_"+str(number_run)+"/models"):
    #     os.makedirs("results/run_"+str(number_run)+"/models")
    return number_run

class Complex(complex):
    def __repr__(self):
        rp = '%7.5f' % self.real if not self.pureImag() else ''
        ip = '%7.5fj' % self.imag if not self.pureReal() else ''
        conj = '' if (
            self.pureImag() or self.pureReal() or self.imag < 0.0
        ) else '+'
        return '0.0' if (
            self.pureImag() and self.pureReal()
        ) else rp + conj + ip

    def pureImag(self):
        return abs(self.real) < 0.000005

    def pureReal(self):
        return abs(self.imag) < 0.000005


def croots(n):
    if n <= 0:
        return None
    return (Complex(cmath.rect(1, 2 * k * cmath.pi / n)) for k in range(n))





# ### this is just p(R=1 | g, n; beta) = p((-1^{g} alpha | n)) = p(n|allpha) pr(alpha)(p(n))
# def qval(beta, n, guess):
#     #dolinar guessing rule (= max-likelihood for L=1, careful sign of \beta)
#     alpha = 0.4
#     pn = np.sum([Prob(g*alpha, beta, n) for g in [-1,1]])
#     return Prob(guess*alpha, beta, n)/pn
#
# def ps_maxlik(beta):
#     #dolinar guessing rule (= max-likelihood for L=1, careful sign of \beta)
#     alpha = 0.4
#     p=0
#     for n1 in [0,1]:
#        p+=Prob(np.sign(beta)*(-1)**(n1)*alpha, beta, n1)
#     return p/2


#
# # def make_figure(run_id,lr, Nbatch, loss_train, loss_test, predictions,
# #  epochs, betas_train, shuffle="False"):
# #     name_title = "Learning rate: "+ str(lr)+ "\nNumber of batches: "+str(Nbatch)
# #     name="lr"+str(lr)+"_"+"nb"+str(Nbatch)
# #     if shuffle == "True":
# #         name = name+"_shuffled"
# #     plt.figure(figsize=(10,10))
# #     ax1=plt.subplot2grid((2,1),(0,0))
# #     ax2=plt.subplot2grid((2,1),(1,0))
# #     plt.subplots_adjust(hspace=0.5)
# #     plt.title(name_title)
# #     ax1.plot(np.log10(range(1,epochs+1)), loss_train, color="red", label="Training loss", alpha=0.7)
# #     ax1.plot(np.log10(range(1,epochs+1)), loss_test, color="blue", label="Testing loss", alpha=0.7)
# #     ax1.set_xlabel("epoch",size=25)
# #     ax1.legend()
# #
# #     ax2.scatter(betas_test, predictions, s=20, color="red",label="Predictions",alpha=0.7)
# #     ax2.plot(betas_test, ps(betas_test), '--',color="blue",label="Testing range" ,alpha=0.7)
# #     ax2.scatter(betas_train, ps(betas_train), color="green", label="Training set",alpha=0.7 )
# #
# #     ax2.set_xlabel(r'$\beta$',size=25)
# #     ax1.set_title("Loss evolution",size=25)
# #     ax2.set_title("Success probability: prediction vs. true", size=25)
# #     ax2.legend()
# #     plt.savefig(run_id+"/"+str(name)+".png")
# #     plt.close()
# #     return
#

#
#
# def create_dataset(number_betas, reward_samples, fake_dataset=False):
#     data = []
#     betas = np.linspace(-1,1,number_betas)
#     psucs = [ps(b) for b in betas]
#     if fake_dataset==True:
#         for i in range(len(betas)):
#             for l in range(reward_samples):
#                 data.append(np.array([betas[i],psucs[i]]))
#         return np.array(data), betas
#
#     else:
#         for i in range(len(betas)):
#             p=0
#             for l in range(reward_samples):
#                 r=np.random.choice([1,0], 1, p=[psucs[i], 1-psucs[i]])[0]
#                 p+=r
#                 data.append(np.array([betas[i],r]))
#         return np.array(data), betas
#
#     for i in range(len(betas)):
#         p=0
#         for l in range(reward_samples):
#             r=np.random.choice([1,0], 1, p=[psucs[i], 1-psucs[i]])[0]
#             p+=r
#             data.append(np.array([betas[i],r]))
#     return np.array(data), betas
#
#     for i in range(len(betas)):
#         p=0
#         for l in range(reward_samples):
#             r=np.random.choice([1,0], 1, p=[psucs[i], 1-psucs[i]])[0]
#             p+=r
#             data.append(np.array([betas[i],r]))
#     return np.array(data), betas
#
#
#
#
# def Prob(alpha, beta, n1):
#     p0 = np.exp(-(alpha-beta)**2)
#     if n1 == 0:
#         return p0
#     else:
#         return 1-p0
#
#
#
# def give_outcome(alpha,beta):
#     p0=np.exp(-(alpha-beta)**2)
#     out = np.random.choice([0,1],1,p=[p0,1-p0])[0]
#     return out
#
# def ps_maxlik(beta):
#     #dolinar guessing rule (= max-likelihood for L=1, careful sign of \beta)
#     alpha = 0.4
#     p=0
#     for n1 in [0,1]:
#        p+=Prob(np.sign(beta)*(-1)**(n1)*alpha, beta, n1)
#     return p/2
#
#
# def ps(beta,g):
#     #g = [guess|0, guess|1]
#     #dolinar guessing rule (= max-likelihood for L=1, careful sign of \beta)
#     alpha = 0.4
#     p=0
#     for n1 in [0,1]:
#        p+=Prob(g[n1]*alpha, beta, n1)
#     return p/2
#
#
#
#
#
# def check_folder(name):
#     if not os.path.exists(name):
#         os.makedirs(name)
#     os.chdir(name)
#
#
#
# def greedy_action(q1, betas, ep=0):
#     if np.random.random()<ep:
#         l = np.random.choice(range(len(betas)), 1)[0]
#         return l, betas[l]
#     else:
#         qs= np.squeeze(q1(np.expand_dims(betas, axis=1)).numpy())
#         l=np.where(qs==max(qs))[0]
#         # print(qs)
#         # print(ps(betas))
#         # print("***")
#         if len(l)>1:
#             l=np.random.choice(l,1)[0]
#         else:
#             l=l[0]
#         return l, betas[l]
#
#
# def gredy_action(q2, inp):
#     qval = np.squeeze(q2(np.expand_dims(inp, axis=0)).numpy())
#     l=np.where(qs==max(qs))[0]
#     if len(l)>1:
#         l=np.random.choice(l,1)[0]
#     else:
#         l=l[0]
#     return l, [-1,1][l]
#
#
# class ReplayBufferv1:
#     def __init__(self, buffer_size=10**3):
#         self.buffer_size = buffer_size
#         self.count = 0
#         self.buffer = deque()
#
#     def add(self, a, r):
#         experience = (a,r)
#         if self.count < self.buffer_size:
#             self.buffer.append(experience)
#             self.count += 1
#         else:
#             self.buffer.popleft()
#             self.buffer.append(experience)
#
#     def size(self):
#         return self.count
#
#     def sample(self, batch_size):
#         batch = []
#         if self.count < batch_size:
#             batch = random.sample(self.buffer, self.count)
#         else:
#             batch = random.sample(self.buffer, batch_size)
#         a_batch, r_batch= list(map(np.array, list(zip(*batch))))
#         return a_batch, r_batch
#
#     def clear(self):
#         self.buffer.clear()
#         self.count = 0
#
#
#
# class ReplayBufferKennedy:
#     def __init__(self, buffer_size=10**3):
#         self.buffer_size = buffer_size
#         self.count = 0
#         self.buffer = deque()
#
#     def add(self, a, n, g, r):
#         experience = (a, n , g, r)
#         if self.count < self.buffer_size:
#             self.buffer.append(experience)
#             self.count += 1
#         else:
#             self.buffer.popleft()
#             self.buffer.append(experience)
#
#     def size(self):
#         return self.count
#
#     def sample(self, batch_size):
#         batch = []
#         if self.count < batch_size:
#             batch = random.sample(self.buffer, self.count)
#         else:
#             batch = random.sample(self.buffer, batch_size)
#         a_batch, outcome_batch, guess_batch, r_batch= list(map(np.array, list(zip(*batch))))
#         return a_batch, outcome_batch, guess_batch, r_batch
#
#     def clear(self):
#         self.buffer.clear()
#         self.count = 0
#
#
# def save_models(run_id, models):
#     for k in models:
#         k.save_weights(run_id+"/models/"+k.__str__())
#
# def plot_evolution_vKennedy1(rt, pt, optimal, betas, preds=None, loss=False,
# history_betas=False, history_would_have_done=False,run_id="algo"):
#
#     matplotlib.rc('font', serif='cm10')
#     plt.rcParams.update({'font.size': 50})
#
#     plt.figure(figsize=(40,40), dpi=100)
#     T=len(rt)
#     ax1=plt.subplot2grid((2,2),(0,0))
#     ax2=plt.subplot2grid((2,2),(1,0))
#     ax3=plt.subplot2grid((2,2),(0,1))
#     ax4=plt.subplot2grid((2,2),(1,1))
#
#     ax1.plot(np.log10(np.arange(1,T+1)),rt, color="red", linewidth=15, alpha=0.8, label=r'$R_t$')
#     ax1.plot(np.log10(np.arange(1,T+1)),optimal*np.ones(T), color="black",  linewidth=15,alpha=0.5, label="optimal")
#     ax2.plot(np.log10(np.arange(1,T+1)),pt, color="red", linewidth=15, alpha=0.8, label=r'$P_t$')
#     ax2.plot(np.log10(np.arange(1,T+1)),optimal*np.ones(T), color="black",  linewidth=15,alpha=0.5, label="optimal")
#
#     if preds!=None:
#         ax3.scatter(betas, preds, color="red", s=250, label="predictions", alpha=0.6)
#
#     ax3.scatter(betas, ps_maxlik(betas), color="blue", s=250, label="true values", alpha=0.6)
#     ax4.plot(loss, color="black",  linewidth=15,alpha=0.5, label="Loss evolution")
#
#     for ax in [ax1, ax2, ax3,ax4]:
#         ax.legend(prop={"size":30})
#     plt.savefig(run_id+"/learning_curves.png")
#     plt.close()
#
#     if history_betas!=False:
#         optimal_beta = betas[np.where(ps_maxlik(betas) == max(ps_maxlik(betas)))[0][0]]
#         plt.figure(figsize=(40,40), dpi=100)
#
#         plt.hist(history_betas,bins=100, facecolor='r', alpha=0.6, edgecolor='blue', label="done")
#         plt.hist(history_would_have_done,bins=100, facecolor='g', alpha=0.4, edgecolor='black', label="would have done")
#         plt.legend()
#         plt.text(optimal_beta, 0, "*", size=350)
#         plt.text(-optimal_beta, 0, "*", size=350)
#
#         plt.savefig(run_id+"/histograma_betas.png")
#         plt.close()
#
#
#
#         plt.figure(figsize=(40,40), dpi=100)
#         T=len(rt)
#         ax1=plt.subplot2grid((1,1),(0,0))
#
#         ax1.plot(np.arange(1, len(history_betas)+1),history_betas, color="red", linewidth=15, alpha=0.8, label="done")
#         ax1.plot(np.arange(1, len(history_betas)+1),history_would_have_done, color="green", linewidth=15, alpha=0.8, label="would have done")
#         ax1.plot(np.arange(1, len(history_betas)+1),np.ones(len(history_betas))*optimal_beta, color="black", linewidth=15, alpha=0.8, label="optimal-beta")
#         ax1.plot(np.arange(1, len(history_betas)+1),-np.ones(len(history_betas))*optimal_beta, color="black", linewidth=15, alpha=0.8, label="optimal-beta")
#
#
#         for ax in [ax1]:
#             ax.legend(prop={"size":30})
#         plt.savefig(run_id+"/evolution_actions.png")
#         plt.close()
#     return
#
# def qguess(b,n1,sign=1):
#     alpha=0.4
#     pn1 = np.sum([Prob(ph*alpha, b, n1) for ph in [-1,1]])
#     return Prob(sign*alpha, b, n1)/pn1
#
#
# def plot_evolution_vKennedy_last(rt, pt, optimal, betas, preds=None, pred_guess=None, loss=False,
# history_betas=False, history_would_have_done=False,run_id="algo"):
#
#     matplotlib.rc('font', serif='cm10')
#     plt.rcParams.update({'font.size': 50})
#
#     plt.figure(figsize=(40,40), dpi=100)
#     T=len(rt)
#     ax1=plt.subplot2grid((2,3),(0,0))
#     ax2=plt.subplot2grid((2,3),(1,0))
#     ax3=plt.subplot2grid((2,3),(0,1))
#     ax4=plt.subplot2grid((2,3),(1,1))
#     ax5=plt.subplot2grid((2,3),(0,2))
#     ax6=plt.subplot2grid((2,3),(1,2))
#
#
#     ax1.plot(np.log10(np.arange(1,T+1)),rt, color="red", linewidth=15, alpha=0.8, label=r'$R_t$')
#     ax1.plot(np.log10(np.arange(1,T+1)),optimal*np.ones(T), color="black",  linewidth=15,alpha=0.5, label="optimal")
#     ax2.plot(np.log10(np.arange(1,T+1)),pt, color="red", linewidth=15, alpha=0.8, label=r'$P_t$')
#     ax2.plot(np.log10(np.arange(1,T+1)),optimal*np.ones(T), color="black",  linewidth=15,alpha=0.5, label="optimal")
#
#     ax3.scatter(betas, preds, color="red", s=250, label="predictions", alpha=0.6)
#     ax3.scatter(betas, ps_maxlik(betas), color="blue", s=250, label="true values", alpha=0.6)
#     ax4.plot(loss, color="black",  linewidth=15,alpha=0.5, label="Loss evolution")
#
#     pred_guess_0, pred_guess_1 = pred_guess
#     ax5.plot(betas, pred_guess_0[:,0], color="red", linewidth=15, alpha=0.8, label=r'$\hat{Q}((\beta,0), +)$')
#     ax5.plot(betas, [qguess(b,0,sign=1) for b in betas], color="blue", linewidth=15, alpha=0.8, label=r'$Q((\beta,0), +)$')
#     ax5.plot(betas, pred_guess_1[:,0], color="red", linewidth=15, alpha=0.8, label=r'$\hat{Q}((\beta, 1), +)$')
#     ax5.plot(betas, [qguess(b,1,sign=1) for b in betas], color="blue", linewidth=15, alpha=0.8, label=r'$Q((\beta, 1) +)$')
#
#     ax6.plot(betas, pred_guess_0[:,1], color="red", linewidth=15, alpha=0.8, label=r'$\hat{Q}((\beta,0), -)$')
#     ax6.plot(betas, [qguess(b,0,sign=-1) for b in betas], color="blue", linewidth=15, alpha=0.8, label=r'$Q((\beta,0), -)$')
#     ax6.plot(betas, pred_guess_1[:,1], color="red", linewidth=15, alpha=0.8, label=r'$\hat{Q}((\beta, 1), -)$')
#     ax6.plot(betas, [qguess(b,1,sign=-1) for b in betas], color="blue", linewidth=15, alpha=0.8, label=r'$Q((\beta, 1) -)$')
#
#     for ax in [ax1, ax2, ax3,ax4,ax5,ax6]:
#         ax.legend(prop={"size":30})
#     plt.savefig(run_id+"/learning_curves.png")
#     plt.close()
#
#     if history_betas!=False:
#         optimal_beta = betas[np.where(ps_maxlik(betas) == max(ps_maxlik(betas)))[0][0]]
#         plt.figure(figsize=(50,50), dpi=100)
#
#         plt.hist(history_betas,bins=100, facecolor='r', alpha=0.6, edgecolor='blue', label="done")
#         plt.hist(history_would_have_done,bins=100, facecolor='g', alpha=0.4, edgecolor='black', label="would have done")
#         plt.legend()
#         plt.text(optimal_beta, 0, "*", size=350)
#         plt.text(-optimal_beta, 0, "*", size=350)
#
#         plt.savefig(run_id+"/histograma_betas.png")
#         plt.close()
#
#
#
#         plt.figure(figsize=(40,40), dpi=100)
#         T=len(rt)
#         ax1=plt.subplot2grid((1,1),(0,0))
#
#         ax1.plot(np.arange(1, len(history_betas)+1),history_betas, color="red", linewidth=15, alpha=0.8, label="done")
#         ax1.plot(np.arange(1, len(history_betas)+1),history_would_have_done, color="green", linewidth=15, alpha=0.8, label="would have done")
#         ax1.plot(np.arange(1, len(history_betas)+1),np.ones(len(history_betas))*optimal_beta, color="black", linewidth=15, alpha=0.8, label="optimal-beta")
#         ax1.plot(np.arange(1, len(history_betas)+1),-np.ones(len(history_betas))*optimal_beta, color="black", linewidth=15, alpha=0.8, label="optimal-beta")
#
#
#         for ax in [ax1]:
#             ax.legend(prop={"size":30})
#         plt.savefig(run_id+"/evolution_actions.png")
#         plt.close()
#
#
#     return
#
#
#
# def plot_evolution_v1(rt, pt, optimal, betas, preds=None, loss=False,
# history_betas=False, history_would_have_done=False,run_id="algo"):
#     matplotlib.rc('font', serif='cm10')
#     plt.rcParams.update({'font.size': 50})
#
#     if loss != "False":
#         plt.figure(figsize=(40,40), dpi=100)
#         T=len(rt)
#         ax1=plt.subplot2grid((2,2),(0,0))
#         ax2=plt.subplot2grid((2,2),(1,0))
#         ax3=plt.subplot2grid((2,2),(0,1))
#         ax4=plt.subplot2grid((2,2),(1,1))
#
#         ax1.plot(np.log10(np.arange(1,T+1)),rt, color="red", linewidth=15, alpha=0.8, label=r'$R_t$')
#         ax1.plot(np.log10(np.arange(1,T+1)),optimal*np.ones(T), color="black",  linewidth=15,alpha=0.5, label="optimal")
#         ax2.plot(np.log10(np.arange(1,T+1)),pt, color="red", linewidth=15, alpha=0.8, label=r'$P_t$')
#         ax2.plot(np.log10(np.arange(1,T+1)),optimal*np.ones(T), color="black",  linewidth=15,alpha=0.5, label="optimal")
#         if preds!=None:
#             ax3.scatter(betas, preds, color="red", s=250, label="predictions", alpha=0.6)
#         ax3.scatter(betas, ps(betas), color="blue", s=250, label="true values", alpha=0.6)
#         ax4.plot(loss, color="black",  linewidth=15,alpha=0.5, label="Loss evolution")
#
#         for ax in [ax1, ax2, ax3,ax4]:
#             ax.legend(prop={"size":30})
#         plt.savefig(run_id+"/learning_curves.png")
#         plt.close()
#
#         if history_betas!=False:
#             optimal_beta = betas[np.where(ps(betas) == max(ps(betas)))[0][0]]
#             plt.figure(figsize=(40,40), dpi=100)
#
#             plt.hist(history_betas,bins=100, facecolor='r', alpha=0.6, edgecolor='blue', label="done")
#             plt.hist(history_would_have_done,bins=100, facecolor='g', alpha=0.4, edgecolor='black', label="would have done")
#             plt.legend()
#             plt.text(optimal_beta, 0, "*", size=350)
#             plt.text(-optimal_beta, 0, "*", size=350)
#
#             plt.savefig(run_id+"/histograma_betas.png")
#             plt.close()
#         return
#     else:
#         plt.figure(figsize=(30,15))
#         T=len(rt)
#         ax1=plt.subplot2grid((2,2),(0,0))
#         ax2=plt.subplot2grid((2,2),(1,0))
#         ax3=plt.subplot2grid((2,2),(0,1), rowspan=2)
#         ax1.plot(np.arange(1,T+1),rt, color="red", linewidth=15, alpha=0.8, label=r'$R_t$')
#         ax1.plot(optimal*np.ones(T), color="black",  linewidth=15,alpha=0.5, label="optimal")
#         ax2.plot(np.arange(1,T+1),pt, color="red", linewidth=15, alpha=0.8, label=r'$P_t$')
#         ax2.plot(optimal*np.ones(T), color="black",  linewidth=15,alpha=0.5, label="optimal")
#         if preds!=None:
#             ax3.scatter(betas, preds, color="red", s=250, label="predictions", alpha=0.6)
#         ax3.scatter(betas, ps(betas), color="blue", s=250, label="true values", alpha=0.6)
#
#         for ax in [ax1, ax2, ax3]:
#             ax.legend(prop={"size":30})
#         plt.savefig(run_id+"/learning_curves.png")
#         plt.close()
#         return
