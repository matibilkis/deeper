{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm as tqdm\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "import random\n",
    "import matplotlib\n",
    "from environment import Environment\n",
    "from plots import just_plot\n",
    "from misc import *\n",
    "from nets import *\n",
    "from buffer import ReplayBuffer\n",
    "\n",
    "amplitude=0.4\n",
    "tau = .01\n",
    "lr_critic = 0.0001\n",
    "lr_actor=0.001\n",
    "noise_displacement = .1\n",
    "ep_guess=0.01\n",
    "dolinar_layers=2\n",
    "number_phases=2\n",
    "buffer_size = 5000\n",
    "batch_size = 8.\n",
    "\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def step_critic_tf(batched_input,labels_critic, critic, optimizer_critic):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(critic.trainable_variables)\n",
    "        preds_critic = critic(batched_input)\n",
    "        loss_critic = tf.keras.losses.MSE(tf.expand_dims(labels_critic, axis=2), preds_critic)\n",
    "        loss_critic = tf.reduce_mean(loss_critic)\n",
    "        grads = tape.gradient(loss_critic, critic.trainable_variables)\n",
    "        optimizer_critic.apply_gradients(zip(grads, critic.trainable_variables))\n",
    "        return tf.squeeze(loss_critic)\n",
    "\n",
    "@tf.function\n",
    "def critic_grad_tf(critic, experiences):\n",
    "    with tf.GradientTape() as tape:\n",
    "        unstacked_exp = tf.unstack(tf.convert_to_tensor(experiences), axis=1)\n",
    "        to_stack = []\n",
    "        actions_wathed_index = []\n",
    "        for index in range(0,experiences.shape[-1]-3,2): # I consider from first outcome to last one (but guess)\n",
    "            actions_wathed_index.append(index)\n",
    "            to_stack.append(tf.reshape(unstacked_exp[index],(experiences.shape[0],1,1)))\n",
    "\n",
    "        actions_indexed = tf.concat(to_stack,axis=1)\n",
    "        tape.watch(actions_indexed)\n",
    "\n",
    "        index_actions=0\n",
    "        watched_exps=[tf.ones((experiences.shape[0],1,1))*critic.pad_value]\n",
    "        watched_actions_unstacked = tf.unstack(actions_indexed, axis=1)\n",
    "        for index in range(0,experiences.shape[-1]-1):\n",
    "            if index in actions_wathed_index:\n",
    "                watched_exps.append(tf.expand_dims(watched_actions_unstacked[index_actions], axis=2))\n",
    "                index_actions+=1\n",
    "            else:\n",
    "                watched_exps.append(tf.reshape(unstacked_exp[index],(experiences.shape[0],1,1)))\n",
    "\n",
    "        qvals = critic(tf.reshape(tf.concat(watched_exps, axis=2), (experiences.shape[0],critic.dolinar_layers+1,2)))\n",
    "\n",
    "        dq_da = tape.gradient(qvals, actions_indexed)\n",
    "        return dq_da\n",
    "\n",
    "@tf.function\n",
    "def actor_grad_tf(actor, dq_da, experiences, optimizer_actor):\n",
    "    unstacked_exp = tf.unstack(experiences, axis=1)\n",
    "    actions_per_episode={}\n",
    "    context_outcome_actor = np.reshape(np.array([actor.pad_value]),(1,1,1)).astype(np.float32)\n",
    "    finns = [tf.multiply(actor(context_outcome_actor), tf.ones((experiences.shape[0],1,1)))]\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(actor.trainable_variables)\n",
    "        for index in range(1,2*actor.dolinar_layers-2,2):\n",
    "            actions_per_episode[str(index)] = []\n",
    "            for k in tf.unstack(unstacked_exp[index]):\n",
    "                actions_per_episode[str(index)].append(actor(tf.reshape(k, (1,1,1))))\n",
    "            finns.append(tf.concat(actions_per_episode[str(index)], axis=0))\n",
    "        final_preds = tf.concat(finns, axis=1)\n",
    "        da_dtheta=tape.gradient(final_preds, actor.trainable_variables, output_gradients=-dq_da)\n",
    "        optimizer_actor.apply_gradients(zip(da_dtheta, actor.trainable_variables))\n",
    "    return\n",
    "\n",
    "\n",
    "    # states_to_act=[tf.ones((experiences.shape[0],1,1))*actor.pad_value]\n",
    "    #\n",
    "    # to_stack = []\n",
    "    # actions_wathed_index = []\n",
    "    # for index in range(1,2*actor.dolinar_layers-2,2):\n",
    "    #     states_to_act.append(tf.reshape(unstacked_exp[index],(experiences.shape[0],1,1)))\n",
    "    # inps_actor = tf.concat(states_to_act, axis=1)\n",
    "    # actor.lstm.stateful=False\n",
    "    # actor_thinks = actor(inps_actor)\n",
    "    # actor.lstm.stateful=True\n",
    "    # da_dtheta = tape.gradient(actor_thinks, actor.trainable_variables, output_gradients=-dq_da)\n",
    "    # optimizer_actor.apply_gradients(zip(da_dtheta, actor.trainable_variables))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def optimization_step(experiences, critic, critic_target, actor, actor_target, optimizer_critic, optimizer_actor):\n",
    "    # actor.lstm.reset_states()\n",
    "    actor.lstm.stateful=False\n",
    "    # experiences = experiences.astype(np.float32)\n",
    "    targeted_experience = actor_target.process_sequence_of_experiences_tf(experiences)\n",
    "    sequences, zeroed_rews = critic_target.process_sequence_tf(targeted_experience)\n",
    "    labels_critic = critic_target.give_td_errors_tf( sequences, zeroed_rews)\n",
    "\n",
    "    loss_critic = step_critic_tf(sequences ,labels_critic, critic, optimizer_critic)\n",
    "\n",
    "    dq_da = critic_grad_tf(critic, experiences)\n",
    "\n",
    "    actor_grad_tf(actor, dq_da, experiences, optimizer_actor)\n",
    "\n",
    "    actor.lstm.stateful=True\n",
    "    return loss_critic\n",
    "\n",
    "\n",
    "env = Environment(amplitude=amplitude, dolinar_layers = dolinar_layers, number_phases=number_phases)\n",
    "buffer = ReplayBuffer(buffer_size=buffer_size)\n",
    "\n",
    "critic = Critic(nature=\"primary\",valreg=0.01, dolinar_layers = dolinar_layers, number_phases=number_phases)\n",
    "critic_target = Critic(nature=\"target\", dolinar_layers = dolinar_layers, number_phases=number_phases)\n",
    "actor = Actor(nature=\"primary\", dolinar_layers = dolinar_layers)\n",
    "actor_target = Actor(nature=\"target\", dolinar_layers = dolinar_layers)\n",
    "\n",
    "optimizer_critic = tf.keras.optimizers.Adam(lr=lr_critic)\n",
    "optimizer_actor = tf.keras.optimizers.Adam(lr=lr_actor)\n",
    "\n",
    "policy_evaluator = PolicyEvaluator(amplitude = amplitude, dolinar_layers=dolinar_layers, number_phases = number_phases)\n",
    "\n",
    "experiences = np.load(\"tutorials_functions/expe_2L.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor.lstm.stateful=False\n",
    "experiences = experiences.astype(np.float32)\n",
    "targeted_experience = actor_target.process_sequence_of_experiences_tf(experiences)\n",
    "sequences, zeroed_rews = critic_target.process_sequence_tf(targeted_experience)\n",
    "labels_critic = critic_target.give_td_errors_tf( sequences, zeroed_rews)\n",
    "dq_da = critic_grad_tf(critic, experiences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tf.function\n",
    "def actor_grad_tf_old(actor, dq_da, experiences, optimizer_actor):\n",
    "    unstacked_exp = tf.unstack(experiences, axis=1)\n",
    "    actions_per_episode={}\n",
    "    context_outcome_actor = np.reshape(np.array([actor.pad_value]),(1,1,1)).astype(np.float32)\n",
    "    finns = [tf.multiply(actor(context_outcome_actor), tf.ones((experiences.shape[0],1,1)))]\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(actor.trainable_variables)\n",
    "        for index in range(1,2*actor.dolinar_layers-2,2):\n",
    "            actions_per_episode[str(index)] = []\n",
    "            for k in tf.unstack(unstacked_exp[index]):\n",
    "                actions_per_episode[str(index)].append(actor(tf.reshape(k, (1,1,1))))\n",
    "            finns.append(tf.concat(actions_per_episode[str(index)], axis=0))\n",
    "        final_preds = tf.concat(finns, axis=1)\n",
    "        da_dtheta=tape.gradient(final_preds, actor.trainable_variables, output_gradients=-dq_da)\n",
    "        optimizer_actor.apply_gradients(zip(da_dtheta, actor.trainable_variables))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def act_v2(actor, dq_da, experiences, optimizer_actor):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(actor.trainable_variables)\n",
    "        finns = [actor(tf.ones((experiences.shape[0], 1,1))*actor.pad_value)]\n",
    "        unstacked_exp = tf.unstack(experiences, axis=1)\n",
    "        for index in range(1,2*actor.dolinar_layers-2,2):\n",
    "            finns.append(actor(tf.reshape(unstacked_exp[index], (experiences.shape[0], 1,1))))    \n",
    "        final_preds = tf.concat(finns, axis=1)\n",
    "        da_dtheta=tape.gradient(final_preds, actor.trainable_variables, output_gradients=-dq_da)\n",
    "        optimizer_actor.apply_gradients(zip(da_dtheta, actor.trainable_variables))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit act_v2(actor, dq_da, experiences, optimizer_actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit act_v2(actor, dq_da, tf.convert_to_tensor(experiences), optimizer_actor) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def act_v3(actor, dq_da, experiences, optimizer_actor):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(actor.trainable_variables)\n",
    "        finns = [tf.ones((experiences.shape[0], 1,1))*actor.pad_value]\n",
    "        unstacked_exp = tf.unstack(experiences, axis=1)\n",
    "        for index in range(1,2*actor.dolinar_layers-2,2):\n",
    "            finns.append(tf.reshape(unstacked_exp[index], (experiences.shape[0], 1,1)))\n",
    "        final_preds = tf.concat(finns, axis=1)\n",
    "        final_preds = actor(final_preds)\n",
    "        da_dtheta=tape.gradient(final_preds, actor.trainable_variables, output_gradients=-dq_da)\n",
    "        optimizer_actor.apply_gradients(zip(da_dtheta, actor.trainable_variables))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.2 ms ± 768 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit act_v3(actor, dq_da, experiences, optimizer_actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
