{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm as tqdm\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "import random\n",
    "import matplotlib\n",
    "from environment import Environment\n",
    "from plots import just_plot\n",
    "from misc import *\n",
    "from nets import *\n",
    "from buffer import ReplayBuffer\n",
    "import timeit\n",
    "\n",
    "amplitude=0.4\n",
    "dolinar_layers=2\n",
    "number_phases=2\n",
    "total_episodes = 10**3\n",
    "buffer_size=500\n",
    "batch_size=64\n",
    "ep_guess=0.01\n",
    "noise_displacement=0.5\n",
    "lr_actor=0.01\n",
    "lr_critic=0.001\n",
    "tau=0.005\n",
    "\n",
    "\n",
    "exper = np.load(\"example_buffer/2_sample.npy\")\n",
    "env = Environment(amplitude=amplitude, dolinar_layers = dolinar_layers, number_phases=number_phases)\n",
    "# buffer = ReplayBuffer(buffer_size=buffer_size)\n",
    "\n",
    "critic = Critic(nature=\"primary\",valreg=0.01, dolinar_layers = dolinar_layers, number_phases=number_phases)\n",
    "critic_target = Critic(nature=\"target\", dolinar_layers = dolinar_layers, number_phases=number_phases)\n",
    "actor = Actor(nature=\"primary\", dolinar_layers = dolinar_layers)\n",
    "actor_target = Actor(nature=\"target\", dolinar_layers = dolinar_layers)\n",
    "\n",
    "optimizer_critic = tf.keras.optimizers.Adam(lr=lr_critic)\n",
    "optimizer_actor = tf.keras.optimizers.Adam(lr=lr_actor)\n",
    "\n",
    "policy_evaluator = PolicyEvaluator(amplitude = amplitude, dolinar_layers=dolinar_layers, number_phases = number_phases)\n",
    "\n",
    "#\n",
    "experiences = exper.astype(np.float32)\n",
    "targeted_experience = actor_target.process_sequence_of_experiences_tf(experiences)\n",
    "sequences, zeroed_rews = critic_target.process_sequence_tf(targeted_experience)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def give_td_error_Kennedy_guess_tf(critic,sequences,zeroed_rews):\n",
    "    if critic.nature != \"target\":\n",
    "        raise AttributeError(\"I'm not the target!\")\n",
    "\n",
    "    final_rews = tf.reshape(zeroed_rews[:,-1], (sequences.shape[0],1,1))\n",
    "    bellman_tds_noguess = critic(sequences)[:,1:-1,:]\n",
    "\n",
    "    phases = tf.range(critic.number_phases, dtype=np.float32)/critic.number_phases\n",
    "\n",
    "    unstacked = tf.unstack(tf.convert_to_tensor(sequences))\n",
    "    phases_concs = {}\n",
    "    for ph in range(critic.number_phases):\n",
    "        phases_concs[str(ph)] = []\n",
    "    stacked = {}\n",
    "\n",
    "    for episode in unstacked:\n",
    "        prefinal = episode[:-1]\n",
    "        for ph in range(critic.number_phases):\n",
    "            final = tf.expand_dims(tf.stack([tf.unstack(episode[-1])[0], phases[ph]], axis=0), axis=0)\n",
    "            phases_concs[str(ph)].append(tf.concat([prefinal, final], axis=0))\n",
    "    #\n",
    "        for ph in range(critic.number_phases):\n",
    "            stacked[str(ph)] = tf.stack(phases_concs[str(ph)], axis=0)\n",
    "\n",
    "    all_preds = tf.concat([critic(stacked[str(ph)]) for ph in range(critic.number_phases)], axis=2)\n",
    "    maxs = tf.math.reduce_max(all_preds,axis=2)[:,-1]\n",
    "    bellman_td = tf.concat([tf.reshape(bellman_tds_noguess,(sequences.shape[0],critic.dolinar_layers-1)), tf.reshape(maxs,(sequences.shape[0],1))], axis=1)\n",
    "    return tf.concat([bellman_td, tf.reshape(zeroed_rews[:,-1], (sequences.shape[0],1))], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_td_error_Kennedy_guess(critic,sequences,zeroed_rews):\n",
    "    if critic.nature != \"target\":\n",
    "        raise AttributeError(\"I'm not the target!\")\n",
    "\n",
    "    final_rews = tf.reshape(zeroed_rews[:,-1], (sequences.shape[0],1,1))\n",
    "    bellman_tds_noguess = critic(sequences)[:,1:-1,:]\n",
    "\n",
    "    phases = tf.range(critic.number_phases, dtype=np.float32)/critic.number_phases\n",
    "\n",
    "    unstacked = tf.unstack(tf.convert_to_tensor(sequences))\n",
    "    phases_concs = {}\n",
    "    for ph in range(critic.number_phases):\n",
    "        phases_concs[str(ph)] = []\n",
    "    stacked = {}\n",
    "\n",
    "    for episode in unstacked:\n",
    "        prefinal = episode[:-1]\n",
    "        for ph in range(critic.number_phases):\n",
    "            final = tf.expand_dims(tf.stack([tf.unstack(episode[-1])[0], phases[ph]], axis=0), axis=0)\n",
    "            phases_concs[str(ph)].append(tf.concat([prefinal, final], axis=0))\n",
    "    #\n",
    "        for ph in range(critic.number_phases):\n",
    "            stacked[str(ph)] = tf.stack(phases_concs[str(ph)], axis=0)\n",
    "\n",
    "    all_preds = tf.concat([critic(stacked[str(ph)]) for ph in range(critic.number_phases)], axis=2)\n",
    "    maxs = tf.math.reduce_max(all_preds,axis=2)[:,-1]\n",
    "    bellman_td = tf.concat([tf.reshape(bellman_tds_noguess,(sequences.shape[0],critic.dolinar_layers-1)), tf.reshape(maxs,(sequences.shape[0],1))], axis=1)\n",
    "    return tf.concat([bellman_td, tf.reshape(zeroed_rews[:,-1], (sequences.shape[0],1))], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit give_td_error_Kennedy_guess_tf(critic_target,sequences,zeroed_rews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit give_td_error_Kennedy_guess(critic_target,sequences,zeroed_rews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_critic = give_td_error_Kennedy_guess_tf(critic_target,sequences,zeroed_rews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def step_critic_tf(labels_critic, critic):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(critic.trainable_variables)\n",
    "        preds_critic = critic(sequences)\n",
    "        loss_critic = tf.keras.losses.MSE(labels_critic, preds_critic)\n",
    "        loss_critic = tf.reduce_mean(loss_critic)\n",
    "        grads = tape.gradient(loss_critic, critic.trainable_variables)\n",
    "        optimizer_critic.apply_gradients(zip(grads, critic.trainable_variables))\n",
    "        return tf.squeeze(loss_critic)\n",
    "    \n",
    "def step_critic(labels_critic, critic):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(critic.trainable_variables)\n",
    "        preds_critic = critic(sequences)\n",
    "        loss_critic = tf.keras.losses.MSE(labels_critic, preds_critic)\n",
    "        loss_critic = tf.reduce_mean(loss_critic)\n",
    "        grads = tape.gradient(loss_critic, critic.trainable_variables)\n",
    "        optimizer_critic.apply_gradients(zip(grads, critic.trainable_variables))\n",
    "        return tf.squeeze(loss_critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit step_critic(tf.expand_dims(labels_critic, axis=2), critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit step_critic_tf(tf.expand_dims(labels_critic, axis=2), critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiences[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tf.function\n",
    "def process_sequence_of_experiences_tf(self, experiences):\n",
    "    self.lstm.stateful=True\n",
    "\n",
    "    unstacked_exp = tf.unstack(tf.convert_to_tensor(experiences), axis=1)\n",
    "    to_stack = []\n",
    "    for index in range(2*self.dolinar_layers-1): # I consider from first outcome to last one (but guess)\n",
    "        if (index==0):\n",
    "            to_stack.append(unstacked_exp[index])\n",
    "        if (index%2 == 1):\n",
    "            to_stack.append(unstacked_exp[index])\n",
    "\n",
    "            to_stack.append(tf.squeeze(self(tf.reshape(unstacked_exp[index],(experiences.shape[0],1,1)))))\n",
    "    for index in range(2*self.dolinar_layers-1, 2*self.dolinar_layers+2):\n",
    "        to_stack.append(unstacked_exp[index])\n",
    "    self.lstm.stateful=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def critic_derivative(experiences, actor, critic):\n",
    "    actions_indexed = [0.]*(actor.dolinar_layers)\n",
    " \n",
    "    with tf.GradientTape() as tape:\n",
    "        unstacked_exp = tf.unstack(tf.convert_to_tensor(experiences), axis=1)\n",
    "        to_stack = []\n",
    "        actions_wathed_index = []\n",
    "        for index in range(0,experiences.shape[-1]-3,2): # I consider from first outcome to last one (but guess)\n",
    "            actions_wathed_index.append(index)\n",
    "            to_stack.append(tf.reshape(unstacked_exp[index],(experiences.shape[0],1,1)))\n",
    "   \n",
    "        actions_indexed = tf.concat(to_stack,axis=1)\n",
    "    tape.watch(actions_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def critic_grad(critic, experiences):\n",
    "    with tf.GradientTape() as tape:\n",
    "        unstacked_exp = tf.unstack(tf.convert_to_tensor(experiences), axis=1)\n",
    "        to_stack = []\n",
    "        actions_wathed_index = []\n",
    "        for index in range(0,experiences.shape[-1]-3,2): # I consider from first outcome to last one (but guess)\n",
    "            actions_wathed_index.append(index)\n",
    "            to_stack.append(tf.reshape(unstacked_exp[index],(experiences.shape[0],1,1)))\n",
    "\n",
    "        actions_indexed = tf.concat(to_stack,axis=1)\n",
    "        tape.watch(actions_indexed)\n",
    "\n",
    "\n",
    "        index_actions=0\n",
    "        watched_exps=[tf.ones((experiences.shape[0],1,1))*actor.pad_value]\n",
    "        watched_actions_unstacked = tf.unstack(actions_indexed, axis=1)\n",
    "        for index in range(0,experiences.shape[-1]-1): \n",
    "            if index in actions_wathed_index:\n",
    "                watched_exps.append(tf.expand_dims(watched_actions_unstacked[index_actions], axis=2))\n",
    "                index_actions+=1\n",
    "            else:\n",
    "                watched_exps.append(tf.reshape(unstacked_exp[index],(experiences.shape[0],1,1)))\n",
    "\n",
    "        qvals = critic(tf.reshape(tf.concat(watched_exps, axis=2), (experiences.shape[0],critic.dolinar_layers+1,2)))\n",
    "\n",
    "        dq_da = tape.gradient(qvals, actions_indexed)\n",
    "        return dq_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit critic_grad(critic, experiences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def critic_grad_tf(critic, experiences):\n",
    "    with tf.GradientTape() as tape:\n",
    "        unstacked_exp = tf.unstack(tf.convert_to_tensor(experiences), axis=1)\n",
    "        to_stack = []\n",
    "        actions_wathed_index = []\n",
    "        for index in range(0,experiences.shape[-1]-3,2): # I consider from first outcome to last one (but guess)\n",
    "            actions_wathed_index.append(index)\n",
    "            to_stack.append(tf.reshape(unstacked_exp[index],(experiences.shape[0],1,1)))\n",
    "\n",
    "        actions_indexed = tf.concat(to_stack,axis=1)\n",
    "        tape.watch(actions_indexed)\n",
    "\n",
    "\n",
    "        index_actions=0\n",
    "        watched_exps=[tf.ones((experiences.shape[0],1,1))*actor.pad_value]\n",
    "        watched_actions_unstacked = tf.unstack(actions_indexed, axis=1)\n",
    "        for index in range(0,experiences.shape[-1]-1): \n",
    "            if index in actions_wathed_index:\n",
    "                watched_exps.append(tf.expand_dims(watched_actions_unstacked[index_actions], axis=2))\n",
    "                index_actions+=1\n",
    "            else:\n",
    "                \n",
    "                watched_exps.append(tf.reshape(unstacked_exp[index],(experiences.shape[0],1,1)))\n",
    "\n",
    "        qvals = critic(tf.reshape(tf.concat(watched_exps, axis=2), (experiences.shape[0],critic.dolinar_layers+1,2)))\n",
    "\n",
    "        dq_da = tape.gradient(qvals, actions_indexed)\n",
    "        return dq_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dq_da= critic_grad_tf(critic, experiences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(actor.trainable_variables)\n",
    "    pads = np.ones(len(experiences)).astype(np.float32)*actor.pad_value\n",
    "    news = np.random.rand(experiences.shape[0], experiences.shape[1]+1).astype(np.float32)\n",
    "    news[:,1:] = experiences\n",
    "    news[:,0] = pads\n",
    "    instances_actor = [i for i in range(0,2*actor.dolinar_layers,2)]\n",
    "    actionss = actor(np.reshape(news[:,instances_actor], (experiences.shape[0],actor.dolinar_layers,1)).astype(np.float32))\n",
    "    da_dtheta = tape.gradient(actionss, actor.trainable_variables, output_gradients=-dq_da)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def actor_grad(actor, dq_da, experiences, optimizer_actor):\n",
    "    with tf.GradientTape() as tape:\n",
    "        unstacked_exp = tf.unstack(tf.convert_to_tensor(experiences), axis=1)\n",
    "        states_to_act=[tf.ones((experiences.shape[0],1,1))*actor.pad_value]\n",
    "\n",
    "        to_stack = [] \n",
    "        actions_wathed_index = []\n",
    "        for index in range(1,2*actor.dolinar_layers-2,2):\n",
    "            states_to_act.append(tf.reshape(unstacked_exp[index],(experiences.shape[0],1,1)))\n",
    "\n",
    "        actor_thinks = actor(tf.concat(states_to_act, axis=1))\n",
    "        da_dtheta = tape.gradient(actor_thinks, actor.trainable_variables, output_gradients=-dq_da)\n",
    "        optimizer_actor.apply_gradients(zip(da_dtheta, actor.trainable_variables))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit actor_grad(actor, dq_da, experiences, optimizer_actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def actor_grad_tf(actor, dq_da, experiences, optimizer_actor):\n",
    "    with tf.GradientTape() as tape:\n",
    "        unstacked_exp = tf.unstack(tf.convert_to_tensor(experiences), axis=1)\n",
    "        states_to_act=[tf.ones((experiences.shape[0],1,1))*actor.pad_value]\n",
    "\n",
    "        to_stack = [] \n",
    "        actions_wathed_index = []\n",
    "        for index in range(1,2*actor.dolinar_layers-2,2):\n",
    "            states_to_act.append(tf.reshape(unstacked_exp[index],(experiences.shape[0],1,1)))\n",
    "\n",
    "        actor_thinks = actor(tf.concat(states_to_act, axis=1))\n",
    "        da_dtheta = tape.gradient(actor_thinks, actor.trainable_variables, output_gradients=-dq_da)\n",
    "        optimizer_actor.apply_gradients(zip(da_dtheta, actor.trainable_variables))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit actor_grad_tf(actor, dq_da, experiences, optimizer_actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit policy_evaluator.greedy_strategy(actor = actor, critic = critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dq_da = critic_grad_tf(critic, experiences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actor_grad_tf(actor, dq_da, experiences, optimizer_actor):\n",
    "    with tf.GradientTape() as tape:\n",
    "        unstacked_exp = tf.unstack(tf.convert_to_tensor(experiences), axis=1)\n",
    "        states_to_act=[tf.ones((experiences.shape[0],1,1))*actor.pad_value]\n",
    "\n",
    "        to_stack = []\n",
    "        actions_wathed_index = []\n",
    "        for index in range(1,2*actor.dolinar_layers-2,2):\n",
    "            states_to_act.append(tf.reshape(unstacked_exp[index],(experiences.shape[0],1,1)))\n",
    "        inps_actor = tf.concat(states_to_act, axis=1)\n",
    "        actor.lstm.stateful=False\n",
    "        actor_thinks = actor(inps_actor)\n",
    "        actor.lstm.stateful=True\n",
    "        da_dtheta = tape.gradient(actor_thinks, actor.trainable_variables, output_gradients=-dq_da)\n",
    "        optimizer_actor.apply_gradients(zip(da_dtheta, actor.trainable_variables))\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1, 1), dtype=float32, numpy=array([[[0.00322904]]], dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor = Actor(nature=\"primary\")\n",
    "context_outcome_actor = np.reshape(np.array([actor.pad_value]),(1,1,1)).astype(np.float32)\n",
    "actor(context_outcome_actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(15,), dtype=float32, numpy=\n",
       " array([ 0.03453873, -0.25866753, -0.24178177, -0.2672065 , -0.1888024 ,\n",
       "        -0.0119067 ,  0.0838516 ,  0.09396052, -0.18203147, -0.2001634 ,\n",
       "        -0.27773562, -0.06698474, -0.29122508, -0.02650449, -0.36419973],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(15,), dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(15,), dtype=float32, numpy=\n",
       " array([-0.33991995, -0.19607863,  0.09374991,  0.04338924, -0.11296402,\n",
       "         0.10449888, -0.223578  , -0.00805294, -0.11425584,  0.02064468,\n",
       "         0.05043116, -0.2857382 , -0.16833827, -0.38040155, -0.17870723],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(15,), dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(15,), dtype=float32, numpy=\n",
       " array([0. , 0.5, 0.5, 0. , 0.5, 0. , 0. , 0.5, 0. , 0. , 0. , 0. , 0.5,\n",
       "        0.5, 0. ], dtype=float32)>,\n",
       " <tf.Tensor: shape=(15,), dtype=float32, numpy=\n",
       " array([1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1.],\n",
       "       dtype=float32)>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.unstack(experiences, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unstacked_exp = tf.unstack(experiences, axis=1)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(actor.trainable_variables)\n",
    "    actions_per_episode={}\n",
    "    context_outcome_actor = np.reshape(np.array([actor.pad_value]),(1,1,1)).astype(np.float32)\n",
    "    finns = [tf.multiply(actor(context_outcome_actor).numpy(), tf.ones((experiences.shape[0],1,1)))]\n",
    "    for index in range(1,2*actor.dolinar_layers-2,2):\n",
    "        actions_per_episode[str(index)] = []\n",
    "        for k in tf.unstack(unstacked_exp[index]):\n",
    "            actions_per_episode[str(index)].append(actor(tf.reshape(k, (1,1,1))))\n",
    "        finns.append(tf.concat(actions_per_episode[str(index)], axis=0))\n",
    "    tf.concat(finns, axis=1)\n",
    "    da_dtheta=tape.gradient(finns, actor.trainable_variables, output_gradients=-dq_da)\n",
    "    optimizer_actor.apply_gradients(zip(da_dtheta, actor.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_dtheta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    states_to_act=[tf.ones((experiences.shape[0],1,1))*actor.pad_value]\n",
    "\n",
    "    to_stack = []\n",
    "    actions_wathed_index = []\n",
    "    for index in range(1,2*actor.dolinar_layers-2,2):\n",
    "        states_to_act.append(tf.reshape(unstacked_exp[index],(experiences.shape[0],1,1)))\n",
    "    inps_actor = tf.concat(states_to_act, axis=1)\n",
    "    actor.lstm.stateful=False\n",
    "    actor_thinks = actor(inps_actor)\n",
    "    actor.lstm.stateful=True\n",
    "    da_dtheta = tape.gradient(actor_thinks, actor.trainable_variables, output_gradients=-dq_da)\n",
    "    optimizer_actor.apply_gradients(zip(da_dtheta, actor.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
