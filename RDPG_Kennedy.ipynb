{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm as tqdm\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "import random\n",
    "import matplotlib\n",
    "\n",
    "from plots import *\n",
    "from misc import Prob, ps_maxlik, qval, record\n",
    "from nets import *\n",
    "from buffer import ReplayBuffer\n",
    "\n",
    "\n",
    "def optimization_step(experiences,critic, critic_target, actor, optimizer_critic, optimizer_actor, train_loss):\n",
    "    sequences, zeroed_rews = critic.process_sequence(experiences)\n",
    "    labels_critic = critic_target.give_td_error_Kennedy_guess( sequences, zeroed_rews)\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(critic.trainable_variables)\n",
    "        preds_critic = critic(sequences)\n",
    "        loss_critic = tf.keras.losses.MSE(labels_critic, preds_critic)\n",
    "        loss_critic = tf.reduce_mean(loss_critic)\n",
    "        grads = tape.gradient(loss_critic, critic.trainable_variables)\n",
    "        optimizer_critic.apply_gradients(zip(grads, critic.trainable_variables))\n",
    "        train_loss(loss_critic)\n",
    "\n",
    "    critic_target.update_target_parameters(critic, tau=0.05)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        ones = tf.ones(shape=(experiences.shape[0],1))\n",
    "        actions = tf.cast(actor(np.expand_dims(np.zeros(len(experiences)),axis=1)), tf.float32)   #This can be improved i think!! (the conversion... )\n",
    "\n",
    "        tape.watch(actions)\n",
    "        qvals = critic(tf.expand_dims(tf.concat([actions, ones], axis=1),axis=1))\n",
    "        dq_da = tape.gradient(qvals, actions)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        actionss = tf.cast(actor(np.expand_dims(np.zeros(len(experiences)),axis=1)), tf.float32)\n",
    "        da_dtheta = tape.gradient(actionss, actor.trainable_variables, output_gradients=-dq_da)\n",
    "\n",
    "    optimizer_actor.apply_gradients(zip(da_dtheta, actor.trainable_variables))\n",
    "    return\n",
    "    ###### END OF OPTIMIZATION STEP ######\n",
    "    ###### END OF OPTIMIZATION STEP ######\n",
    "\n",
    "\n",
    "def ddpgKennedy(special_name=\"\",total_episodes = 10**3,buffer_size=500, batch_size=64, ep_guess=0.01,\n",
    " noise_displacement=0.5,lr_actor=0.01, lr_critic=0.001, tau=0.005, repetitions=1, plots=True):\n",
    "\n",
    "    if not os.path.exists(\"results\"):\n",
    "        os.makedirs(\"results\")\n",
    "\n",
    "    amplitude = 0.4\n",
    "    buffer = ReplayBuffer(buffer_size=buffer_size)\n",
    "\n",
    "    critic = Critic()\n",
    "    critic_target = Critic()\n",
    "    actor = Actor(input_dim=1)\n",
    "\n",
    "    actor(np.array([[0.]]).astype(np.float32)) #initialize the network 0, arbitrary inputs.\n",
    "    #\n",
    "    optimizer_critic = tf.keras.optimizers.Adam(lr=lr_critic)\n",
    "    optimizer_actor = tf.keras.optimizers.Adam(lr=lr_actor)\n",
    "\n",
    "\n",
    "    rt = []\n",
    "    pt = []\n",
    "\n",
    "    #define this global so i use them in a function defined above... optimizatin step and testing()\n",
    "    train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\n",
    "    test_loss = tf.keras.metrics.Mean('test_loss', dtype=tf.float32)\n",
    "\n",
    "\n",
    "    if special_name == \"\":\n",
    "        # current_run_and_time = \"results/{}\".format(datetime.now().strftime(\"%Y%m%d-%H%M\"))\n",
    "        numb = record()\n",
    "        current_run_and_time =\"results/run_\" + str(numb)\n",
    "    else:\n",
    "        current_run_and_time = \"results/\"+special_name\n",
    "\n",
    "    directory = current_run_and_time\n",
    "    train_log =  current_run_and_time + '/train_l0'\n",
    "    test_log =   current_run_and_time + '/test_l0'\n",
    "\n",
    "    train_summary_writer = tf.summary.create_file_writer(train_log)\n",
    "    test_summary_writer_0 = tf.summary.create_file_writer(test_log)\n",
    "\n",
    "    info_optimizers = \"optimizer_critic_guess: {} \\nOptimizer_actor_l0: {}\\n\".format(optimizer_critic.get_config(), optimizer_actor.get_config())\n",
    "    infor_buffer = \"Buffer_size: {}\\n Batch_size for sampling: {}\\n\".format(buffer.buffer_size, batch_size)\n",
    "    info_epsilons= \"epsilon-guess: {}\\nepsilon_displacement_noise: {}\".format(ep_guess,noise_displacement)\n",
    "\n",
    "    data = \"tau: {}, repetitions per optimization step (would be like epochs): {}\".format(tau,repetitions) + \"\\n \\n**** optimizers ***\\n\"+info_optimizers+\"\\n\\n\\n*** BUFFER ***\\n\"+infor_buffer+\"\\n\\n\\n *** NOISE PARAMETERS *** \\n\"+info_epsilons\n",
    "    with open(directory+\"/info.txt\", 'w') as f:\n",
    "        f.write(data)\n",
    "        f.close()\n",
    "\n",
    "    print(\"Beggining to train! \\n \\n\")\n",
    "    print(data)\n",
    "    print(\"starting time: {}\".format(datetime.now().strftime(\"%Y%m%d-%H%M%S\")))\n",
    "    print(\"saving results in \" + str(directory))\n",
    "    avg_train = []\n",
    "    avg_test = []\n",
    "\n",
    "    history_betas = [] #to put in histogram\n",
    "    history_betas_would_have_done=[] #to put in histogram\n",
    "    histo_preds = {\"critic\":{}} #here i save the predictions to plot in a \"straightforward way\"\n",
    "\n",
    "    #######\n",
    "    for episode in tqdm(range(total_episodes)):\n",
    "\n",
    "        alice_phase = np.random.choice([-1.,1.],1)[0]\n",
    "        beta_would_do = actor(np.array([[0.]])).numpy()[0][0]\n",
    "        beta =  beta_would_do + max(0.1, np.random.uniform(-noise_displacement, noise_displacement)*np.exp(-episode/300))\n",
    "        proboutcome = Prob(alice_phase*amplitude,beta,0)\n",
    "        outcome = np.random.choice([0.,1.],1,p=[proboutcome, 1-proboutcome])[0]\n",
    "\n",
    "        history_betas.append(beta)\n",
    "        history_betas_would_have_done.append(beta_would_do)\n",
    "\n",
    "    #\n",
    "        if np.random.random()< ep_guess:\n",
    "            guess = np.random.choice([-1.,1.],1)[0]\n",
    "        else:\n",
    "            sequence = np.array([[ [beta, critic.pad_value], [outcome, -1.]]  ]).astype(np.float32)\n",
    "            guess = critic.give_favourite_guess(sequence)\n",
    "        if guess == alice_phase:\n",
    "            reward = 1.\n",
    "        else:\n",
    "            reward = 0.\n",
    "        buffer.add(beta, outcome, guess, reward)\n",
    "\n",
    "\n",
    "        ###### OPTIMIZATION STEP ######\n",
    "        ###### OPTIMIZATION STEP ######\n",
    "\n",
    "        experiences = buffer.sample(batch_size)\n",
    "        optimization_step(experiences,critic, critic_target, actor, optimizer_critic, optimizer_actor, train_loss)\n",
    "\n",
    "\n",
    "#####\n",
    "        avg_train.append(train_loss.result().numpy())\n",
    "        avg_test.append(test_loss.result().numpy())\n",
    "    #\n",
    "        rt.append(reward)\n",
    "    #\n",
    "\n",
    "        ########################################################################\n",
    "        ### calculate success probability if the agent went greedy ###########\n",
    "        p=0\n",
    "        for outcome in [0.,1.]:\n",
    "            p+=Prob(critic.give_favourite_guess(critic.pad_single_sequence([beta_would_do, outcome, -1.]))*amplitude, beta_would_do,outcome)\n",
    "        p/=2\n",
    "        pt.append(p)\n",
    "        ################\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    rt = [np.sum(rt[:k]) for k in range(len(rt))]\n",
    "    rt = rt/np.arange(1,len(rt)+1)\n",
    "\n",
    "    losses = [avg_train, avg_test]\n",
    "\n",
    "    #BigPlot(buffer,rt, pt, history_betas, history_betas_would_have_done, histo_preds, losses, directory)\n",
    "    return buffer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer actor_4 is casting an input tensor from dtype float32 to the layer's dtype of float64, which is new behavior in TensorFlow 2.  The layer has dtype float64 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float64, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float32 by default, call `tf.keras.backend.set_floatx('float32')`. To change just this layer, pass dtype='float32' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beggining to train! \n",
      " \n",
      "\n",
      "tau: 0.005, repetitions per optimization step (would be like epochs): 1\n",
      " \n",
      "**** optimizers ***\n",
      "optimizer_critic_guess: {'name': 'Adam', 'learning_rate': 0.001, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False} \n",
      "Optimizer_actor_l0: {'name': 'Adam', 'learning_rate': 0.01, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
      "\n",
      "\n",
      "\n",
      "*** BUFFER ***\n",
      "Buffer_size: 500\n",
      " Batch_size for sampling: 64\n",
      "\n",
      "\n",
      "\n",
      " *** NOISE PARAMETERS *** \n",
      "epsilon-guess: 0.01\n",
      "epsilon_displacement_noise: 0.5\n",
      "starting time: 20200511-004230\n",
      "saving results in results/run_19\n",
      "WARNING:tensorflow:Layer critic_8 is casting an input tensor from dtype float32 to the layer's dtype of float64, which is new behavior in TensorFlow 2.  The layer has dtype float64 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float64, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float32 by default, call `tf.keras.backend.set_floatx('float32')`. To change just this layer, pass dtype='float32' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:14<00:00,  6.71it/s]\n"
     ]
    }
   ],
   "source": [
    "buffer = ddpgKennedy(total_episodes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if episode%(total_episodes/10) == 0: #this is for showing 10 results in total.\n",
    "histo_preds = {\"layer0\":{}, \"layer1\":{}} #here i save the predictions to plot in a \"straightforward way\"\n",
    "episode = 10\n",
    "\n",
    "for layer in [\"layer0\",\"layer1\"]: #net_0 will be critic_q0, net_1 will be critic_qguess\n",
    "\n",
    "    histo_preds[layer][str(episode)] ={}\n",
    "    histo_preds[layer][str(episode)][\"episode\"] = episode\n",
    "    histo_preds[layer][str(episode)][\"values\"] = {}\n",
    "\n",
    "simp = np.random.randn(len(buffer.betas),4)\n",
    "simp[:,0] =buffer.betas\n",
    "qvals0 = np.squeeze(critic(critic.process_sequence(simp)[0]).numpy()[:,0]) \n",
    "histo_preds[\"layer0\"][str(episode)][\"values\"] = qvals0\n",
    "\n",
    "index=0\n",
    "for n1 in [0.,1.]:\n",
    "    for guess in [-1.,1.]:\n",
    "        simp[:,1] = n1\n",
    "        simp[:,2] = guess\n",
    "        qvals1 = np.squeeze(critic(critic.process_sequence(simp)[0]).numpy()[:,1]) \n",
    "        histo_preds[\"layer1\"][str(episode)][\"values\"][str(index)] = qvals1\n",
    "        index+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.49618411, 0.49618338, 0.4961832 , 0.49618292, 0.49618164,\n",
       "       0.49618037, 0.49617909, 0.49617781, 0.49617654, 0.49617526,\n",
       "       0.49617398, 0.4961727 , 0.49617142, 0.49617014, 0.49616886,\n",
       "       0.49616758, 0.4961663 , 0.49616502, 0.49616373, 0.49616245,\n",
       "       0.49616116, 0.49615988, 0.49615859, 0.49615731, 0.49615602,\n",
       "       0.49615473, 0.49615344, 0.49615215, 0.49615086, 0.49614957,\n",
       "       0.49614828, 0.49614699, 0.4961457 , 0.49614441, 0.49614311,\n",
       "       0.49614182, 0.49614052, 0.49613923, 0.49613793, 0.49613663,\n",
       "       0.49613534, 0.49613404, 0.49613274, 0.49613144, 0.49613014,\n",
       "       0.49612884, 0.49612754, 0.49612624, 0.49612493, 0.49612363,\n",
       "       0.49612233, 0.49612102, 0.49611972, 0.49611841, 0.4961171 ,\n",
       "       0.4961158 , 0.49611449, 0.49611318, 0.49611187, 0.49611056,\n",
       "       0.49610925, 0.49610794, 0.49610663, 0.49610531, 0.496104  ,\n",
       "       0.49610269, 0.49610137, 0.49610006, 0.49609874, 0.49609742,\n",
       "       0.49609611, 0.49609479, 0.49609347, 0.49609215, 0.49609083,\n",
       "       0.49608951, 0.49608819, 0.49608686, 0.49608554, 0.49608422,\n",
       "       0.49608289, 0.49608157, 0.49608024, 0.49607892, 0.49607759,\n",
       "       0.49607626, 0.49607493, 0.4960736 , 0.49607228, 0.49607094,\n",
       "       0.49606961, 0.49606828, 0.49606695, 0.49606562, 0.49606428,\n",
       "       0.49606295, 0.49606161, 0.49606028, 0.49605894, 0.4960576 ,\n",
       "       0.49605626, 0.49605493, 0.49605359, 0.49605225, 0.4960509 ,\n",
       "       0.49604956, 0.49604822, 0.49604688, 0.49604553, 0.49604419,\n",
       "       0.49604284, 0.4960415 , 0.49604015, 0.4960388 , 0.49603834,\n",
       "       0.49603854, 0.49603874, 0.49603894, 0.49603914, 0.49603934,\n",
       "       0.49603953, 0.49603976, 0.49604006, 0.49604035, 0.49604065,\n",
       "       0.49604094, 0.49604123, 0.49604152, 0.49604181, 0.4960421 ,\n",
       "       0.49604239, 0.49604268, 0.49604297, 0.49604325, 0.49604354,\n",
       "       0.49604382, 0.49604411, 0.49604439, 0.49604467, 0.49604495,\n",
       "       0.49604523, 0.49604534, 0.49604539, 0.49604544, 0.4960455 ,\n",
       "       0.49604555, 0.4960456 , 0.49604564, 0.49604569, 0.49604574,\n",
       "       0.49604578, 0.49604583, 0.49604587, 0.49604592, 0.49604596,\n",
       "       0.496046  , 0.49604604, 0.49604608, 0.49604611, 0.49604615,\n",
       "       0.49604619, 0.49604622, 0.49604626, 0.49604629, 0.49604632,\n",
       "       0.49604635, 0.49604638, 0.49604641, 0.49604644, 0.49604646,\n",
       "       0.49604649, 0.49604651, 0.49604654, 0.49604656, 0.49604658,\n",
       "       0.4960466 , 0.49604662, 0.49604664, 0.49604666, 0.49604667,\n",
       "       0.49604669, 0.4960467 , 0.49604672, 0.49604673, 0.49604674,\n",
       "       0.49604675, 0.49604676, 0.49604677, 0.49604677, 0.49604678,\n",
       "       0.49604678, 0.49604679, 0.49604679, 0.49604679, 0.49604679,\n",
       "       0.49604679, 0.49604679, 0.49604679, 0.49604678, 0.49604678,\n",
       "       0.49604677, 0.49604677, 0.49604676, 0.49604675, 0.49604674,\n",
       "       0.49604673, 0.49604671, 0.4960467 , 0.49604669, 0.49604667,\n",
       "       0.49604665, 0.49604664, 0.49604662, 0.4960466 , 0.49604658,\n",
       "       0.49604655, 0.49604653, 0.49604651, 0.49604648, 0.49604645,\n",
       "       0.49604643, 0.4960464 , 0.49604637, 0.49604634, 0.4960463 ,\n",
       "       0.49604627, 0.49604623, 0.4960462 , 0.49604616, 0.49604612,\n",
       "       0.49604609, 0.49604605, 0.496046  , 0.49604596, 0.49604592,\n",
       "       0.49604587, 0.49604527, 0.49604429, 0.49604332, 0.49604234,\n",
       "       0.49604137, 0.49604039, 0.49603942, 0.49603844, 0.49603746,\n",
       "       0.49603648, 0.4960355 , 0.49603452, 0.49603353, 0.49603255,\n",
       "       0.49603156, 0.49603058, 0.49602959, 0.4960286 , 0.49602761,\n",
       "       0.49602662, 0.49602563, 0.49602464, 0.49602365, 0.49602266,\n",
       "       0.49602166, 0.49602067, 0.49601967, 0.49601867, 0.49601767,\n",
       "       0.49601667, 0.49601567, 0.49601467, 0.49601367, 0.49601267,\n",
       "       0.49601166, 0.49601033, 0.49600899, 0.49600764, 0.4960063 ,\n",
       "       0.49600495, 0.4960036 , 0.49600225, 0.4960009 , 0.49599955,\n",
       "       0.49599819, 0.49599684, 0.49599548, 0.49599412, 0.49599277,\n",
       "       0.49599141, 0.49599005, 0.49598869, 0.49598732, 0.49598596,\n",
       "       0.4959846 , 0.49598323, 0.49598187, 0.4959805 , 0.49597913,\n",
       "       0.49597776, 0.49597666, 0.49597578, 0.4959749 , 0.49597402])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic = Critic()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49618411],\n",
       "       [0.49618338],\n",
       "       [0.4961832 ],\n",
       "       [0.49618292],\n",
       "       [0.49618164],\n",
       "       [0.49618037],\n",
       "       [0.49617909],\n",
       "       [0.49617781],\n",
       "       [0.49617654],\n",
       "       [0.49617526],\n",
       "       [0.49617398],\n",
       "       [0.4961727 ],\n",
       "       [0.49617142],\n",
       "       [0.49617014],\n",
       "       [0.49616886],\n",
       "       [0.49616758],\n",
       "       [0.4961663 ],\n",
       "       [0.49616502],\n",
       "       [0.49616373],\n",
       "       [0.49616245],\n",
       "       [0.49616116],\n",
       "       [0.49615988],\n",
       "       [0.49615859],\n",
       "       [0.49615731],\n",
       "       [0.49615602],\n",
       "       [0.49615473],\n",
       "       [0.49615344],\n",
       "       [0.49615215],\n",
       "       [0.49615086],\n",
       "       [0.49614957],\n",
       "       [0.49614828],\n",
       "       [0.49614699],\n",
       "       [0.4961457 ],\n",
       "       [0.49614441],\n",
       "       [0.49614311],\n",
       "       [0.49614182],\n",
       "       [0.49614052],\n",
       "       [0.49613923],\n",
       "       [0.49613793],\n",
       "       [0.49613663],\n",
       "       [0.49613534],\n",
       "       [0.49613404],\n",
       "       [0.49613274],\n",
       "       [0.49613144],\n",
       "       [0.49613014],\n",
       "       [0.49612884],\n",
       "       [0.49612754],\n",
       "       [0.49612624],\n",
       "       [0.49612493],\n",
       "       [0.49612363],\n",
       "       [0.49612233],\n",
       "       [0.49612102],\n",
       "       [0.49611972],\n",
       "       [0.49611841],\n",
       "       [0.4961171 ],\n",
       "       [0.4961158 ],\n",
       "       [0.49611449],\n",
       "       [0.49611318],\n",
       "       [0.49611187],\n",
       "       [0.49611056],\n",
       "       [0.49610925],\n",
       "       [0.49610794],\n",
       "       [0.49610663],\n",
       "       [0.49610531],\n",
       "       [0.496104  ],\n",
       "       [0.49610269],\n",
       "       [0.49610137],\n",
       "       [0.49610006],\n",
       "       [0.49609874],\n",
       "       [0.49609742],\n",
       "       [0.49609611],\n",
       "       [0.49609479],\n",
       "       [0.49609347],\n",
       "       [0.49609215],\n",
       "       [0.49609083],\n",
       "       [0.49608951],\n",
       "       [0.49608819],\n",
       "       [0.49608686],\n",
       "       [0.49608554],\n",
       "       [0.49608422],\n",
       "       [0.49608289],\n",
       "       [0.49608157],\n",
       "       [0.49608024],\n",
       "       [0.49607892],\n",
       "       [0.49607759],\n",
       "       [0.49607626],\n",
       "       [0.49607493],\n",
       "       [0.4960736 ],\n",
       "       [0.49607228],\n",
       "       [0.49607094],\n",
       "       [0.49606961],\n",
       "       [0.49606828],\n",
       "       [0.49606695],\n",
       "       [0.49606562],\n",
       "       [0.49606428],\n",
       "       [0.49606295],\n",
       "       [0.49606161],\n",
       "       [0.49606028],\n",
       "       [0.49605894],\n",
       "       [0.4960576 ],\n",
       "       [0.49605626],\n",
       "       [0.49605493],\n",
       "       [0.49605359],\n",
       "       [0.49605225],\n",
       "       [0.4960509 ],\n",
       "       [0.49604956],\n",
       "       [0.49604822],\n",
       "       [0.49604688],\n",
       "       [0.49604553],\n",
       "       [0.49604419],\n",
       "       [0.49604284],\n",
       "       [0.4960415 ],\n",
       "       [0.49604015],\n",
       "       [0.4960388 ],\n",
       "       [0.49603834],\n",
       "       [0.49603854],\n",
       "       [0.49603874],\n",
       "       [0.49603894],\n",
       "       [0.49603914],\n",
       "       [0.49603934],\n",
       "       [0.49603953],\n",
       "       [0.49603976],\n",
       "       [0.49604006],\n",
       "       [0.49604035],\n",
       "       [0.49604065],\n",
       "       [0.49604094],\n",
       "       [0.49604123],\n",
       "       [0.49604152],\n",
       "       [0.49604181],\n",
       "       [0.4960421 ],\n",
       "       [0.49604239],\n",
       "       [0.49604268],\n",
       "       [0.49604297],\n",
       "       [0.49604325],\n",
       "       [0.49604354],\n",
       "       [0.49604382],\n",
       "       [0.49604411],\n",
       "       [0.49604439],\n",
       "       [0.49604467],\n",
       "       [0.49604495],\n",
       "       [0.49604523],\n",
       "       [0.49604534],\n",
       "       [0.49604539],\n",
       "       [0.49604544],\n",
       "       [0.4960455 ],\n",
       "       [0.49604555],\n",
       "       [0.4960456 ],\n",
       "       [0.49604564],\n",
       "       [0.49604569],\n",
       "       [0.49604574],\n",
       "       [0.49604578],\n",
       "       [0.49604583],\n",
       "       [0.49604587],\n",
       "       [0.49604592],\n",
       "       [0.49604596],\n",
       "       [0.496046  ],\n",
       "       [0.49604604],\n",
       "       [0.49604608],\n",
       "       [0.49604611],\n",
       "       [0.49604615],\n",
       "       [0.49604619],\n",
       "       [0.49604622],\n",
       "       [0.49604626],\n",
       "       [0.49604629],\n",
       "       [0.49604632],\n",
       "       [0.49604635],\n",
       "       [0.49604638],\n",
       "       [0.49604641],\n",
       "       [0.49604644],\n",
       "       [0.49604646],\n",
       "       [0.49604649],\n",
       "       [0.49604651],\n",
       "       [0.49604654],\n",
       "       [0.49604656],\n",
       "       [0.49604658],\n",
       "       [0.4960466 ],\n",
       "       [0.49604662],\n",
       "       [0.49604664],\n",
       "       [0.49604666],\n",
       "       [0.49604667],\n",
       "       [0.49604669],\n",
       "       [0.4960467 ],\n",
       "       [0.49604672],\n",
       "       [0.49604673],\n",
       "       [0.49604674],\n",
       "       [0.49604675],\n",
       "       [0.49604676],\n",
       "       [0.49604677],\n",
       "       [0.49604677],\n",
       "       [0.49604678],\n",
       "       [0.49604678],\n",
       "       [0.49604679],\n",
       "       [0.49604679],\n",
       "       [0.49604679],\n",
       "       [0.49604679],\n",
       "       [0.49604679],\n",
       "       [0.49604679],\n",
       "       [0.49604679],\n",
       "       [0.49604678],\n",
       "       [0.49604678],\n",
       "       [0.49604677],\n",
       "       [0.49604677],\n",
       "       [0.49604676],\n",
       "       [0.49604675],\n",
       "       [0.49604674],\n",
       "       [0.49604673],\n",
       "       [0.49604671],\n",
       "       [0.4960467 ],\n",
       "       [0.49604669],\n",
       "       [0.49604667],\n",
       "       [0.49604665],\n",
       "       [0.49604664],\n",
       "       [0.49604662],\n",
       "       [0.4960466 ],\n",
       "       [0.49604658],\n",
       "       [0.49604655],\n",
       "       [0.49604653],\n",
       "       [0.49604651],\n",
       "       [0.49604648],\n",
       "       [0.49604645],\n",
       "       [0.49604643],\n",
       "       [0.4960464 ],\n",
       "       [0.49604637],\n",
       "       [0.49604634],\n",
       "       [0.4960463 ],\n",
       "       [0.49604627],\n",
       "       [0.49604623],\n",
       "       [0.4960462 ],\n",
       "       [0.49604616],\n",
       "       [0.49604612],\n",
       "       [0.49604609],\n",
       "       [0.49604605],\n",
       "       [0.496046  ],\n",
       "       [0.49604596],\n",
       "       [0.49604592],\n",
       "       [0.49604587],\n",
       "       [0.49604527],\n",
       "       [0.49604429],\n",
       "       [0.49604332],\n",
       "       [0.49604234],\n",
       "       [0.49604137],\n",
       "       [0.49604039],\n",
       "       [0.49603942],\n",
       "       [0.49603844],\n",
       "       [0.49603746],\n",
       "       [0.49603648],\n",
       "       [0.4960355 ],\n",
       "       [0.49603452],\n",
       "       [0.49603353],\n",
       "       [0.49603255],\n",
       "       [0.49603156],\n",
       "       [0.49603058],\n",
       "       [0.49602959],\n",
       "       [0.4960286 ],\n",
       "       [0.49602761],\n",
       "       [0.49602662],\n",
       "       [0.49602563],\n",
       "       [0.49602464],\n",
       "       [0.49602365],\n",
       "       [0.49602266],\n",
       "       [0.49602166],\n",
       "       [0.49602067],\n",
       "       [0.49601967],\n",
       "       [0.49601867],\n",
       "       [0.49601767],\n",
       "       [0.49601667],\n",
       "       [0.49601567],\n",
       "       [0.49601467],\n",
       "       [0.49601367],\n",
       "       [0.49601267],\n",
       "       [0.49601166],\n",
       "       [0.49601033],\n",
       "       [0.49600899],\n",
       "       [0.49600764],\n",
       "       [0.4960063 ],\n",
       "       [0.49600495],\n",
       "       [0.4960036 ],\n",
       "       [0.49600225],\n",
       "       [0.4960009 ],\n",
       "       [0.49599955],\n",
       "       [0.49599819],\n",
       "       [0.49599684],\n",
       "       [0.49599548],\n",
       "       [0.49599412],\n",
       "       [0.49599277],\n",
       "       [0.49599141],\n",
       "       [0.49599005],\n",
       "       [0.49598869],\n",
       "       [0.49598732],\n",
       "       [0.49598596],\n",
       "       [0.4959846 ],\n",
       "       [0.49598323],\n",
       "       [0.49598187],\n",
       "       [0.4959805 ],\n",
       "       [0.49597913],\n",
       "       [0.49597776],\n",
       "       [0.49597666],\n",
       "       [0.49597578],\n",
       "       [0.4959749 ],\n",
       "       [0.49597402]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
