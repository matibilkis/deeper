{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self,nature, valreg=0.01, seed_val=0.3, pad_value=-7., dolinar_layers=2, tau=0.01):\n",
    "        '''\n",
    "        dolinar_layers= number of photodetections\n",
    "        pad_value: value not considered by the lstm\n",
    "        valreg: regularisation value\n",
    "        seed_val: interval of random parameter inizialitaion.\n",
    "        '''\n",
    "        super(Critic,self).__init__()\n",
    "\n",
    "        self.pad_value = pad_value\n",
    "        self.nature = nature\n",
    "        self.dolinar_layers = dolinar_layers\n",
    "        self.mask = tf.keras.layers.Masking(mask_value=pad_value,\n",
    "                                  input_shape=(self.dolinar_layers, 2)) #(beta1, pad), (n1, beta2), (n2, guess). In general i will have (layer+1)\n",
    "        self.lstm = tf.keras.layers.LSTM(500, return_sequences=True)\n",
    "\n",
    "        self.tau = tau\n",
    "        self.l1 = Dense(250,kernel_initializer=tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val),\n",
    "        bias_initializer = tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val),\n",
    "        kernel_regularizer=tf.keras.regularizers.l1(valreg),\n",
    "    activity_regularizer=tf.keras.regularizers.l2(valreg))\n",
    "\n",
    "        self.l2 = Dense(100, kernel_regularizer=tf.keras.regularizers.l1(valreg),\n",
    "    activity_regularizer=tf.keras.regularizers.l2(valreg),\n",
    "    kernel_initializer=tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val),\n",
    "    bias_initializer = tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val))\n",
    "\n",
    "        self.l3 = Dense(100, kernel_regularizer=tf.keras.regularizers.l1(valreg),\n",
    "    activity_regularizer=tf.keras.regularizers.l2(valreg),\n",
    "    kernel_initializer=tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val),\n",
    "    bias_initializer = tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val))\n",
    "\n",
    "        self.l4 = Dense(1, kernel_regularizer=tf.keras.regularizers.l1(valreg),\n",
    "    activity_regularizer=tf.keras.regularizers.l2(valreg),\n",
    "    kernel_initializer=tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val),\n",
    "    bias_initializer = tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val))\n",
    "\n",
    "\n",
    "\n",
    "    def update_target_parameters(self,primary_net):\n",
    "        #### only\n",
    "        # for i,j in zip(self.get_weights(), primary_net.get_weights()):\n",
    "        #     tf.assign(i, tau*j + (i-tau)*i )\n",
    "        prim_weights = primary_net.get_weights()\n",
    "        targ_weights = self.get_weights()\n",
    "        weights = []\n",
    "        for i in tf.range(len(prim_weights)):\n",
    "            weights.append(self.tau * prim_weights[i] + (1 - self.tau) * targ_weights[i])\n",
    "        self.set_weights(weights)\n",
    "        return\n",
    "\n",
    "    def call(self, inputs):\n",
    "        feat = self.mask(inputs)\n",
    "        feat= self.lstm(feat)\n",
    "        # feat = tf.nn.dropout(feat, rate=0.01)\n",
    "        feat = tf.nn.relu(self.l1(feat))\n",
    "        # feat = tf.nn.dropout(feat, rate=0.01)\n",
    "        feat = tf.nn.relu(self.l2(feat))\n",
    "        feat = tf.nn.relu(self.l3(feat))\n",
    "        feat = tf.nn.sigmoid(self.l4(feat))\n",
    "        return feat\n",
    "\n",
    "\n",
    "    def process_sequence(self,sample_buffer):\n",
    "        \"\"\"\"\n",
    "        sample_buffer: array of shape (N,2*self.layers +1), N>1\n",
    "\n",
    "        gets data obtained from N experiments: data.shape = (N, 2L+1),\n",
    "        where +1 accounts for the guess and 2L for (beta, outcome).\n",
    "\n",
    "        [[a0, o1, a1, o2, a2, o3, a4]\n",
    "         [same but other experiment]\n",
    "        ]\n",
    "\n",
    "        and returns an array of shape (experiments, self.layers, 2 ), as accepted by an RNN\n",
    "        \"\"\"\n",
    "        batch_size = sample_buffer.shape[0]\n",
    "        data = sample_buffer[:,0:(self.dolinar_layers+1+1)]\n",
    "        padded_data = np.ones((batch_size,self.dolinar_layers+1, 2))*self.pad_value\n",
    "        padded_data[:,0][:,0] = data[:,0]\n",
    "        for k in range(1,self.dolinar_layers+1):\n",
    "            padded_data[:,k] = data[:,[k,k+1]]\n",
    "\n",
    "        rewards_obtained = np.zeros((batch_size, self.dolinar_layers+1))\n",
    "        rewards_obtained[:,-1] = sample_buffer[:,-1]\n",
    "        return padded_data, rewards_obtained\n",
    "\n",
    "\n",
    "    def pad_single_sequence(self, seq):\n",
    "        \"\"\"\"\n",
    "        input: [a0, o1, a1, o2, a2, o3, a4]\n",
    "\n",
    "        output: [[a0, pad], [o1, a1], [...]]\n",
    "\n",
    "        the cool thing is that then you can put this to predict the greedy guess/action.\n",
    "        \"\"\"\n",
    "        padded_data = np.ones((1,self.dolinar_layers+1, 2))*self.pad_value\n",
    "        padded_data[0][0][0] = seq[0]\n",
    "        #padded_data[0][0] = data[0]\n",
    "        for k in range(1,self.dolinar_layers+1):\n",
    "            padded_data[0][k] = seq[k:(k+2)]\n",
    "        return padded_data\n",
    "\n",
    "    def give_td_error_Kennedy_guess(self,batched_input,sequential_rews_with_zeros):\n",
    "        # this function takes as input the actions as given by the target actor (but the first one!)\n",
    "        #and outpus the correspoindg TD-errors for DDPG! To obtain them from sample of buffer\n",
    "        #you call the method targeted_sequence from the actor_target and then the process_sequence\n",
    "        #of this critic network.\n",
    "        if self.nature != \"target\":\n",
    "            raise AttributeError(\"I'm not the target!\")\n",
    "            return\n",
    "        b = batched_input.copy()\n",
    "        ll = sequential_rews_with_zeros.copy()\n",
    "        for k in range(0,self.dolinar_layers-1):\n",
    "            print(k)\n",
    "            ll[:,k] = np.squeeze(self(b))[:,k+1] + ll[:,k]\n",
    "\n",
    "        preds1 = self(b)\n",
    "        b[:,-1][:,-1] = -b[:,1][:,1]\n",
    "        preds2 = self(b)\n",
    "        both = tf.concat([preds1,preds2],2)\n",
    "        maxs = np.squeeze(tf.math.reduce_max(both,axis=2).numpy())\n",
    "        ll[:,-2] = maxs[:,1] # This is the last befre the guess.. so the label is max_g Q(h-L, g)\n",
    "        ll = np.expand_dims(ll,axis=1)\n",
    "        return ll\n",
    "\n",
    "\n",
    "    def give_favourite_guess(self,sequence_with_plus):\n",
    "        \"\"\"\"\n",
    "            important !! the 1!\n",
    "        sequence should be [[beta, pad], [outcome, 1]] \"\"\"\n",
    "        pred_1 = self(sequence_with_plus)\n",
    "        sequence_with_plus[:,1][:,1] = -sequence_with_plus[:,1][:,1]\n",
    "        pred_2 = self(sequence_with_plus)\n",
    "        both = tf.concat([pred_1,pred_2],2)\n",
    "        maxs = np.squeeze(tf.argmax(both,axis=2).numpy())[1]\n",
    "\n",
    "        guess = (-1)**maxs\n",
    "        return  guess\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(tf.keras.Model):\n",
    "    def __init__(self, nature, valreg=0.01, seed_val=0.1, pad_value = -7.,\n",
    "                 dolinar_layers=2,tau=0.01):\n",
    "        super(Actor,self).__init__()\n",
    "        self.dolinar_layers = dolinar_layers\n",
    "        self.pad_value = pad_value\n",
    "        self.nature = nature\n",
    "        self.tau = tau\n",
    "\n",
    "        if nature == \"primary\":\n",
    "            self.lstm = tf.keras.layers.LSTM(500, return_sequences=True, stateful=True)\n",
    "            self.mask = tf.keras.layers.Masking(mask_value=pad_value,\n",
    "                                  input_shape=(1,None,1), dynamic=True)\n",
    "        elif nature == \"target\":\n",
    "            self.lstm = tf.keras.layers.LSTM(500, return_sequences=True, stateful=False)\n",
    "            self.mask = tf.keras.layers.Masking(mask_value=pad_value,\n",
    "                                  input_shape=(self.dolinar_layers, 1)) #'cause i feed altoghether.\n",
    "        else:\n",
    "            print(\"Hey! the character is either primary or target\")\n",
    "        self.l1 = Dense(250,kernel_initializer=tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val),\n",
    "        bias_initializer = tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val),\n",
    "        kernel_regularizer=tf.keras.regularizers.l1(valreg),\n",
    "    activity_regularizer=tf.keras.regularizers.l2(valreg))\n",
    "\n",
    "        self.l2 = Dense(100, kernel_regularizer=tf.keras.regularizers.l1(valreg),\n",
    "    activity_regularizer=tf.keras.regularizers.l2(valreg),\n",
    "    kernel_initializer=tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val),\n",
    "    bias_initializer = tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val))\n",
    "\n",
    "        self.l3 = Dense(100, kernel_regularizer=tf.keras.regularizers.l1(valreg),\n",
    "    activity_regularizer=tf.keras.regularizers.l2(valreg),\n",
    "    kernel_initializer=tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val),\n",
    "    bias_initializer = tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val))\n",
    "\n",
    "        self.l4 = Dense(1, kernel_regularizer=tf.keras.regularizers.l1(valreg),\n",
    "    activity_regularizer=tf.keras.regularizers.l2(valreg),\n",
    "    kernel_initializer=tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val),\n",
    "    bias_initializer = tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val))\n",
    "\n",
    "\n",
    "\n",
    "    def update_target_parameters(self,primary_net):\n",
    "        #### only\n",
    "        # for i,j in zip(self.get_weights(), primary_net.get_weights()):\n",
    "        #     tf.assign(i, tau*j + (i-tau)*i )\n",
    "        prim_weights = primary_net.get_weights()\n",
    "        targ_weights = self.get_weights()\n",
    "        weights = []\n",
    "        for i in tf.range(len(prim_weights)):\n",
    "            weights.append(self.tau * prim_weights[i] + (1 - self.tau) * targ_weights[i])\n",
    "        self.set_weights(weights)\n",
    "        return\n",
    "\n",
    "    def call(self, inputs):\n",
    "        feat = self.mask(inputs)\n",
    "        feat= self.lstm(feat)\n",
    "        # feat = tf.nn.dropout(feat, rate=0.01)\n",
    "        feat = tf.nn.relu(self.l1(feat))\n",
    "        # feat = tf.nn.dropout(feat, rate=0.01)\n",
    "        feat = tf.nn.relu(self.l2(feat))\n",
    "        feat = tf.nn.relu(self.l3(feat))\n",
    "        feat = tf.nn.sigmoid(self.l4(feat))\n",
    "\n",
    "        return feat\n",
    "\n",
    "    def process_sequence_of_experiences(self, experiences):\n",
    "\n",
    "        #This function takes a vector of experiences:\n",
    "        #vector = (\\beta1, o1, \\beta2, o2, \\beta3, o3,...,o_L, guess)\n",
    "        #and retrieves\n",
    "        #(\\beta1, o1, \\beta2_target, o2, \\beta3_target, o3, \\beta4_target,... ,o_L, guess)\n",
    "\n",
    "        #For the primary it should give again the actions that generated the experience (this is to consider the wegiths\n",
    "        #in the graph)\n",
    "\n",
    "        #For the target it gives the \"opinion\" of the actions it should've taken...\n",
    "\n",
    "        # if self.nature != \"target\":\n",
    "        #     raise AttributeError(\"check the lstm memory of actor target, stateful == True ?\")\n",
    "        #     return\n",
    "        export = experiences.copy()\n",
    "        for index in range(1,2*self.dolinar_layers-1,2): # I consider from first outcome to last one (but guess)\n",
    "            export[:,index+1] = np.squeeze(self(np.reshape(np.array(export[:,index]),\n",
    "                                                                 (experiences.shape[0],1,1))))\n",
    "        return export\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "experinces = np.load(\"expe_2L.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = Actor(nature=\"primary\")\n",
    "actor.mask.get_config()\n",
    "actor.lstm.stateful=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer actor_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(100, 1, 1), dtype=float32, numpy=\n",
       "array([[[0.50762874]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.5076383 ]],\n",
       "\n",
       "       [[0.5076383 ]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.5076383 ]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.5076383 ]],\n",
       "\n",
       "       [[0.5076383 ]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.5076383 ]],\n",
       "\n",
       "       [[0.5076383 ]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.5076383 ]],\n",
       "\n",
       "       [[0.5076383 ]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.5076383 ]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.5076383 ]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.5076383 ]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.5076383 ]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.5076383 ]],\n",
       "\n",
       "       [[0.5076383 ]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.5076383 ]],\n",
       "\n",
       "       [[0.5076383 ]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.5076383 ]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.5076383 ]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.5076383 ]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.5076383 ]],\n",
       "\n",
       "       [[0.5076383 ]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.5076383 ]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.5076383 ]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.5076383 ]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.5076383 ]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.5076383 ]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.5076383 ]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.50762874]],\n",
       "\n",
       "       [[0.5076383 ]],\n",
       "\n",
       "       [[0.5076383 ]],\n",
       "\n",
       "       [[0.5076287 ]],\n",
       "\n",
       "       [[0.5076287 ]],\n",
       "\n",
       "       [[0.5076287 ]],\n",
       "\n",
       "       [[0.5076383 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor(np.reshape(experinces[:,1], (len(experinces[:,1]), 1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 1, 1), dtype=float32, numpy=\n",
       "array([[[0.505339]],\n",
       "\n",
       "       [[0.505339]],\n",
       "\n",
       "       [[0.505339]],\n",
       "\n",
       "       [[0.505339]]], dtype=float32)>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor(np.reshape(experinces[:,1][:4], (len(experinces[:,1][:4]), 1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'masking_6',\n",
       " 'trainable': True,\n",
       " 'batch_input_shape': (None, None),\n",
       " 'dtype': 'float32',\n",
       " 'dynamic': True,\n",
       " 'mask_value': -7.0}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor.mask.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on bool object:\n",
      "\n",
      "class bool(int)\n",
      " |  bool(x) -> bool\n",
      " |  \n",
      " |  Returns True when the argument x is true, False otherwise.\n",
      " |  The builtins True and False are the only two instances of the class bool.\n",
      " |  The class bool is a subclass of the class int, and cannot be subclassed.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      bool\n",
      " |      int\n",
      " |      object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __and__(self, value, /)\n",
      " |      Return self&value.\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  __or__(self, value, /)\n",
      " |      Return self|value.\n",
      " |  \n",
      " |  __rand__(self, value, /)\n",
      " |      Return value&self.\n",
      " |  \n",
      " |  __repr__(self, /)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __ror__(self, value, /)\n",
      " |      Return value|self.\n",
      " |  \n",
      " |  __rxor__(self, value, /)\n",
      " |      Return value^self.\n",
      " |  \n",
      " |  __str__(self, /)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  __xor__(self, value, /)\n",
      " |      Return self^value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from int:\n",
      " |  \n",
      " |  __abs__(self, /)\n",
      " |      abs(self)\n",
      " |  \n",
      " |  __add__(self, value, /)\n",
      " |      Return self+value.\n",
      " |  \n",
      " |  __bool__(self, /)\n",
      " |      self != 0\n",
      " |  \n",
      " |  __ceil__(...)\n",
      " |      Ceiling of an Integral returns itself.\n",
      " |  \n",
      " |  __divmod__(self, value, /)\n",
      " |      Return divmod(self, value).\n",
      " |  \n",
      " |  __eq__(self, value, /)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __float__(self, /)\n",
      " |      float(self)\n",
      " |  \n",
      " |  __floor__(...)\n",
      " |      Flooring an Integral returns itself.\n",
      " |  \n",
      " |  __floordiv__(self, value, /)\n",
      " |      Return self//value.\n",
      " |  \n",
      " |  __format__(...)\n",
      " |      default object formatter\n",
      " |  \n",
      " |  __ge__(self, value, /)\n",
      " |      Return self>=value.\n",
      " |  \n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |  \n",
      " |  __getnewargs__(...)\n",
      " |  \n",
      " |  __gt__(self, value, /)\n",
      " |      Return self>value.\n",
      " |  \n",
      " |  __hash__(self, /)\n",
      " |      Return hash(self).\n",
      " |  \n",
      " |  __index__(self, /)\n",
      " |      Return self converted to an integer, if self is suitable for use as an index into a list.\n",
      " |  \n",
      " |  __int__(self, /)\n",
      " |      int(self)\n",
      " |  \n",
      " |  __invert__(self, /)\n",
      " |      ~self\n",
      " |  \n",
      " |  __le__(self, value, /)\n",
      " |      Return self<=value.\n",
      " |  \n",
      " |  __lshift__(self, value, /)\n",
      " |      Return self<<value.\n",
      " |  \n",
      " |  __lt__(self, value, /)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __mod__(self, value, /)\n",
      " |      Return self%value.\n",
      " |  \n",
      " |  __mul__(self, value, /)\n",
      " |      Return self*value.\n",
      " |  \n",
      " |  __ne__(self, value, /)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __neg__(self, /)\n",
      " |      -self\n",
      " |  \n",
      " |  __pos__(self, /)\n",
      " |      +self\n",
      " |  \n",
      " |  __pow__(self, value, mod=None, /)\n",
      " |      Return pow(self, value, mod).\n",
      " |  \n",
      " |  __radd__(self, value, /)\n",
      " |      Return value+self.\n",
      " |  \n",
      " |  __rdivmod__(self, value, /)\n",
      " |      Return divmod(value, self).\n",
      " |  \n",
      " |  __rfloordiv__(self, value, /)\n",
      " |      Return value//self.\n",
      " |  \n",
      " |  __rlshift__(self, value, /)\n",
      " |      Return value<<self.\n",
      " |  \n",
      " |  __rmod__(self, value, /)\n",
      " |      Return value%self.\n",
      " |  \n",
      " |  __rmul__(self, value, /)\n",
      " |      Return value*self.\n",
      " |  \n",
      " |  __round__(...)\n",
      " |      Rounding an Integral returns itself.\n",
      " |      Rounding with an ndigits argument also returns an integer.\n",
      " |  \n",
      " |  __rpow__(self, value, mod=None, /)\n",
      " |      Return pow(value, self, mod).\n",
      " |  \n",
      " |  __rrshift__(self, value, /)\n",
      " |      Return value>>self.\n",
      " |  \n",
      " |  __rshift__(self, value, /)\n",
      " |      Return self>>value.\n",
      " |  \n",
      " |  __rsub__(self, value, /)\n",
      " |      Return value-self.\n",
      " |  \n",
      " |  __rtruediv__(self, value, /)\n",
      " |      Return value/self.\n",
      " |  \n",
      " |  __sizeof__(...)\n",
      " |      Returns size in memory, in bytes\n",
      " |  \n",
      " |  __sub__(self, value, /)\n",
      " |      Return self-value.\n",
      " |  \n",
      " |  __truediv__(self, value, /)\n",
      " |      Return self/value.\n",
      " |  \n",
      " |  __trunc__(...)\n",
      " |      Truncating an Integral returns itself.\n",
      " |  \n",
      " |  bit_length(...)\n",
      " |      int.bit_length() -> int\n",
      " |      \n",
      " |      Number of bits necessary to represent self in binary.\n",
      " |      >>> bin(37)\n",
      " |      '0b100101'\n",
      " |      >>> (37).bit_length()\n",
      " |      6\n",
      " |  \n",
      " |  conjugate(...)\n",
      " |      Returns self, the complex conjugate of any int.\n",
      " |  \n",
      " |  from_bytes(...) from builtins.type\n",
      " |      int.from_bytes(bytes, byteorder, *, signed=False) -> int\n",
      " |      \n",
      " |      Return the integer represented by the given array of bytes.\n",
      " |      \n",
      " |      The bytes argument must be a bytes-like object (e.g. bytes or bytearray).\n",
      " |      \n",
      " |      The byteorder argument determines the byte order used to represent the\n",
      " |      integer.  If byteorder is 'big', the most significant byte is at the\n",
      " |      beginning of the byte array.  If byteorder is 'little', the most\n",
      " |      significant byte is at the end of the byte array.  To request the native\n",
      " |      byte order of the host system, use `sys.byteorder' as the byte order value.\n",
      " |      \n",
      " |      The signed keyword-only argument indicates whether two's complement is\n",
      " |      used to represent the integer.\n",
      " |  \n",
      " |  to_bytes(...)\n",
      " |      int.to_bytes(length, byteorder, *, signed=False) -> bytes\n",
      " |      \n",
      " |      Return an array of bytes representing an integer.\n",
      " |      \n",
      " |      The integer is represented using length bytes.  An OverflowError is\n",
      " |      raised if the integer is not representable with the given number of\n",
      " |      bytes.\n",
      " |      \n",
      " |      The byteorder argument determines the byte order used to represent the\n",
      " |      integer.  If byteorder is 'big', the most significant byte is at the\n",
      " |      beginning of the byte array.  If byteorder is 'little', the most\n",
      " |      significant byte is at the end of the byte array.  To request the native\n",
      " |      byte order of the host system, use `sys.byteorder' as the byte order value.\n",
      " |      \n",
      " |      The signed keyword-only argument determines whether two's complement is\n",
      " |      used to represent the integer.  If signed is False and a negative integer\n",
      " |      is given, an OverflowError is raised.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from int:\n",
      " |  \n",
      " |  denominator\n",
      " |      the denominator of a rational number in lowest terms\n",
      " |  \n",
      " |  imag\n",
      " |      the imaginary part of a complex number\n",
      " |  \n",
      " |  numerator\n",
      " |      the numerator of a rational number in lowest terms\n",
      " |  \n",
      " |  real\n",
      " |      the real part of a complex number\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(actor.mask.dynamic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor.mask = tf.keras.layers.Masking(mask_value=actor.pad_value,\n",
    "                                  input_shape=(1,1), dynamic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Tensor's shape (1, 100, 500) is not compatible with supplied shape [1, 1, 500]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-eafc7c586009>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mactor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    967\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-ddfe8da4ea27>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mfeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mfeat\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;31m# feat = tf.nn.dropout(feat, rate=0.01)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mfeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m     \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    967\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent_v2.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask, training, initial_state)\u001b[0m\n\u001b[1;32m   1182\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m           last_output, outputs, new_h, new_c, runtime = standard_lstm(\n\u001b[0;32m-> 1184\u001b[0;31m               **normal_lstm_kwargs)\n\u001b[0m\u001b[1;32m   1185\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m         (last_output, outputs, new_h, new_c,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent_v2.py\u001b[0m in \u001b[0;36mstandard_lstm\u001b[0;34m(inputs, init_h, init_c, kernel, recurrent_kernel, bias, activation, recurrent_activation, mask, time_major, go_backwards, sequence_lengths, zero_output_for_mask)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       input_length=(sequence_lengths\n\u001b[1;32m   1319\u001b[0m                     if sequence_lengths is not None else timesteps),\n\u001b[0;32m-> 1320\u001b[0;31m       zero_output_for_mask=zero_output_for_mask)\n\u001b[0m\u001b[1;32m   1321\u001b[0m   return (last_output, outputs, new_states[0], new_states[1],\n\u001b[1;32m   1322\u001b[0m           _runtime(_RUNTIME_CPU))\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mrnn\u001b[0;34m(step_function, inputs, initial_states, go_backwards, mask, constants, unroll, input_length, time_major, zero_output_for_mask)\u001b[0m\n\u001b[1;32m   4257\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4259\u001b[0;31m   \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4261\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtime_major\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mset_shape\u001b[0;34m(output_)\u001b[0m\n\u001b[1;32m   4254\u001b[0m       \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4255\u001b[0m       \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4256\u001b[0;31m       \u001b[0moutput_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4257\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shape\u001b[0;34m(self, shape)\u001b[0m\n\u001b[1;32m   1105\u001b[0m       raise ValueError(\n\u001b[1;32m   1106\u001b[0m           \u001b[0;34m\"Tensor's shape %s is not compatible with supplied shape %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1107\u001b[0;31m           (self.shape, shape))\n\u001b[0m\u001b[1;32m   1108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m   \u001b[0;31m# Methods not supported / implemented for Eager Tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor's shape (1, 100, 500) is not compatible with supplied shape [1, 1, 500]"
     ]
    }
   ],
   "source": [
    "actor(np.reshape(np.array([0.]), (1,1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.69102163,  0.        ,  0.83695079,  0.        , -1.        ,\n",
       "         1.        ],\n",
       "       [ 0.98145692,  0.        ,  0.04985608,  0.        , -1.        ,\n",
       "         1.        ],\n",
       "       [ 0.69588898,  0.        ,  0.30091034,  0.        ,  1.        ,\n",
       "         0.        ],\n",
       "       [ 0.65480695,  0.        ,  0.72952473,  1.        , -1.        ,\n",
       "         0.        ],\n",
       "       [ 0.07486903,  0.        ,  0.39324444,  1.        ,  1.        ,\n",
       "         1.        ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experinces[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor.lstm.stateful = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer actor_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(100, 1, 1), dtype=float32, numpy=\n",
       "array([[[0.49875805]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49860242]],\n",
       "\n",
       "       [[0.49860242]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49860242]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49860242]],\n",
       "\n",
       "       [[0.49860242]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49860242]],\n",
       "\n",
       "       [[0.49860242]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49860242]],\n",
       "\n",
       "       [[0.49860242]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49860242]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49860242]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49860242]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49860242]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49860242]],\n",
       "\n",
       "       [[0.49860242]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49860242]],\n",
       "\n",
       "       [[0.49860242]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49860242]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49860242]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49860242]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49860242]],\n",
       "\n",
       "       [[0.49860242]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49860242]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49860242]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49860242]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49860242]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49860242]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49860242]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49875805]],\n",
       "\n",
       "       [[0.49860242]],\n",
       "\n",
       "       [[0.49860242]],\n",
       "\n",
       "       [[0.49875808]],\n",
       "\n",
       "       [[0.49875808]],\n",
       "\n",
       "       [[0.49875808]],\n",
       "\n",
       "       [[0.49860242]]], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor(np.reshape(experinces[:,1], (len(experinces[:,1]), 1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Incompatible shapes: [5,2000] vs. [100,2000] [Op:AddV2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-4d43b0a9182b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mactor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperinces\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperinces\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    967\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-4fc19da5c8aa>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mfeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0mfeat\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;31m# feat = tf.nn.dropout(feat, rate=0.01)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0mfeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m     \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    967\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent_v2.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask, training, initial_state)\u001b[0m\n\u001b[1;32m   1182\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m           last_output, outputs, new_h, new_c, runtime = standard_lstm(\n\u001b[0;32m-> 1184\u001b[0;31m               **normal_lstm_kwargs)\n\u001b[0m\u001b[1;32m   1185\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m         (last_output, outputs, new_h, new_c,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent_v2.py\u001b[0m in \u001b[0;36mstandard_lstm\u001b[0;34m(inputs, init_h, init_c, kernel, recurrent_kernel, bias, activation, recurrent_activation, mask, time_major, go_backwards, sequence_lengths, zero_output_for_mask)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       input_length=(sequence_lengths\n\u001b[1;32m   1319\u001b[0m                     if sequence_lengths is not None else timesteps),\n\u001b[0;32m-> 1320\u001b[0;31m       zero_output_for_mask=zero_output_for_mask)\n\u001b[0m\u001b[1;32m   1321\u001b[0m   return (last_output, outputs, new_states[0], new_states[1],\n\u001b[1;32m   1322\u001b[0m           _runtime(_RUNTIME_CPU))\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mrnn\u001b[0;34m(step_function, inputs, initial_states, go_backwards, mask, constants, unroll, input_length, time_major, zero_output_for_mask)\u001b[0m\n\u001b[1;32m   4090\u001b[0m     \u001b[0;31m# the value is discarded.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4091\u001b[0m     output_time_zero, _ = step_function(\n\u001b[0;32m-> 4092\u001b[0;31m         input_time_zero, tuple(initial_states) + tuple(constants))\n\u001b[0m\u001b[1;32m   4093\u001b[0m     output_ta = tuple(\n\u001b[1;32m   4094\u001b[0m         tensor_array_ops.TensorArray(\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent_v2.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(cell_inputs, cell_states)\u001b[0m\n\u001b[1;32m   1295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m     \u001b[0mz\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_tm1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurrent_kernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1298\u001b[0m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    982\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 984\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    985\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36m_add_dispatch\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1274\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36madd_v2\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6651\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6652\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6653\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6654\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [5,2000] vs. [100,2000] [Op:AddV2]"
     ]
    }
   ],
   "source": [
    "actor(np.reshape(experinces[:,1][:5], (len(experinces[:,1][:5]), 1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.69102163,  0.        ,  0.49785164,  0.        , -1.        ,\n",
       "         1.        ],\n",
       "       [ 0.98145692,  0.        ,  0.49785164,  0.        , -1.        ,\n",
       "         1.        ],\n",
       "       [ 0.69588898,  0.        ,  0.49785164,  0.        ,  1.        ,\n",
       "         0.        ],\n",
       "       [ 0.65480695,  0.        ,  0.49785164,  1.        , -1.        ,\n",
       "         0.        ],\n",
       "       [ 0.07486903,  0.        ,  0.49785164,  1.        ,  1.        ,\n",
       "         1.        ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor.process_sequence_of_experiences(experinces)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.reshape(experinces[:,1],(100,1,1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(100, 1, 1), dtype=float32, numpy=\n",
       "array([[[0.49785164]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49676186]],\n",
       "\n",
       "       [[0.49676186]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49676186]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49676186]],\n",
       "\n",
       "       [[0.49676186]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49676186]],\n",
       "\n",
       "       [[0.49676186]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49676186]],\n",
       "\n",
       "       [[0.49676186]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49676186]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49676186]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49676186]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49676186]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49676186]],\n",
       "\n",
       "       [[0.49676186]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49676186]],\n",
       "\n",
       "       [[0.49676186]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49676186]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49676186]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49676186]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49676186]],\n",
       "\n",
       "       [[0.49676186]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49676186]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49676186]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49676186]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49676186]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49676186]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49676186]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49676186]],\n",
       "\n",
       "       [[0.49676186]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.49785164]],\n",
       "\n",
       "       [[0.4967618 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor(np.reshape(experinces[:,1],(100,1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'masking_1',\n",
       " 'trainable': True,\n",
       " 'batch_input_shape': (None, 1, 1),\n",
       " 'dtype': 'float32',\n",
       " 'mask_value': -7.0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor.mask.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor(np.reshape(experinces[:,1],(100,1,1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
