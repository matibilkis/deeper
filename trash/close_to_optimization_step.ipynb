{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self,nature, valreg=0.01, seed_val=0.3, pad_value=-7., dolinar_layers=2, tau=0.01):\n",
    "        '''\n",
    "        dolinar_layers= number of photodetections\n",
    "        pad_value: value not considered by the lstm\n",
    "        valreg: regularisation value\n",
    "        seed_val: interval of random parameter inizialitaion.\n",
    "        '''\n",
    "        super(Critic,self).__init__()\n",
    "\n",
    "        self.pad_value = pad_value\n",
    "        self.nature = nature\n",
    "        self.dolinar_layers = dolinar_layers\n",
    "        self.mask = tf.keras.layers.Masking(mask_value=pad_value,\n",
    "                                  input_shape=(self.dolinar_layers, 2)) #(beta1, pad), (n1, beta2), (n2, guess). In general i will have (layer+1)\n",
    "        self.lstm = tf.keras.layers.LSTM(500, return_sequences=True)\n",
    "\n",
    "        self.tau = tau\n",
    "        self.l1 = Dense(250,kernel_initializer=tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val),\n",
    "        bias_initializer = tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val),\n",
    "        kernel_regularizer=tf.keras.regularizers.l1(valreg),\n",
    "    activity_regularizer=tf.keras.regularizers.l2(valreg))\n",
    "\n",
    "        self.l2 = Dense(100, kernel_regularizer=tf.keras.regularizers.l1(valreg),\n",
    "    activity_regularizer=tf.keras.regularizers.l2(valreg),\n",
    "    kernel_initializer=tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val),\n",
    "    bias_initializer = tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val))\n",
    "\n",
    "        self.l3 = Dense(100, kernel_regularizer=tf.keras.regularizers.l1(valreg),\n",
    "    activity_regularizer=tf.keras.regularizers.l2(valreg),\n",
    "    kernel_initializer=tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val),\n",
    "    bias_initializer = tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val))\n",
    "\n",
    "        self.l4 = Dense(1, kernel_regularizer=tf.keras.regularizers.l1(valreg),\n",
    "    activity_regularizer=tf.keras.regularizers.l2(valreg),\n",
    "    kernel_initializer=tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val),\n",
    "    bias_initializer = tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val))\n",
    "\n",
    "\n",
    "\n",
    "    def update_target_parameters(self,primary_net):\n",
    "        #### only\n",
    "        # for i,j in zip(self.get_weights(), primary_net.get_weights()):\n",
    "        #     tf.assign(i, tau*j + (i-tau)*i )\n",
    "        prim_weights = primary_net.get_weights()\n",
    "        targ_weights = self.get_weights()\n",
    "        weights = []\n",
    "        for i in tf.range(len(prim_weights)):\n",
    "            weights.append(self.tau * prim_weights[i] + (1 - self.tau) * targ_weights[i])\n",
    "        self.set_weights(weights)\n",
    "        return\n",
    "\n",
    "    def call(self, inputs):\n",
    "        feat = self.mask(inputs)\n",
    "        feat= self.lstm(feat)\n",
    "        # feat = tf.nn.dropout(feat, rate=0.01)\n",
    "        feat = tf.nn.relu(self.l1(feat))\n",
    "        # feat = tf.nn.dropout(feat, rate=0.01)\n",
    "        feat = tf.nn.relu(self.l2(feat))\n",
    "        feat = tf.nn.relu(self.l3(feat))\n",
    "        feat = tf.nn.sigmoid(self.l4(feat))\n",
    "        return feat\n",
    "\n",
    "\n",
    "    def process_sequence(self,sample_buffer):\n",
    "        \"\"\"\"\n",
    "        sample_buffer: array of shape (N,2*self.layers +1), N>1\n",
    "\n",
    "        gets data obtained from N experiments: data.shape = (N, 2L+1),\n",
    "        where +1 accounts for the guess and 2L for (beta, outcome).\n",
    "\n",
    "        [[a0, o1, a1, o2, a2, o3, a4]\n",
    "         [same but other experiment]\n",
    "        ]\n",
    "\n",
    "        and returns an array of shape (experiments, self.layers, 2 ), as accepted by an RNN\n",
    "        \"\"\"\n",
    "        batch_size = sample_buffer.shape[0]\n",
    "        data = sample_buffer[:,0:(self.dolinar_layers+1+1)]\n",
    "        padded_data = np.ones((batch_size,self.dolinar_layers+1, 2))*self.pad_value\n",
    "        padded_data[:,0][:,0] = data[:,0]\n",
    "        for k in range(1,self.dolinar_layers+1):\n",
    "            padded_data[:,k] = data[:,[k,k+1]]\n",
    "\n",
    "        rewards_obtained = np.zeros((batch_size, self.dolinar_layers+1))\n",
    "        rewards_obtained[:,-1] = sample_buffer[:,-1]\n",
    "        return padded_data, rewards_obtained\n",
    "\n",
    "\n",
    "    def pad_single_sequence(self, seq):\n",
    "        \"\"\"\"\n",
    "        input: [a0, o1, a1, o2, a2, o3, a4]\n",
    "\n",
    "        output: [[a0, pad], [o1, a1], [...]]\n",
    "\n",
    "        the cool thing is that then you can put this to predict the greedy guess/action.\n",
    "        \"\"\"\n",
    "        padded_data = np.ones((1,self.dolinar_layers+1, 2))*self.pad_value\n",
    "        padded_data[0][0][0] = seq[0]\n",
    "        #padded_data[0][0] = data[0]\n",
    "        for k in range(1,self.dolinar_layers+1):\n",
    "            padded_data[0][k] = seq[k:(k+2)]\n",
    "        return padded_data\n",
    "\n",
    "    def give_td_error_Kennedy_guess(self,batched_input,sequential_rews_with_zeros):\n",
    "        # this function takes as input the actions as given by the target actor (but the first one!)\n",
    "        #and outpus the correspoindg TD-errors for DDPG! To obtain them from sample of buffer\n",
    "        #you call the method targeted_sequence from the actor_target and then the process_sequence\n",
    "        #of this critic network.\n",
    "        if self.nature != \"target\":\n",
    "            raise AttributeError(\"I'm not the target!\")\n",
    "            return\n",
    "        b = batched_input.copy()\n",
    "        ll = sequential_rews_with_zeros.copy()\n",
    "        for k in range(0,self.dolinar_layers-1):\n",
    "            print(k)\n",
    "            ll[:,k] = np.squeeze(self(b))[:,k+1] + ll[:,k]\n",
    "\n",
    "        preds1 = self(b)\n",
    "        b[:,-1][:,-1] = -b[:,1][:,1]\n",
    "        preds2 = self(b)\n",
    "        both = tf.concat([preds1,preds2],2)\n",
    "        maxs = np.squeeze(tf.math.reduce_max(both,axis=2).numpy())\n",
    "        ll[:,-2] = maxs[:,1] # This is the last befre the guess.. so the label is max_g Q(h-L, g)\n",
    "        ll = np.expand_dims(ll,axis=1)\n",
    "        return ll\n",
    "\n",
    "\n",
    "    def give_favourite_guess(self,sequence_with_plus):\n",
    "        \"\"\"\"\n",
    "            important !! the 1!\n",
    "        sequence should be [[beta, pad], [outcome, 1]] \"\"\"\n",
    "        pred_1 = self(sequence_with_plus)\n",
    "        sequence_with_plus[:,1][:,1] = -sequence_with_plus[:,1][:,1]\n",
    "        pred_2 = self(sequence_with_plus)\n",
    "        both = tf.concat([pred_1,pred_2],2)\n",
    "        maxs = np.squeeze(tf.argmax(both,axis=2).numpy())[1]\n",
    "\n",
    "        guess = (-1)**maxs\n",
    "        return  guess\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### ACTOR CLASSS ####\n",
    "class Actor(tf.keras.Model):\n",
    "    def __init__(self, nature, valreg=0.01, seed_val=0.1, pad_value = -7.,\n",
    "                 dolinar_layers=2,tau=0.01):\n",
    "        super(Actor,self).__init__()\n",
    "        self.dolinar_layers = dolinar_layers\n",
    "        self.pad_value = pad_value\n",
    "        self.nature = nature\n",
    "        self.tau = tau\n",
    "\n",
    "        if nature == \"primary\":\n",
    "            self.lstm = tf.keras.layers.LSTM(500, return_sequences=True, stateful=True)\n",
    "            self.mask = tf.keras.layers.Masking(mask_value=pad_value,\n",
    "                                  input_shape=(1,1))#CHECK\n",
    "        elif nature == \"target\":\n",
    "            self.lstm = tf.keras.layers.LSTM(500, return_sequences=True, stateful=False)\n",
    "            self.mask = tf.keras.layers.Masking(mask_value=pad_value,\n",
    "                                  input_shape=(self.dolinar_layers, 1)) #'cause i feed altoghether.\n",
    "        else:\n",
    "            print(\"Hey! the character is either primary or target\")\n",
    "        self.l1 = Dense(250,kernel_initializer=tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val),\n",
    "        bias_initializer = tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val),\n",
    "        kernel_regularizer=tf.keras.regularizers.l1(valreg),\n",
    "    activity_regularizer=tf.keras.regularizers.l2(valreg))\n",
    "\n",
    "        self.l2 = Dense(100, kernel_regularizer=tf.keras.regularizers.l1(valreg),\n",
    "    activity_regularizer=tf.keras.regularizers.l2(valreg),\n",
    "    kernel_initializer=tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val),\n",
    "    bias_initializer = tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val))\n",
    "\n",
    "        self.l3 = Dense(100, kernel_regularizer=tf.keras.regularizers.l1(valreg),\n",
    "    activity_regularizer=tf.keras.regularizers.l2(valreg),\n",
    "    kernel_initializer=tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val),\n",
    "    bias_initializer = tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val))\n",
    "\n",
    "        self.l4 = Dense(1, kernel_regularizer=tf.keras.regularizers.l1(valreg),\n",
    "    activity_regularizer=tf.keras.regularizers.l2(valreg),\n",
    "    kernel_initializer=tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val),\n",
    "    bias_initializer = tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val))\n",
    "\n",
    "\n",
    "\n",
    "    def update_target_parameters(self,primary_net):\n",
    "        #### only\n",
    "        # for i,j in zip(self.get_weights(), primary_net.get_weights()):\n",
    "        #     tf.assign(i, tau*j + (i-tau)*i )\n",
    "        prim_weights = primary_net.get_weights()\n",
    "        targ_weights = self.get_weights()\n",
    "        weights = []\n",
    "        for i in tf.range(len(prim_weights)):\n",
    "            weights.append(self.tau * prim_weights[i] + (1 - self.tau) * targ_weights[i])\n",
    "        self.set_weights(weights)\n",
    "        return\n",
    "\n",
    "    def call(self, inputs):\n",
    "        feat = self.mask(inputs)\n",
    "        feat= self.lstm(feat)\n",
    "        # feat = tf.nn.dropout(feat, rate=0.01)\n",
    "        feat = tf.nn.relu(self.l1(feat))\n",
    "        # feat = tf.nn.dropout(feat, rate=0.01)\n",
    "        feat = tf.nn.relu(self.l2(feat))\n",
    "        feat = tf.nn.relu(self.l3(feat))\n",
    "        feat = tf.nn.sigmoid(self.l4(feat))\n",
    "\n",
    "        return feat\n",
    "\n",
    "    def process_sequence_of_experiences(self, experiences):\n",
    "\n",
    "        #This function takes a vector of experiences:\n",
    "        #vector = (\\beta1, o1, \\beta2, o2, \\beta3, o3,...,o_L, guess)\n",
    "        #and retrieves\n",
    "        #(\\beta1, o1, \\beta2_target, o2, \\beta3_target, o3, \\beta4_target,... ,o_L, guess)\n",
    "\n",
    "        #For the primary it should give again the actions that generated the experience (this is to consider the wegiths\n",
    "        #in the graph)\n",
    "\n",
    "        #For the target it gives the \"opinion\" of the actions it should've taken...\n",
    "\n",
    "        # if self.nature != \"target\":\n",
    "        #     raise AttributeError(\"check the lstm memory of actor target, stateful == True ?\")\n",
    "        #     return\n",
    "        export = experiences.copy()\n",
    "        for index in range(1,2*self.dolinar_layers-1,2): # I consider from first outcome to last one (but guess)\n",
    "            export[:,index+1] = np.squeeze(self(np.reshape(np.array(export[:,index]),\n",
    "                                                                 (experiences.shape[0],1,1))))\n",
    "        return export\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimization_step(experiences, critic, critic_target, actor, actor_target, optimizer_critic, optimizer_actor):\n",
    "    targeted_experience = actor_target.process_sequence_of_experiences(experiences)\n",
    "    sequences, zeroed_rews = critic_target.process_sequence(targeted_experience)\n",
    "    labels_critic = critic_target.give_td_error_Kennedy_guess( sequences, zeroed_rews)\n",
    "    #\n",
    "    ###### train the critic ######\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(critic.trainable_variables)\n",
    "        preds_critic = critic(sequences)\n",
    "        loss_critic = tf.keras.losses.MSE(labels_critic, preds_critic)\n",
    "        loss_critic = tf.reduce_mean(loss_critic)\n",
    "        grads = tape.gradient(loss_critic, critic.trainable_variables)\n",
    "        optimizer_critic.apply_gradients(zip(grads, critic.trainable_variables))\n",
    "        loss_critic = np.squeeze(loss_critic.numpy())\n",
    "    #\n",
    "    #\n",
    "    actor.lstm.reset_states()\n",
    "    actor.lstm.stateful=False\n",
    "    with tf.GradientTape() as tape:\n",
    "        # export = experiences.copy()\n",
    "        # actions = [0.]*actor.dolinar_layers\n",
    "        # actons[0] = actor(np.reshape(np.array(actor.pad_value), (experiences.shape[0],1,1)))\n",
    "        #\n",
    "        # for ind,index in enumerate(range(1,2*actor.dolinar_layers-1,2)): # I consider from first outcome to last one (but guess)\n",
    "        #     actions[:,ind] = actor(np.reshape(np.array(export[:,index]),\n",
    "        #                                                          (experiences.shape[0],1,1)))\n",
    "\n",
    "        actions_with_outcomes = experiences.copy()\n",
    "        actions_indexed = []\n",
    "        for ind, index in enumerate(range(1,2*actor.dolinar_layers-1,2)): # I consider from first outcome to last one (but guess)\n",
    "            ac = actor_target(np.reshape(actions_with_outcomes[:,index], (len(actions_with_outcomes[:,index]), 1,1 )))\n",
    "            # print(actor(np.reshape(actions_with_outcomes[:,index], (len(actions_with_outcomes[:,index]), 1,1 ))))\n",
    "            print(\"****\")\n",
    "            # actor(np.reshape(actions_with_outcomes[:,index], (len(actions_with_outcomes[:,index]), 1,1 )))\n",
    "            # print(np.reshape(actions_with_outcomes[:,index], (len(actions_with_outcomes[:,index]), 1,1 )).shape)\n",
    "            actions_indexed.append(ac)\n",
    "            actions_with_outcomes[:,index+1] = np.squeeze(ac)\n",
    "        # tape.watch(actions_indexed)\n",
    "        tape.watch(actions_indexed)\n",
    "        print(actions_indexed)\n",
    "        acionts_indexed = tf.concat(actions_indexed,axis=0)\n",
    "        bbs, rrs = critic.process_sequence(actions_with_outcomes)\n",
    "        qvals = critic(bbs)\n",
    "        print(qvals)\n",
    "        dq_da = tape.gradient(qvals, actions_indexed)\n",
    "        print(dq_da)\n",
    "    # #\n",
    "    # with tf.GradientTape() as tape:\n",
    "    #     actionss = actor(np.expand_dims(np.zeros(len(experiences)),axis=1))\n",
    "    #     da_dtheta = tape.gradient(actionss, actor.trainable_variables, output_gradients=-dq_da)\n",
    "    #\n",
    "    # optimizer_actor.apply_gradients(zip(da_dtheta, actor.trainable_variables))\n",
    "    actor.lstm.stateful=True\n",
    "    return loss_critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.6910216 ,  0.        ,  0.8369508 ,  0.        , -1.        ],\n",
       "       [ 0.98145694,  0.        ,  0.04985608,  0.        , -1.        ],\n",
       "       [ 0.695889  ,  0.        ,  0.30091032,  0.        ,  1.        ],\n",
       "       [ 0.654807  ,  0.        ,  0.72952473,  1.        , -1.        ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer actor_18 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer actor_19 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "0\n",
      "WARNING:tensorflow:Layer critic_19 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer critic_18 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "****\n",
      "[<tf.Tensor: shape=(100, 1, 1), dtype=float32, numpy=\n",
      "array([[[0.50754774]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50727713]],\n",
      "\n",
      "       [[0.50727713]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50727713]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50727713]],\n",
      "\n",
      "       [[0.50727713]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50727713]],\n",
      "\n",
      "       [[0.50727713]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50727713]],\n",
      "\n",
      "       [[0.50727713]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50727713]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50727713]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50727713]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50727713]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50727713]],\n",
      "\n",
      "       [[0.50727713]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50727713]],\n",
      "\n",
      "       [[0.50727713]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50727713]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50727713]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50727713]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50727713]],\n",
      "\n",
      "       [[0.50727713]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50727713]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50727713]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50727713]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50727713]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50727713]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50727713]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50727713]],\n",
      "\n",
      "       [[0.50727713]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50754774]],\n",
      "\n",
      "       [[0.50727713]]], dtype=float32)>]\n",
      "tf.Tensor(\n",
      "[[[1.47018009e-05]\n",
      "  [1.21175043e-08]\n",
      "  [5.43055878e-10]]\n",
      "\n",
      " [[1.30823009e-05]\n",
      "  [1.03347242e-08]\n",
      "  [4.58921484e-10]]\n",
      "\n",
      " [[1.46747934e-05]\n",
      "  [1.20849828e-08]\n",
      "  [5.41511613e-10]]\n",
      "\n",
      " [[1.49043244e-05]\n",
      "  [1.23625119e-08]\n",
      "  [1.69308678e-09]]\n",
      "\n",
      " [[1.85662702e-05]\n",
      "  [1.70773742e-08]\n",
      "  [2.38742093e-09]]\n",
      "\n",
      " [[1.60845848e-05]\n",
      "  [8.00053979e-09]\n",
      "  [3.32251615e-10]]\n",
      "\n",
      " [[1.42456420e-05]\n",
      "  [6.68976474e-09]\n",
      "  [2.75355405e-10]]\n",
      "\n",
      " [[1.37041734e-05]\n",
      "  [1.09920908e-08]\n",
      "  [4.89818131e-10]]\n",
      "\n",
      " [[1.43849056e-05]\n",
      "  [1.17376331e-08]\n",
      "  [1.60140545e-09]]\n",
      "\n",
      " [[1.56230544e-05]\n",
      "  [1.32325404e-08]\n",
      "  [1.82340798e-09]]\n",
      "\n",
      " [[1.81018986e-05]\n",
      "  [9.57303570e-09]\n",
      "  [4.00428884e-10]]\n",
      "\n",
      " [[1.57655868e-05]\n",
      "  [1.34075595e-08]\n",
      "  [6.05260120e-10]]\n",
      "\n",
      " [[1.66594491e-05]\n",
      "  [1.45227110e-08]\n",
      "  [6.59118593e-10]]\n",
      "\n",
      " [[1.32869618e-05]\n",
      "  [1.05491766e-08]\n",
      "  [4.68981742e-10]]\n",
      "\n",
      " [[1.56394071e-05]\n",
      "  [1.32525955e-08]\n",
      "  [5.97769501e-10]]\n",
      "\n",
      " [[1.34148831e-05]\n",
      "  [6.17573770e-09]\n",
      "  [2.53039145e-10]]\n",
      "\n",
      " [[1.45792264e-05]\n",
      "  [6.91810786e-09]\n",
      "  [8.62762195e-10]]\n",
      "\n",
      " [[1.73028275e-05]\n",
      "  [1.53688813e-08]\n",
      "  [7.00110803e-10]]\n",
      "\n",
      " [[1.86733105e-05]\n",
      "  [1.72306596e-08]\n",
      "  [7.91390509e-10]]\n",
      "\n",
      " [[1.75699788e-05]\n",
      "  [1.57243747e-08]\n",
      "  [7.17461646e-10]]\n",
      "\n",
      " [[1.49720072e-05]\n",
      "  [1.24436790e-08]\n",
      "  [1.70518799e-09]]\n",
      "\n",
      " [[1.60684267e-05]\n",
      "  [1.37812552e-08]\n",
      "  [6.23360474e-10]]\n",
      "\n",
      " [[1.56240822e-05]\n",
      "  [7.66004327e-09]\n",
      "  [3.17590482e-10]]\n",
      "\n",
      " [[1.44012402e-05]\n",
      "  [6.79500944e-09]\n",
      "  [2.79931772e-10]]\n",
      "\n",
      " [[1.35649261e-05]\n",
      "  [1.08436371e-08]\n",
      "  [1.47097090e-09]]\n",
      "\n",
      " [[1.88866870e-05]\n",
      "  [1.75393655e-08]\n",
      "  [8.06614553e-10]]\n",
      "\n",
      " [[1.60588515e-05]\n",
      "  [1.37695100e-08]\n",
      "  [6.22787655e-10]]\n",
      "\n",
      " [[1.38846490e-05]\n",
      "  [1.11853549e-08]\n",
      "  [1.52072732e-09]]\n",
      "\n",
      " [[1.44835358e-05]\n",
      "  [6.85180490e-09]\n",
      "  [2.82405099e-10]]\n",
      "\n",
      " [[1.77416568e-05]\n",
      "  [9.28222210e-09]\n",
      "  [3.87798932e-10]]\n",
      "\n",
      " [[1.50369724e-05]\n",
      "  [1.25215802e-08]\n",
      "  [5.62370317e-10]]\n",
      "\n",
      " [[1.73164299e-05]\n",
      "  [1.53868918e-08]\n",
      "  [7.00990044e-10]]\n",
      "\n",
      " [[1.30325161e-05]\n",
      "  [5.94450977e-09]\n",
      "  [2.43026238e-10]]\n",
      "\n",
      " [[1.67199487e-05]\n",
      "  [1.46003947e-08]\n",
      "  [6.62878141e-10]]\n",
      "\n",
      " [[1.30298567e-05]\n",
      "  [1.02803446e-08]\n",
      "  [1.38890444e-09]]\n",
      "\n",
      " [[1.62697161e-05]\n",
      "  [1.40309968e-08]\n",
      "  [6.35466069e-10]]\n",
      "\n",
      " [[1.62628439e-05]\n",
      "  [1.40223548e-08]\n",
      "  [6.35049235e-10]]\n",
      "\n",
      " [[1.59858591e-05]\n",
      "  [1.36794345e-08]\n",
      "  [6.18425700e-10]]\n",
      "\n",
      " [[1.53986002e-05]\n",
      "  [1.29586928e-08]\n",
      "  [5.83522508e-10]]\n",
      "\n",
      " [[1.46400707e-05]\n",
      "  [1.20432189e-08]\n",
      "  [5.39524980e-10]]\n",
      "\n",
      " [[1.71197480e-05]\n",
      "  [1.51261084e-08]\n",
      "  [6.88326451e-10]]\n",
      "\n",
      " [[1.86075176e-05]\n",
      "  [1.71364327e-08]\n",
      "  [7.86746002e-10]]\n",
      "\n",
      " [[1.90647388e-05]\n",
      "  [1.78004882e-08]\n",
      "  [8.19510682e-10]]\n",
      "\n",
      " [[1.48948748e-05]\n",
      "  [1.23510571e-08]\n",
      "  [1.69139869e-09]]\n",
      "\n",
      " [[1.81855430e-05]\n",
      "  [1.65553686e-08]\n",
      "  [7.58167806e-10]]\n",
      "\n",
      " [[1.72271048e-05]\n",
      "  [8.87284912e-09]\n",
      "  [3.69997977e-10]]\n",
      "\n",
      " [[1.48212348e-05]\n",
      "  [1.22618147e-08]\n",
      "  [5.49905954e-10]]\n",
      "\n",
      " [[1.88745144e-05]\n",
      "  [1.02189190e-08]\n",
      "  [4.28529490e-10]]\n",
      "\n",
      " [[1.47970559e-05]\n",
      "  [1.22325217e-08]\n",
      "  [5.48516954e-10]]\n",
      "\n",
      " [[1.80813859e-05]\n",
      "  [1.64136615e-08]\n",
      "  [2.29086861e-09]]\n",
      "\n",
      " [[1.51379791e-05]\n",
      "  [7.30966843e-09]\n",
      "  [3.02388170e-10]]\n",
      "\n",
      " [[1.47253886e-05]\n",
      "  [1.21459198e-08]\n",
      "  [1.66127079e-09]]\n",
      "\n",
      " [[1.38069017e-05]\n",
      "  [1.11019514e-08]\n",
      "  [4.94999652e-10]]\n",
      "\n",
      " [[1.30514090e-05]\n",
      "  [5.95579053e-09]\n",
      "  [2.43512960e-10]]\n",
      "\n",
      " [[1.45468803e-05]\n",
      "  [6.89568580e-09]\n",
      "  [2.84315654e-10]]\n",
      "\n",
      " [[1.65982510e-05]\n",
      "  [1.44452761e-08]\n",
      "  [2.00411976e-09]]\n",
      "\n",
      " [[1.56427177e-05]\n",
      "  [7.67368658e-09]\n",
      "  [3.18179816e-10]]\n",
      "\n",
      " [[1.35632572e-05]\n",
      "  [6.26648600e-09]\n",
      "  [2.56974914e-10]]\n",
      "\n",
      " [[1.89010661e-05]\n",
      "  [1.75604526e-08]\n",
      "  [8.07653722e-10]]\n",
      "\n",
      " [[1.50957103e-05]\n",
      "  [7.27978211e-09]\n",
      "  [3.01081160e-10]]\n",
      "\n",
      " [[1.89116854e-05]\n",
      "  [1.75759673e-08]\n",
      "  [8.08418110e-10]]\n",
      "\n",
      " [[1.67972539e-05]\n",
      "  [8.53633786e-09]\n",
      "  [3.55399654e-10]]\n",
      "\n",
      " [[1.75629593e-05]\n",
      "  [1.57150488e-08]\n",
      "  [2.18893681e-09]]\n",
      "\n",
      " [[1.52609573e-05]\n",
      "  [1.27916797e-08]\n",
      "  [1.75724391e-09]]\n",
      "\n",
      " [[1.83604170e-05]\n",
      "  [1.67942176e-08]\n",
      "  [2.34621500e-09]]\n",
      "\n",
      " [[1.54349145e-05]\n",
      "  [1.30028379e-08]\n",
      "  [1.78889392e-09]]\n",
      "\n",
      " [[1.32495697e-05]\n",
      "  [6.07508843e-09]\n",
      "  [2.48675580e-10]]\n",
      "\n",
      " [[1.33668791e-05]\n",
      "  [1.06335776e-08]\n",
      "  [4.72937745e-10]]\n",
      "\n",
      " [[1.44497071e-05]\n",
      "  [1.18149011e-08]\n",
      "  [5.28705302e-10]]\n",
      "\n",
      " [[1.57387403e-05]\n",
      "  [7.74439712e-09]\n",
      "  [3.21218940e-10]]\n",
      "\n",
      " [[1.45611630e-05]\n",
      "  [6.90557034e-09]\n",
      "  [2.84747087e-10]]\n",
      "\n",
      " [[1.34470465e-05]\n",
      "  [1.07184697e-08]\n",
      "  [4.76933493e-10]]\n",
      "\n",
      " [[1.51183149e-05]\n",
      "  [1.26194521e-08]\n",
      "  [5.67100034e-10]]\n",
      "\n",
      " [[1.41115343e-05]\n",
      "  [6.60583899e-09]\n",
      "  [2.71707046e-10]]\n",
      "\n",
      " [[1.59153824e-05]\n",
      "  [1.35923326e-08]\n",
      "  [6.14202023e-10]]\n",
      "\n",
      " [[1.70736548e-05]\n",
      "  [8.75214923e-09]\n",
      "  [1.11054610e-09]]\n",
      "\n",
      " [[1.63961176e-05]\n",
      "  [1.41902987e-08]\n",
      "  [6.43113729e-10]]\n",
      "\n",
      " [[1.59097071e-05]\n",
      "  [7.87087107e-09]\n",
      "  [9.91766891e-10]]\n",
      "\n",
      " [[1.43265697e-05]\n",
      "  [1.16681917e-08]\n",
      "  [5.21754917e-10]]\n",
      "\n",
      " [[1.42405888e-05]\n",
      "  [6.68661393e-09]\n",
      "  [2.75216766e-10]]\n",
      "\n",
      " [[1.79879771e-05]\n",
      "  [1.62869860e-08]\n",
      "  [2.27247310e-09]]\n",
      "\n",
      " [[1.87617588e-05]\n",
      "  [1.73578218e-08]\n",
      "  [2.42828513e-09]]\n",
      "\n",
      " [[1.51231452e-05]\n",
      "  [1.26253017e-08]\n",
      "  [5.67383529e-10]]\n",
      "\n",
      " [[1.32303776e-05]\n",
      "  [6.06345463e-09]\n",
      "  [2.48171900e-10]]\n",
      "\n",
      " [[1.47533892e-05]\n",
      "  [1.21797896e-08]\n",
      "  [5.46011791e-10]]\n",
      "\n",
      " [[1.61102689e-05]\n",
      "  [1.38330032e-08]\n",
      "  [6.25869467e-10]]\n",
      "\n",
      " [[1.72701857e-05]\n",
      "  [1.53255595e-08]\n",
      "  [2.13205631e-09]]\n",
      "\n",
      " [[1.35160753e-05]\n",
      "  [1.07917248e-08]\n",
      "  [1.46344015e-09]]\n",
      "\n",
      " [[1.81151809e-05]\n",
      "  [1.64595271e-08]\n",
      "  [7.53462459e-10]]\n",
      "\n",
      " [[1.67453054e-05]\n",
      "  [1.46335717e-08]\n",
      "  [6.64478195e-10]]\n",
      "\n",
      " [[1.69850791e-05]\n",
      "  [1.49482009e-08]\n",
      "  [6.79709289e-10]]\n",
      "\n",
      " [[1.37374882e-05]\n",
      "  [6.37357545e-09]\n",
      "  [2.61626276e-10]]\n",
      "\n",
      " [[1.67673097e-05]\n",
      "  [1.46623487e-08]\n",
      "  [6.65871247e-10]]\n",
      "\n",
      " [[1.59006067e-05]\n",
      "  [1.35740414e-08]\n",
      "  [6.13317064e-10]]\n",
      "\n",
      " [[1.62812485e-05]\n",
      "  [8.14698442e-09]\n",
      "  [1.02888331e-09]]\n",
      "\n",
      " [[1.35397622e-05]\n",
      "  [6.25211172e-09]\n",
      "  [2.56350247e-10]]\n",
      "\n",
      " [[1.83537286e-05]\n",
      "  [1.67850907e-08]\n",
      "  [7.69458275e-10]]\n",
      "\n",
      " [[1.47888177e-05]\n",
      "  [1.22225394e-08]\n",
      "  [1.67250969e-09]]\n",
      "\n",
      " [[1.60184245e-05]\n",
      "  [1.37195952e-08]\n",
      "  [6.20366758e-10]]\n",
      "\n",
      " [[1.61356547e-05]\n",
      "  [8.03847389e-09]\n",
      "  [3.33887445e-10]]], shape=(100, 3, 1), dtype=float32)\n",
      "[None]\n"
     ]
    }
   ],
   "source": [
    "amplitude = 0.4\n",
    "lr_critic = lr_actor = 0.01\n",
    "\n",
    "critic = Critic(nature=\"primary\",valreg=0.01)\n",
    "critic_target = Critic(nature=\"target\")\n",
    "actor = Actor(nature=\"primary\")\n",
    "actor_target = Actor(nature=\"target\")\n",
    "\n",
    "optimizer_critic = tf.keras.optimizers.Adam(lr=lr_critic)\n",
    "optimizer_actor = tf.keras.optimizers.Adam(lr=lr_actor) #0.001 works well\n",
    "\n",
    "experiences = np.load(\"expe_2L.npy\")\n",
    "\n",
    "input_actor = np.reshape(np.array([actor.pad_value]),(1,1,1))\n",
    "beta_would_do = np.squeeze(actor(input_actor))\n",
    "    \n",
    "new_loss = optimization_step(experiences,critic, critic_target, actor, actor_target, optimizer_critic, optimizer_actor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbs, ll = critic.process_sequence(experiences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    preds = critic(bbs)\n",
    "    tape.watch(preds)\n",
    "    ll = tf.keras.losses.MSE(np.ones(preds.numpy().shape), preds)\n",
    "    grads = tape.gradient(ll, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(100, 3, 1), dtype=float32, numpy=\n",
       "array([[[-9.41753387e-06],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-8.10623169e-06],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-9.29832458e-06],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-9.53674316e-06],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.29938126e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.04904175e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-8.94069672e-06],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-8.58306885e-06],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-9.05990601e-06],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.02519989e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.25169754e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.02519989e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.09672546e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-8.22544098e-06],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.02519989e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-8.34465027e-06],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-9.29832458e-06],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.18017197e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.32322311e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.21593475e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-9.53674316e-06],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.04904175e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.02519989e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-9.29832458e-06],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-8.34465027e-06],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.34706497e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.04904175e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-8.82148743e-06],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-9.05990601e-06],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.21593475e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-9.53674316e-06],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.19209290e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-8.10623169e-06],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.12056732e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-8.10623169e-06],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.07288361e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.07288361e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.06096268e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.00135803e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-9.29832458e-06],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.15633011e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.31130219e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.37090683e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-9.41753387e-06],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.26361847e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.16825104e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-9.29832458e-06],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.33514404e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-9.29832458e-06],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.23977661e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-9.65595245e-06],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-9.29832458e-06],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-8.82148743e-06],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-8.10623169e-06],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-9.29832458e-06],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.09672546e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.02519989e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-8.34465027e-06],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.35898590e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-9.53674316e-06],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.34706497e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.12056732e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.20401382e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-9.89437103e-06],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.28746033e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.00135803e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-8.34465027e-06],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-8.34465027e-06],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-9.17911530e-06],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.02519989e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-9.29832458e-06],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-8.34465027e-06],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-9.77516174e-06],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-9.05990601e-06],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.02519989e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.15633011e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.09672546e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.02519989e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-9.05990601e-06],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-8.82148743e-06],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.26361847e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.33514404e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-9.77516174e-06],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-8.22544098e-06],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-9.53674316e-06],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.04904175e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.18017197e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-8.46385956e-06],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.25169754e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.12056732e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.14440918e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-8.58306885e-06],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.12056732e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.03712082e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.07288361e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-8.34465027e-06],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.31130219e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-9.53674316e-06],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.04904175e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]],\n",
       "\n",
       "       [[-1.07288361e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]]], dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiences = experiences[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiences = experiences[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_indexed = [0.]*(actor.dolinar_layers)\n",
    "actions_indexed[0] = tf.convert_to_tensor(np.reshape(experiences[:,0], (len(experiences),1,1)))\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    ##### get the actions only ######\n",
    "    actions_with_outcomes = experiences.copy()\n",
    "    act_ind=0\n",
    "    for ind in range(len(experiences)):\n",
    "        if (ind%2 == 0)&(ind!=len(experiences)):\n",
    "            ac = tf.convert_to_tensor(np.reshape(experiences[:,ind], (len(experiences),1,1)))\n",
    "            actions_indexed[act_ind] = ac\n",
    "            act_ind+=1\n",
    "    actions_indexed = tf.concat(actions_indexed,axis=1)\n",
    "    tape.watch(actions_indexed) ####watch the ations \n",
    "    \n",
    "    ### now prepare the state acions to put them into the critic###\n",
    "    padded_data = [tf.ones((batch_size,1))*actor.pad_value]\n",
    "    watched_input_critic  = padded_data.copy()\n",
    "    ind_actions=0\n",
    "    for ind,k in enumerate(tf.unstack(tf.convert_to_tensor(experiences),axis=1)):\n",
    "        if (ind%2==0)&(ind != len(experiences)):\n",
    "            padded_data.append(actions_indexed[:,ind_actions]) ### i add the input of the critic the watched actions!\n",
    "            ind_actions+=1\n",
    "        else:\n",
    "            padded_data.append(tf.expand_dims(k, axis=1))\n",
    "        if ind == 0:\n",
    "            watched_input_critic = tf.stack([padded_data[0], padded_data[1]], axis=2) #importantly i put the padd first (state_action.)\n",
    "        if (ind%2 == 0)&(ind!=0):\n",
    "            intermediate = tf.stack([padded_data[ind], padded_data[ind+1]], axis=2)\n",
    "            watched_input_critic = tf.concat([watched_input_critic, intermediate], axis=1)\n",
    "    \n",
    "    qvals = critic(watched_input_critic)\n",
    "    dq_da = tape.gradient(qvals, actions_indexed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "\n",
    "    pads = np.ones(len(experiences))*actor.pad_value\n",
    "    news = np.random.rand(experiences.shape[0], experiences.shape[1]+1)\n",
    "    news[:,1:] = experiences\n",
    "    news[:,0] = pads\n",
    "    instances_actor = [i for i in range(0,2*actor.dolinar_layers,2)]\n",
    "    actionss = actor(np.reshape(news[:,instances_actor], (experiences.shape[0],actor.dolinar_layers,1)))\n",
    "    \n",
    "    da_dtheta = tape.gradient(actionss, actor.trainable_variables, output_gradients=-dq_da)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(1, 2000), dtype=float32, numpy=array([[0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(500, 2000), dtype=float32, numpy=\n",
       " array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2000,), dtype=float32, numpy=array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)>,\n",
       " <tf.Tensor: shape=(500, 250), dtype=float32, numpy=\n",
       " array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(250,), dtype=float32, numpy=\n",
       " array([ 3.1427753e-03, -5.9653060e-03,  0.0000000e+00,  0.0000000e+00,\n",
       "         1.3609142e-03,  2.6171082e-03,  0.0000000e+00, -8.9284504e-04,\n",
       "        -1.0439263e-02,  0.0000000e+00,  0.0000000e+00, -3.3545401e-03,\n",
       "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  8.4193936e-03,\n",
       "         2.6431965e-04, -5.2743964e-03,  0.0000000e+00,  0.0000000e+00,\n",
       "         0.0000000e+00, -4.0904412e-04, -5.8293324e-03, -1.6449175e-03,\n",
       "        -2.8730193e-04,  0.0000000e+00, -1.0272447e-02, -3.2259421e-03,\n",
       "         0.0000000e+00,  1.2301217e-03,  0.0000000e+00,  0.0000000e+00,\n",
       "         0.0000000e+00,  7.4155964e-03,  0.0000000e+00,  4.1017141e-03,\n",
       "        -3.8433678e-03,  3.9667371e-03,  0.0000000e+00,  1.6419207e-03,\n",
       "         7.0100115e-03,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        -3.5609528e-03,  4.9569565e-03,  0.0000000e+00,  0.0000000e+00,\n",
       "        -1.5535203e-03,  0.0000000e+00,  8.9874202e-03,  0.0000000e+00,\n",
       "         0.0000000e+00,  3.0631383e-03,  0.0000000e+00, -6.4748758e-03,\n",
       "         3.0568137e-03,  9.1369338e-03,  0.0000000e+00, -5.1136650e-03,\n",
       "         5.8304253e-03,  0.0000000e+00,  0.0000000e+00, -3.3524134e-03,\n",
       "         4.9797189e-03,  0.0000000e+00,  7.8760348e-03,  4.0945048e-03,\n",
       "        -5.9683397e-03,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "         0.0000000e+00,  0.0000000e+00, -1.9176175e-03, -8.8920591e-05,\n",
       "         0.0000000e+00, -3.3968238e-03,  9.1712838e-03,  4.7366021e-04,\n",
       "         2.8352400e-03,  0.0000000e+00,  0.0000000e+00, -3.9662598e-03,\n",
       "        -3.7245676e-03, -7.2098761e-03,  0.0000000e+00,  2.0689904e-03,\n",
       "        -3.4687917e-03,  1.3199302e-03,  2.4856785e-03, -1.0179842e-03,\n",
       "        -1.0390838e-02,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "         0.0000000e+00,  2.5048160e-03,  1.0215305e-02,  0.0000000e+00,\n",
       "         3.8740118e-03,  1.3215914e-04,  1.1151326e-03,  0.0000000e+00,\n",
       "         3.1821909e-03,  0.0000000e+00,  8.1197042e-03, -6.2119788e-03,\n",
       "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        -4.0650327e-04,  3.0751375e-03, -1.5371549e-03,  2.2933111e-03,\n",
       "         1.5024397e-03, -2.3306429e-03,  1.4532499e-03, -5.4162638e-03,\n",
       "         0.0000000e+00,  5.5249478e-04,  2.2957304e-03,  0.0000000e+00,\n",
       "         6.6817913e-04,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "         9.5066456e-03, -5.9954752e-03,  0.0000000e+00, -5.0609438e-03,\n",
       "         5.1062102e-03,  1.4371950e-03,  0.0000000e+00,  0.0000000e+00,\n",
       "         4.2856930e-04,  0.0000000e+00,  4.5723170e-03,  0.0000000e+00,\n",
       "         0.0000000e+00,  1.7263183e-03, -5.2882743e-04,  0.0000000e+00,\n",
       "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00, -1.9097750e-03,\n",
       "         1.8565407e-03,  0.0000000e+00,  0.0000000e+00,  7.1011214e-03,\n",
       "         8.0728773e-03,  0.0000000e+00,  5.0304463e-04, -1.4807185e-03,\n",
       "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  4.3520499e-03,\n",
       "        -1.1608874e-03,  7.8365579e-03,  0.0000000e+00,  4.9455540e-04,\n",
       "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00, -2.3362413e-03,\n",
       "         0.0000000e+00,  4.3271058e-03,  0.0000000e+00,  0.0000000e+00,\n",
       "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "         5.1565762e-03,  2.1228339e-03,  0.0000000e+00, -2.3624843e-03,\n",
       "        -7.5321002e-03,  5.2596368e-03, -2.9620219e-03,  0.0000000e+00,\n",
       "         4.1976647e-05,  3.7686967e-03, -8.1521017e-04,  0.0000000e+00,\n",
       "         2.2023150e-03, -9.9724913e-03, -1.2775918e-04, -4.4859361e-04,\n",
       "        -2.8351467e-04,  6.2935506e-03,  7.2912895e-04, -3.6992747e-03,\n",
       "         0.0000000e+00,  7.0447163e-03,  4.7935429e-03, -1.3032235e-02,\n",
       "         6.7847880e-04,  0.0000000e+00,  0.0000000e+00, -4.5280377e-03,\n",
       "        -1.7353679e-03, -8.0161421e-03,  0.0000000e+00,  0.0000000e+00,\n",
       "        -6.7279711e-03, -5.1953257e-03,  0.0000000e+00, -4.1178493e-03,\n",
       "        -7.3265415e-03, -1.3662021e-03,  0.0000000e+00,  0.0000000e+00,\n",
       "         0.0000000e+00,  0.0000000e+00,  4.8410604e-03,  1.3883399e-03,\n",
       "         0.0000000e+00,  0.0000000e+00,  7.3987624e-04,  0.0000000e+00,\n",
       "         0.0000000e+00,  9.8786212e-04,  0.0000000e+00,  6.4518480e-03,\n",
       "         1.0528054e-02, -4.3579172e-03,  0.0000000e+00,  0.0000000e+00,\n",
       "        -6.0766093e-03,  0.0000000e+00,  0.0000000e+00,  1.6479429e-03,\n",
       "         0.0000000e+00, -3.4114700e-03,  0.0000000e+00,  0.0000000e+00,\n",
       "         4.7090189e-03,  0.0000000e+00, -4.4995463e-03,  0.0000000e+00,\n",
       "         0.0000000e+00,  5.1911571e-03,  2.8855039e-03,  3.5259533e-03,\n",
       "         0.0000000e+00, -2.7319719e-04], dtype=float32)>,\n",
       " <tf.Tensor: shape=(250, 100), dtype=float32, numpy=\n",
       " array([[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 1.5915028e-04,\n",
       "         0.0000000e+00, 0.0000000e+00],\n",
       "        [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 4.2934276e-05,\n",
       "         0.0000000e+00, 0.0000000e+00],\n",
       "        [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 0.0000000e+00,\n",
       "         0.0000000e+00, 0.0000000e+00],\n",
       "        ...,\n",
       "        [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 5.8675556e-05,\n",
       "         0.0000000e+00, 0.0000000e+00],\n",
       "        [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 0.0000000e+00,\n",
       "         0.0000000e+00, 0.0000000e+00],\n",
       "        [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 1.3992123e-04,\n",
       "         0.0000000e+00, 0.0000000e+00]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(100,), dtype=float32, numpy=\n",
       " array([ 0.        ,  0.        ,  0.        ,  0.02608295, -0.01645213,\n",
       "         0.01604337,  0.00742413,  0.        ,  0.00891295,  0.        ,\n",
       "         0.00440666,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.00841043,  0.00416368,  0.        ,  0.        ,\n",
       "        -0.00784969, -0.00514077,  0.        , -0.00412454,  0.        ,\n",
       "         0.        , -0.00131862,  0.        ,  0.01850986,  0.01719533,\n",
       "         0.        ,  0.        , -0.00615309,  0.01857986,  0.        ,\n",
       "         0.01248322,  0.        ,  0.0044089 , -0.00685696,  0.        ,\n",
       "        -0.01387857, -0.00092316,  0.        ,  0.        ,  0.        ,\n",
       "         0.00185957,  0.01666254, -0.01213953,  0.        ,  0.000145  ,\n",
       "         0.00729243,  0.        ,  0.00474243,  0.        ,  0.00525329,\n",
       "         0.        ,  0.        ,  0.00793944, -0.0109363 , -0.01031489,\n",
       "        -0.00537385,  0.        ,  0.00412626,  0.        ,  0.        ,\n",
       "         0.        , -0.01292563,  0.01988458,  0.01534285, -0.0064203 ,\n",
       "        -0.01096823, -0.00041477,  0.        ,  0.        ,  0.        ,\n",
       "        -0.00959963,  0.        , -0.00258511,  0.        ,  0.01300217,\n",
       "         0.00707903,  0.        ,  0.01468711,  0.        ,  0.00233821,\n",
       "         0.01238464,  0.        , -0.00057678,  0.00529372,  0.01571722,\n",
       "         0.        ,  0.        ,  0.00578427, -0.01259974,  0.01213928,\n",
       "         0.        ,  0.        ,  0.00161676,  0.        ,  0.        ],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(100, 100), dtype=float32, numpy=\n",
       " array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        ...,\n",
       "        [-0.0002864 ,  0.        ,  0.        , ..., -0.00025796,\n",
       "         -0.00020039,  0.00029078],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(100,), dtype=float32, numpy=\n",
       " array([-0.0187796 ,  0.        ,  0.        , -0.02164887,  0.        ,\n",
       "        -0.01680948,  0.02058801,  0.        , -0.01875222,  0.02857197,\n",
       "         0.        , -0.02154922, -0.00984853,  0.        ,  0.        ,\n",
       "         0.01208585,  0.        ,  0.00539606,  0.        ,  0.        ,\n",
       "        -0.02303661,  0.        , -0.0046446 ,  0.        ,  0.        ,\n",
       "        -0.03867908,  0.        ,  0.01374111,  0.        , -0.00020365,\n",
       "         0.00444665, -0.02282242,  0.03220992,  0.        ,  0.        ,\n",
       "         0.04038206,  0.        , -0.02693971,  0.        , -0.004944  ,\n",
       "         0.        , -0.00164814,  0.        , -0.02262997, -0.00571354,\n",
       "         0.00613978,  0.01320893,  0.01501218, -0.00175721,  0.00117123,\n",
       "         0.        , -0.01580527, -0.03816199,  0.        , -0.04349312,\n",
       "         0.04173753,  0.        ,  0.        , -0.03790686,  0.00690613,\n",
       "         0.        , -0.01495374, -0.02472883,  0.01387132, -0.03767461,\n",
       "         0.00546701,  0.03699677,  0.        ,  0.        ,  0.03281021,\n",
       "         0.        ,  0.02510305,  0.00062373,  0.        , -0.01346363,\n",
       "        -0.00067001, -0.00230082,  0.        , -0.03642141,  0.        ,\n",
       "         0.04179331, -0.01661908,  0.        ,  0.        ,  0.03382245,\n",
       "        -0.01043994,  0.        , -0.03139728,  0.        , -0.02406201,\n",
       "         0.        ,  0.        ,  0.        , -0.03810708, -0.0184557 ,\n",
       "         0.        ,  0.01054803, -0.01691474, -0.01313958,  0.01906694],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(100, 1), dtype=float32, numpy=\n",
       " array([[-0.01585437],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [-0.01604203],\n",
       "        [ 0.        ],\n",
       "        [-0.05383964],\n",
       "        [-0.0219963 ],\n",
       "        [ 0.        ],\n",
       "        [-0.03236002],\n",
       "        [-0.04760396],\n",
       "        [ 0.        ],\n",
       "        [-0.02176983],\n",
       "        [-0.01946148],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [-0.03424856],\n",
       "        [ 0.        ],\n",
       "        [-0.02140193],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [-0.01447945],\n",
       "        [ 0.        ],\n",
       "        [-0.03393226],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [-0.02338335],\n",
       "        [ 0.        ],\n",
       "        [-0.01853827],\n",
       "        [ 0.        ],\n",
       "        [-0.02313775],\n",
       "        [-0.01455115],\n",
       "        [-0.00850527],\n",
       "        [-0.01964132],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [-0.00661725],\n",
       "        [ 0.        ],\n",
       "        [-0.02831479],\n",
       "        [ 0.        ],\n",
       "        [-0.01606103],\n",
       "        [ 0.        ],\n",
       "        [-0.04717835],\n",
       "        [ 0.        ],\n",
       "        [-0.02291084],\n",
       "        [-0.04223678],\n",
       "        [-0.02920663],\n",
       "        [-0.00484939],\n",
       "        [-0.05412946],\n",
       "        [-0.00734741],\n",
       "        [-0.01349057],\n",
       "        [ 0.        ],\n",
       "        [-0.01022208],\n",
       "        [-0.0250373 ],\n",
       "        [ 0.        ],\n",
       "        [-0.02898638],\n",
       "        [-0.03353888],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [-0.01650423],\n",
       "        [-0.03057663],\n",
       "        [ 0.        ],\n",
       "        [-0.01971537],\n",
       "        [-0.01319052],\n",
       "        [-0.06207474],\n",
       "        [-0.01835639],\n",
       "        [-0.00090216],\n",
       "        [-0.05065171],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [-0.03164766],\n",
       "        [ 0.        ],\n",
       "        [-0.04918846],\n",
       "        [-0.01374115],\n",
       "        [ 0.        ],\n",
       "        [-0.02081447],\n",
       "        [-0.02008931],\n",
       "        [-0.01745757],\n",
       "        [ 0.        ],\n",
       "        [-0.04373408],\n",
       "        [ 0.        ],\n",
       "        [-0.04150227],\n",
       "        [-0.01101342],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [-0.01858339],\n",
       "        [-0.024437  ],\n",
       "        [ 0.        ],\n",
       "        [-0.00381803],\n",
       "        [ 0.        ],\n",
       "        [-0.00803294],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [-0.04077448],\n",
       "        [-0.00016791],\n",
       "        [ 0.        ],\n",
       "        [-0.04207716],\n",
       "        [-0.01943088],\n",
       "        [-0.0129694 ],\n",
       "        [-0.02774921]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-0.4375844], dtype=float32)>]"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiences = experiences.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_data = tf.ones((batch_size,1))*actor.pad_value\n",
    "first_step = tf.stack([padded_data, actions_indexed[:,0]], axis=2)\n",
    "step_2 = tf.stack([np.expand_dims(experiences[:,1],1), actions_indexed[:,1]], axis=2)\n",
    "step_2 = tf.concat([first_step, step_2], axis=1)\n",
    "final_step =  tf.stack([np.expand_dims(experiences[:,-3],1), np.expand_dims(experiences[:,-2],1)], axis=2)\n",
    "step_3 = tf.concat([step_2,final_step], axis=1)\n",
    "step_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 3, 2), dtype=float32, numpy=\n",
       "array([[[-7.        ,  0.49959072],\n",
       "        [ 0.        ,  0.4999036 ],\n",
       "        [ 0.        , -1.        ]],\n",
       "\n",
       "       [[-7.        ,  0.49959072],\n",
       "        [ 0.        ,  0.49960613],\n",
       "        [ 0.        , -1.        ]],\n",
       "\n",
       "       [[-7.        ,  0.49959072],\n",
       "        [ 0.        ,  0.4996912 ],\n",
       "        [ 0.        ,  1.        ]],\n",
       "\n",
       "       [[-7.        ,  0.49959072],\n",
       "        [ 0.        ,  0.49986365],\n",
       "        [ 1.        , -1.        ]]], dtype=float32)>"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 2, 2), dtype=float32, numpy=\n",
       "array([[[-7.        ,  0.49959072],\n",
       "        [ 0.        ,  0.4999036 ]],\n",
       "\n",
       "       [[-7.        ,  0.49959072],\n",
       "        [ 0.        ,  0.49960613]],\n",
       "\n",
       "       [[-7.        ,  0.49959072],\n",
       "        [ 0.        ,  0.4996912 ]],\n",
       "\n",
       "       [[-7.        ,  0.49959072],\n",
       "        [ 0.        ,  0.49986365]]], dtype=float32)>"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1., -1.,  1., -1.])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiences[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0., 0.])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiences[:,5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for k in range(1,self.dolinar_layers+1):\n",
    "    padded_data[:,k] = data[:,[k,k+1]]\n",
    "\n",
    "rewards_obtained = np.zeros((batch_size, self.dolinar_layers+1))\n",
    "rewards_obtained[:,-1] = sample_buffer[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(4, 1, 1), dtype=float32, numpy=\n",
       " array([[[0.49959072]],\n",
       " \n",
       "        [[0.49959072]],\n",
       " \n",
       "        [[0.49959072]],\n",
       " \n",
       "        [[0.49959072]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(4, 1, 1), dtype=float32, numpy=\n",
       " array([[[0.49959072]],\n",
       " \n",
       "        [[0.49959072]],\n",
       " \n",
       "        [[0.49959072]],\n",
       " \n",
       "        [[0.49959072]]], dtype=float32)>]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions_indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = experiences.shape[0]\n",
    "data = experiences[:,0:(actor.dolinar_layers+1+1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(4, 1, 1), dtype=float32, numpy=\n",
       " array([[[0.49959072]],\n",
       " \n",
       "        [[0.49959072]],\n",
       " \n",
       "        [[0.49959072]],\n",
       " \n",
       "        [[0.49959072]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(4, 1, 1), dtype=float32, numpy=\n",
       " array([[[0.49959072]],\n",
       " \n",
       "        [[0.49959072]],\n",
       " \n",
       "        [[0.49959072]],\n",
       " \n",
       "        [[0.49959072]]], dtype=float32)>]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_data = np.ones((batch_size,self.dolinar_layers+1, 2))*self.pad_value\n",
    "padded_data[:,0][:,0] = data[:,0]\n",
    "for k in range(1,self.dolinar_layers+1):\n",
    "    padded_data[:,k] = data[:,[k,k+1]]\n",
    "\n",
    "rewards_obtained = np.zeros((batch_size, self.dolinar_layers+1))\n",
    "rewards_obtained[:,-1] = sample_buffer[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.69102163,  0.        ,  0.83695079,  0.        , -1.        ,\n",
       "         1.        ],\n",
       "       [ 0.98145692,  0.        ,  0.04985608,  0.        , -1.        ,\n",
       "         1.        ],\n",
       "       [ 0.69588898,  0.        ,  0.30091034,  0.        ,  1.        ,\n",
       "         0.        ],\n",
       "       [ 0.65480695,  0.        ,  0.72952473,  1.        , -1.        ,\n",
       "         0.        ]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "batch_size = experiences.shape[0]\n",
    "data = sample_buffer[:,0:(self.dolinar_layers+1+1)]\n",
    "padded_data = np.ones((batch_size,self.dolinar_layers+1, 2))*self.pad_value\n",
    "padded_data[:,0][:,0] = data[:,0]\n",
    "for k in range(1,self.dolinar_layers+1):\n",
    "    padded_data[:,k] = data[:,[k,k+1]]\n",
    "\n",
    "rewards_obtained = np.zeros((batch_size, self.dolinar_layers+1))\n",
    "rewards_obtained[:,-1] = sample_buffer[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 1, 1), dtype=float32, numpy=\n",
       "array([[[0.49959072]],\n",
       "\n",
       "       [[0.49959072]],\n",
       "\n",
       "       [[0.49959072]],\n",
       "\n",
       "       [[0.49959072]]], dtype=float32)>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions_indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1.,\n",
       "       0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
       "       0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1.])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(100, 1, 1), dtype=float32, numpy=\n",
       "array([[[1.5807092e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5804997e-05]],\n",
       "\n",
       "       [[1.5804997e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5804997e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5804997e-05]],\n",
       "\n",
       "       [[1.5804997e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5804997e-05]],\n",
       "\n",
       "       [[1.5804997e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5804997e-05]],\n",
       "\n",
       "       [[1.5804997e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5804997e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5804997e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5804997e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5804997e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5804997e-05]],\n",
       "\n",
       "       [[1.5804997e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5804997e-05]],\n",
       "\n",
       "       [[1.5804997e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5804997e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5804997e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5804997e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5804997e-05]],\n",
       "\n",
       "       [[1.5804997e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5804997e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5804997e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5804997e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5804997e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5804997e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5804997e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5804997e-05]],\n",
       "\n",
       "       [[1.5804997e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5807092e-05]],\n",
       "\n",
       "       [[1.5804997e-05]]], dtype=float32)>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
