{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try it with the Actor outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm as tqdm\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "import random\n",
    "import matplotlib\n",
    "from environment import Environment\n",
    "from plots import just_plot\n",
    "# from plots import *\n",
    "from nets import *\n",
    "from buffer import ReplayBuffer\n",
    "from misc import *\n",
    "import numpy as np\n",
    "\n",
    "dolinar_layers=2\n",
    "amplitude = 0.4\n",
    "number_phases=2\n",
    "total_episodes = 10**3\n",
    "buffer_size=500\n",
    "batch_size=64\n",
    "ep_guess=0.01\n",
    "noise_displacement=0.5\n",
    "lr_actor=0.01\n",
    "lr_critic=0.001\n",
    "tau=0.005\n",
    "\n",
    "\n",
    "\n",
    "class PolicyEvaluator(Basics):\n",
    "    def __init__(self, **kwargs):\n",
    "        amplitude= kwargs.get(\"amplitude\", .4)\n",
    "        dolinar_layers=kwargs.get(\"dolinar_layers\", 2)\n",
    "        number_phases=kwargs.get(\"number_phases\", 2)\n",
    "        super().__init__(amplitude=amplitude, dolinar_layers=dolinar_layers, number_phases=number_phases)\n",
    "        \n",
    "        displacement_tree = {}\n",
    "        #self.at = make_attenuations(self.number_layers)\n",
    "        for layer in range(self.dolinar_layers+1):\n",
    "            displacement_tree[str(layer)] = {}\n",
    "\n",
    "        for k in outcomes_universe(self.dolinar_layers):\n",
    "            for layer in range(self.dolinar_layers+1):\n",
    "                displacement_tree[str(layer)][str(k[:layer])] = 0\n",
    "                \n",
    "        self.displacement_tree = displacement_tree\n",
    "        \n",
    "    def random_tree(self):\n",
    "        actions = self.displacement_tree.copy()\n",
    "        for k in outcomes_universe(self.dolinar_layers):\n",
    "            for layer in range(self.dolinar_layers+1):\n",
    "                actions[str(layer)][str(k[:layer])] = np.random.random()\n",
    "        return actions\n",
    "    \n",
    "    def success_probability(self, displacements_tree):\n",
    "        \"\"\"\n",
    "        Given a tree of conditional actions (on the outcomes history), computes\n",
    "        the success probability. Notice the final action is the guess\n",
    "        for the phase of the state given a given branch.\n",
    "        \"\"\"\n",
    "        p=0\n",
    "        for ot in outcomes_universe(self.dolinar_layers):\n",
    "            c=1\n",
    "            for layer in range(self.dolinar_layers):\n",
    "                eff_at = np.prod(np.sin(self.at[:layer]))*np.cos(self.at[layer])\n",
    "                c*=P(displacements_tree[str(self.dolinar_layers)][str(ot)]*self.amplitude,\n",
    "                     displacements_tree[str(layer)][str(ot[:(layer)])], eff_at, ot[self.dolinar_layers-1-layer] ) #notice i respect that the columns of the outcomes_universe\n",
    "                #correspond to the layer: the last column is the first layer.\n",
    "            p += c\n",
    "        return p/self.number_phases\n",
    "    \n",
    "    \n",
    "    def greedy_strategy(self, actor, critic):\n",
    "        \"\"\"Assuming actor, critic and self have the same dolinar_layers.\n",
    "            self.possible_phases are the possible phases of the coherent states\n",
    "\n",
    "        \"\"\"\n",
    "        rr = np.ones((2**(self.dolinar_layers-1), self.dolinar_layers, 1))*actor.pad_value\n",
    "        rr[:,1:] = np.reshape(outcomes_universe(self.dolinar_layers-1),(2**(self.dolinar_layers-1), self.dolinar_layers-1,1))\n",
    "        preds = np.squeeze(actor(rr))\n",
    "\n",
    "        for ot, seqot in zip(outcomes_universe(self.dolinar_layers-1), preds):\n",
    "            for layer in range(self.dolinar_layers):\n",
    "                self.displacement_tree[str(layer)][str(ot[:layer])] = seqot[layer]\n",
    "\n",
    "            history = []\n",
    "            index_seqot, index_ot= 0, 0\n",
    "            for index_history in range(2*self.dolinar_layers-1):\n",
    "                if index_history%2==0:\n",
    "                    history.append(seqot[index_seqot])\n",
    "                    index_seqot+=1\n",
    "                else:\n",
    "                    history.append(ot[index_ot])\n",
    "                    index_ot+=1   \n",
    "            for final_outcome in [0,1]:\n",
    "                final_history = np.append(history, final_outcome)\n",
    "                self.displacement_tree[str(self.dolinar_layers)][str(np.append(ot,final_outcome))] = self.possible_phases[critic.give_favourite_guess(final_history)[0]]\n",
    "\n",
    "        return self.success_probability(displacement_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dolinar_layers=2\n",
    "\n",
    "policy_evaluator = PolicyEvaluator(amplitude = amplitude, dolinar_layers=dolinar_layers, number_phases = number_phases)\n",
    "env = Environment(amplitude=0.4, dolinar_layers = dolinar_layers)\n",
    "buffer = ReplayBuffer(buffer_size=buffer_size)\n",
    "\n",
    "critic = Critic(nature=\"primary\",valreg=0.01, dolinar_layers = dolinar_layers, number_phases=number_phases)\n",
    "critic_target = Critic(nature=\"target\", dolinar_layers = dolinar_layers, number_phases=number_phases)\n",
    "actor = Actor(nature=\"primary\", dolinar_layers = dolinar_layers)\n",
    "actor_target = Actor(nature=\"target\", dolinar_layers = dolinar_layers)\n",
    "\n",
    "\n",
    "rt = []\n",
    "pt = []\n",
    "new_loss=0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer critic_8 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer actor_8 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-d2c4c97fd837>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_evaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgreedy_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-96d99f1b40a1>\u001b[0m in \u001b[0;36mgreedy_strategy\u001b[0;34m(self, actor, critic)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseqot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutcomes_universe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdolinar_layers\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdolinar_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplacement_tree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseqot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "for episode in tqdm(range(10)):\n",
    "\n",
    "    env.pick_phase()\n",
    "    experiences=[] #where the current history of the current episode is stored\n",
    "    context_outcome_actor = np.reshape(np.array([actor.pad_value]),(1,1,1)).astype(np.float32)\n",
    "    for layer in range(actor.dolinar_layers):\n",
    "        beta_would_do = np.squeeze(actor(context_outcome_actor))\n",
    "        beta =  beta_would_do + np.random.uniform(-noise_displacement, noise_displacement)\n",
    "        outcome = env.give_outcome(beta,layer)\n",
    "        experiences.append(beta)\n",
    "        experiences.append(outcome)\n",
    "\n",
    "        context_outcome_actor = np.reshape(np.array([outcome]),(1,1,1)).astype(np.float32)\n",
    "\n",
    "    ### ep-gredy guessing of the phase###\n",
    "    if np.random.random()< ep_guess:\n",
    "        val = np.random.choice(range(number_phases),1)[0]\n",
    "        guess_index, guess_input_network = val, val/critic.number_phases\n",
    "    else:\n",
    "        guess_index, guess_input_network = critic.give_favourite_guess(experiences) #experiences is the branch of the current tree of actions + outcomes.\n",
    "    experiences.append(guess_input_network)\n",
    "\n",
    "    reward = env.give_reward(guess_input_network)\n",
    "    experiences.append(reward)\n",
    "    buffer.add(tuple(experiences))\n",
    "    \n",
    "    actor.lstm.stateful=False\n",
    "    pt = policy_evaluator.greedy_strategy(actor = actor, critic = critic)\n",
    "    actor.lstm.stateful=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacement_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_strategy(self, actor, critic):\n",
    "    \"\"\"Assuming actor, critic and self have the same dolinar_layers.\n",
    "        self.possible_phases are the possible phases of the coherent states\n",
    "    \n",
    "    \"\"\"\n",
    "    rr = np.ones((2**(self.dolinar_layers-1), self.dolinar_layers, 1))*actor.pad_value\n",
    "    rr[:,1:] = np.reshape(outcomes_universe(actor.dolinar_layers-1),(2**(self.dolinar_layers-1), self.dolinar_layers-1,1))\n",
    "    preds = np.squeeze(actor(rr))\n",
    "\n",
    "    for ot, seqot in zip(outcomes_universe(actor.dolinar_layers-1), preds):\n",
    "        for layer in range(actor.dolinar_layers):\n",
    "            self.displacement_tree[str(layer)][str(ot[:layer])] = seqot[layer]\n",
    "\n",
    "        history = []\n",
    "        index_seqot, index_ot= 0, 0\n",
    "        for index_history in range(2*actor.dolinar_layers-1):\n",
    "            if index_history%2==0:\n",
    "                history.append(seqot[index_seqot])\n",
    "                index_seqot+=1\n",
    "            else:\n",
    "                history.append(ot[index_ot])\n",
    "                index_ot+=1   \n",
    "        for final_outcome in [0,1]:\n",
    "            final_history = np.append(history, final_outcome)\n",
    "            self.displacement_tree[str(actor.dolinar_layers)][str(np.append(ot,final_outcome))] = self.possible_phases[critic.give_favourite_guess(final_history)[0]]\n",
    "            \n",
    "    return self.success_probability(displacement_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = Actions(number_layers=3, number_phases=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions.success_probability(displacement_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic.dolinar_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_phases = env.possible_phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
