{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nets import Actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1, 1), dtype=float32, numpy=array([[[-0.05466267]]], dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor = Actor(nature=\"primary\")\n",
    "assert actor.lstm.stateful\n",
    "context_outcome_actor = np.reshape(np.array([actor.pad_value]),(1,1,1)).astype(np.float32)\n",
    "actor(context_outcome_actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars1 = actor.lstm.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = Actor(nature=\"primary\")\n",
    "assert actor.lstm.stateful\n",
    "context_outcome_actor = np.reshape(np.array([actor.pad_value]),(1,1,1)).astype(np.float32)\n",
    "actor(context_outcome_actor)\n",
    "\n",
    "@tf.function\n",
    "def eval(actor):\n",
    "    actor.lstm.stateful=False\n",
    "    context_outcome_actor = np.reshape(np.array([actor.pad_value]*40),(40,1,1)).astype(np.float32)\n",
    "    return actor(context_outcome_actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    <ipython-input-20-a6c23f6fcb2a>:5 eval  *\n        return actor(context_outcome_actor)\n    /home/cooper-cooper/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py:968 __call__  **\n        outputs = self.call(cast_inputs, *args, **kwargs)\n    /home/cooper-cooper/Desktop/deeper/nets.py:230 call\n        feat= self.lstm(feat)\n    /home/cooper-cooper/.local/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py:654 __call__\n        return super(RNN, self).__call__(inputs, **kwargs)\n    /home/cooper-cooper/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py:886 __call__\n        self.name)\n    /home/cooper-cooper/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/input_spec.py:227 assert_input_compatibility\n        ', found shape=' + str(shape))\n\n    ValueError: Input 0 is incompatible with layer lstm_3: expected shape=(1, None, 1), found shape=[40, 1, 1]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-7cf35d893e67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    625\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    504\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    505\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 506\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2444\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2445\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2446\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2447\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2777\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2778\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2665\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2666\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2667\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2668\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2669\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    <ipython-input-20-a6c23f6fcb2a>:5 eval  *\n        return actor(context_outcome_actor)\n    /home/cooper-cooper/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py:968 __call__  **\n        outputs = self.call(cast_inputs, *args, **kwargs)\n    /home/cooper-cooper/Desktop/deeper/nets.py:230 call\n        feat= self.lstm(feat)\n    /home/cooper-cooper/.local/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py:654 __call__\n        return super(RNN, self).__call__(inputs, **kwargs)\n    /home/cooper-cooper/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py:886 __call__\n        self.name)\n    /home/cooper-cooper/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/input_spec.py:227 assert_input_compatibility\n        ', found shape=' + str(shape))\n\n    ValueError: Input 0 is incompatible with layer lstm_3: expected shape=(1, None, 1), found shape=[40, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "eval(actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "##### ACTOR CLASSS ####\n",
    "class Actor(tf.keras.Model):\n",
    "    def __init__(self, nature, valreg=0.01, seed_val=0.1, pad_value = -7.,\n",
    "                 dolinar_layers=2,tau=0.01):\n",
    "        super(Actor,self).__init__()\n",
    "        self.dolinar_layers = dolinar_layers\n",
    "        self.pad_value = pad_value\n",
    "        self.nature = nature\n",
    "        self.tau = tau\n",
    "\n",
    "\n",
    "        if nature == \"primary\":\n",
    "            self.dropout_rate = 0.1\n",
    "            self.lstm = tf.keras.layers.LSTM(500, return_sequences=True, stateful=True)\n",
    "            self.mask = tf.keras.layers.Masking(mask_value=pad_value,\n",
    "                                  input_shape=(1,1))#CHECK\n",
    "\n",
    "\n",
    "        elif nature == \"target\":\n",
    "            self.dropout_rate = 0.\n",
    "            self.lstm = tf.keras.layers.LSTM(500, return_sequences=True, stateful=True)\n",
    "            self.mask = tf.keras.layers.Masking(mask_value=pad_value,\n",
    "                                  input_shape=(1,1))#CHECK\n",
    "\n",
    "            #\n",
    "            # self.lstm = tf.keras.layers.LSTM(500, return_sequences=True, stateful=False)\n",
    "            # self.mask = tf.keras.layers.Masking(mask_value=pad_value, input_shape=(1,1))\n",
    "            #                       input_shape=(self.dolinar_layers, 1)) #'cause i feed altoghether.\n",
    "        else:\n",
    "            print(\"Hey! the character is either primary or target\")\n",
    "            \n",
    "        self.l0 = tf.keras.layers.Input(shape=(None,1,1))\n",
    "        self.l1 = Dense(500,kernel_initializer=tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val),\n",
    "        bias_initializer = tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val),\n",
    "        kernel_regularizer=tf.keras.regularizers.l1(valreg),\n",
    "    activity_regularizer=tf.keras.regularizers.l2(valreg), dtype='float32')\n",
    "\n",
    "        self.l2 = Dense(300, kernel_regularizer=tf.keras.regularizers.l1(valreg),\n",
    "    activity_regularizer=tf.keras.regularizers.l2(valreg),\n",
    "    kernel_initializer=tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val),\n",
    "    bias_initializer = tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val), dtype='float32')\n",
    "\n",
    "        self.l3 = Dense(300, kernel_regularizer=tf.keras.regularizers.l1(valreg),\n",
    "    activity_regularizer=tf.keras.regularizers.l2(valreg),\n",
    "    kernel_initializer=tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val),\n",
    "    bias_initializer = tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val), dtype='float32')\n",
    "\n",
    "        self.l4 = Dense(1, kernel_regularizer=tf.keras.regularizers.l1(valreg),\n",
    "    activity_regularizer=tf.keras.regularizers.l2(valreg),\n",
    "    kernel_initializer=tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val),\n",
    "    bias_initializer = tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val), dtype='float32')\n",
    "\n",
    "\n",
    "\n",
    "    def update_target_parameters(self,primary_net):\n",
    "        prim_weights = primary_net.get_weights()\n",
    "        targ_weights = self.get_weights()\n",
    "        weights = []\n",
    "        for i in tf.range(len(prim_weights)):\n",
    "            weights.append(self.tau * prim_weights[i] + (1 - self.tau) * targ_weights[i])\n",
    "        self.set_weights(weights)\n",
    "        return\n",
    "\n",
    "    def call(self, inputs):\n",
    "        feat = self.mask(feat)\n",
    "        feat= self.lstm(feat)\n",
    "        feat = tf.nn.dropout(feat, rate=self.dropout_rate)\n",
    "        feat = tf.nn.relu(self.l1(feat))\n",
    "        feat = tf.nn.dropout(feat, rate=self.dropout_rate)\n",
    "        feat = tf.nn.relu(self.l2(feat))\n",
    "        feat = tf.nn.relu(self.l3(feat))\n",
    "        feat = tf.nn.tanh(self.l4(feat))\n",
    "        feat = tf.clip_by_value(feat, -1.0, 1.0)\n",
    "        return feat\n",
    "\n",
    "    def process_sequence_of_experiences(self, experiences):\n",
    "        self.lstm.stateful=True\n",
    "        export = experiences.copy()\n",
    "        for index in range(1,2*self.dolinar_layers-1,2): # I consider from first outcome to last one (but guess)\n",
    "            export[:,index+1] = np.squeeze(self(np.reshape(np.array(export[:,index]),\n",
    "                                                                 (experiences.shape[0],1,1))))\n",
    "        self.lstm.stateful=False\n",
    "\n",
    "        return export\n",
    "\n",
    "    #@tf.function\n",
    "    def process_sequence_of_experiences_tf(self, experiences):\n",
    "\n",
    "        unstacked_exp = tf.unstack(tf.convert_to_tensor(experiences), axis=1)\n",
    "        to_stack = []\n",
    "        for index in range(2*self.dolinar_layers-1): # I consider from first outcome to last one (but guess)\n",
    "            if (index==0):\n",
    "                to_stack.append(unstacked_exp[index])\n",
    "            if (index%2 == 1):\n",
    "                to_stack.append(unstacked_exp[index])\n",
    "\n",
    "                to_stack.append(tf.squeeze(self(tf.reshape(unstacked_exp[index],(experiences.shape[0],1,1)))))\n",
    "        for index in range(2*self.dolinar_layers-1, 2*self.dolinar_layers+2):\n",
    "            to_stack.append(unstacked_exp[index])\n",
    "        self.lstm.reset_states()\n",
    "\n",
    "        return tf.stack(to_stack, axis=1)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Tensor' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-dc4335a42f64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mactor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mActor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"primary\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcontext_outcome_actor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_value\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mactor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_outcome_actor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    967\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-bcf4050ad32b>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mfeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mfeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mfeat\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Tensor' object is not callable"
     ]
    }
   ],
   "source": [
    "actor = Actor(nature=\"primary\")\n",
    "context_outcome_actor = np.reshape(np.array([actor.pad_value]),(1,1,1)).astype(np.float32)\n",
    "actor(context_outcome_actor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = tf.keras.layers.Input(shape=(None,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Tensor in module tensorflow.python.framework.ops object:\n",
      "\n",
      "class Tensor(tensorflow.python.framework.tensor_like._TensorLike)\n",
      " |  A tensor represents a rectangular array of data.\n",
      " |  \n",
      " |  When writing a TensorFlow program, the main object you manipulate and pass\n",
      " |  around is the `tf.Tensor`. A `tf.Tensor` object represents a rectangular array\n",
      " |  of arbitrary dimension, filled with data of a specific data type.\n",
      " |  \n",
      " |  A `tf.Tensor` has the following properties:\n",
      " |  \n",
      " |  * a data type (float32, int32, or string, for example)\n",
      " |  * a shape\n",
      " |  \n",
      " |  Each element in the Tensor has the same data type, and the data type is always\n",
      " |  known.\n",
      " |  \n",
      " |  In eager execution, which is the default mode in TensorFlow, results are\n",
      " |  calculated immediately.\n",
      " |  \n",
      " |  >>> # Compute some values using a Tensor\n",
      " |  >>> c = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
      " |  >>> d = tf.constant([[1.0, 1.0], [0.0, 1.0]])\n",
      " |  >>> e = tf.matmul(c, d)\n",
      " |  >>> print(e)\n",
      " |  tf.Tensor(\n",
      " |  [[1. 3.]\n",
      " |   [3. 7.]], shape=(2, 2), dtype=float32)\n",
      " |  \n",
      " |  \n",
      " |  Note that during eager execution, you may discover your `Tensors` are actually\n",
      " |  of type `EagerTensor`.  This is an internal detail, but it does give you\n",
      " |  access to a useful function, `numpy`:\n",
      " |  \n",
      " |  >>> type(e)\n",
      " |  <class '...ops.EagerTensor'>\n",
      " |  >>> print(e.numpy())\n",
      " |    [[1. 3.]\n",
      " |     [3. 7.]]\n",
      " |  \n",
      " |  TensorFlow can define computations without immediately executing them, most\n",
      " |  commonly inside `tf.function`s, as well as in (legacy) Graph mode. In those\n",
      " |  cases, the shape (that is, the rank of the Tensor and the size of\n",
      " |  each dimension) might be only partially known.\n",
      " |  \n",
      " |  Most operations produce tensors of fully-known shapes if the shapes of their\n",
      " |  inputs are also fully known, but in some cases it's only possible to find the\n",
      " |  shape of a tensor at execution time.\n",
      " |  \n",
      " |  There are specialized tensors; for these, see `tf.Variable`, `tf.constant`,\n",
      " |  `tf.placeholder`, `tf.SparseTensor`, and `tf.RaggedTensor`.\n",
      " |  \n",
      " |  For more on Tensors, see the [guide](https://tensorflow.org/guide/tensor`).\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Tensor\n",
      " |      tensorflow.python.framework.tensor_like._TensorLike\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __abs__ = abs(x, name=None)\n",
      " |      Computes the absolute value of a tensor.\n",
      " |      \n",
      " |      Given a tensor of integer or floating-point values, this operation returns a\n",
      " |      tensor of the same type, where each element contains the absolute value of the\n",
      " |      corresponding element in the input.\n",
      " |      \n",
      " |      Given a tensor `x` of complex numbers, this operation returns a tensor of type\n",
      " |      `float32` or `float64` that is the absolute value of each element in `x`. For\n",
      " |      a complex number \\\\(a + bj\\\\), its absolute value is computed as \\\\(\\sqrt{a^2\n",
      " |      + b^2}\\\\).  For example:\n",
      " |      \n",
      " |      >>> x = tf.constant([[-2.25 + 4.75j], [-3.25 + 5.75j]])\n",
      " |      >>> tf.abs(x)\n",
      " |      <tf.Tensor: shape=(2, 1), dtype=float64, numpy=\n",
      " |      array([[5.25594901],\n",
      " |             [6.60492241]])>\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` or `SparseTensor` of type `float16`, `float32`, `float64`,\n",
      " |          `int32`, `int64`, `complex64` or `complex128`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` or `SparseTensor` of the same size, type and sparsity as `x`,\n",
      " |          with absolute values. Note, for `complex64` or `complex128` input, the\n",
      " |          returned `Tensor` will be of type `float32` or `float64`, respectively.\n",
      " |      \n",
      " |        If `x` is a `SparseTensor`, returns\n",
      " |        `SparseTensor(x.indices, tf.math.abs(x.values, ...), x.dense_shape)`\n",
      " |  \n",
      " |  __add__ = binary_op_wrapper(x, y)\n",
      " |      Dispatches to add for strings and add_v2 for all other types.\n",
      " |  \n",
      " |  __and__ = binary_op_wrapper(x, y)\n",
      " |      Returns the truth value of x AND y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `LogicalAnd` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `bool`.\n",
      " |        y: A `Tensor` of type `bool`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __array__(self)\n",
      " |  \n",
      " |  __bool__(self)\n",
      " |      Dummy method to prevent a tensor from being used as a Python `bool`.\n",
      " |      \n",
      " |      This overload raises a `TypeError` when the user inadvertently\n",
      " |      treats a `Tensor` as a boolean (most commonly in an `if` or `while`\n",
      " |      statement), in code that was not converted by AutoGraph. For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      if tf.constant(True):  # Will raise.\n",
      " |        # ...\n",
      " |      \n",
      " |      if tf.constant(5) < tf.constant(7):  # Will raise.\n",
      " |        # ...\n",
      " |      ```\n",
      " |      \n",
      " |      Raises:\n",
      " |        `TypeError`.\n",
      " |  \n",
      " |  __copy__(self)\n",
      " |  \n",
      " |  __div__ = binary_op_wrapper(x, y)\n",
      " |      Divide two values using Python 2 semantics.\n",
      " |      \n",
      " |      Used for Tensor.__div__.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: `Tensor` numerator of real numeric type.\n",
      " |        y: `Tensor` denominator of real numeric type.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        `x / y` returns the quotient of x and y.\n",
      " |  \n",
      " |  __eq__ = tensor_equals(self, other)\n",
      " |      Compares two tensors element-wise for equality.\n",
      " |  \n",
      " |  __floordiv__ = binary_op_wrapper(x, y)\n",
      " |      Divides `x / y` elementwise, rounding toward the most negative integer.\n",
      " |      \n",
      " |      The same as `tf.compat.v1.div(x,y)` for integers, but uses\n",
      " |      `tf.floor(tf.compat.v1.div(x,y))` for\n",
      " |      floating point arguments so that the result is always an integer (though\n",
      " |      possibly an integer represented as floating point).  This op is generated by\n",
      " |      `x // y` floor division in Python 3 and in Python 2.7 with\n",
      " |      `from __future__ import division`.\n",
      " |      \n",
      " |      `x` and `y` must have the same type, and the result will have the same type\n",
      " |      as well.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: `Tensor` numerator of real numeric type.\n",
      " |        y: `Tensor` denominator of real numeric type.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        `x / y` rounded down.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If the inputs are complex.\n",
      " |  \n",
      " |  __ge__ = greater_equal(x, y, name=None)\n",
      " |      Returns the truth value of (x >= y) element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.greater_equal` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      x = tf.constant([5, 4, 6, 7])\n",
      " |      y = tf.constant([5, 2, 5, 10])\n",
      " |      tf.math.greater_equal(x, y) ==> [True, True, True, False]\n",
      " |      \n",
      " |      x = tf.constant([5, 4, 6, 7])\n",
      " |      y = tf.constant([5])\n",
      " |      tf.math.greater_equal(x, y) ==> [True, False, True, True]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __getitem__ = _slice_helper(tensor, slice_spec, var=None)\n",
      " |      Overload for Tensor.__getitem__.\n",
      " |      \n",
      " |      This operation extracts the specified region from the tensor.\n",
      " |      The notation is similar to NumPy with the restriction that\n",
      " |      currently only support basic indexing. That means that\n",
      " |      using a non-scalar tensor as input is not currently allowed.\n",
      " |      \n",
      " |      Some useful examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      # Strip leading and trailing 2 elements\n",
      " |      foo = tf.constant([1,2,3,4,5,6])\n",
      " |      print(foo[2:-2].eval())  # => [3,4]\n",
      " |      \n",
      " |      # Skip every other row and reverse the order of the columns\n",
      " |      foo = tf.constant([[1,2,3], [4,5,6], [7,8,9]])\n",
      " |      print(foo[::2,::-1].eval())  # => [[3,2,1], [9,8,7]]\n",
      " |      \n",
      " |      # Use scalar tensors as indices on both dimensions\n",
      " |      print(foo[tf.constant(0), tf.constant(2)].eval())  # => 3\n",
      " |      \n",
      " |      # Insert another dimension\n",
      " |      foo = tf.constant([[1,2,3], [4,5,6], [7,8,9]])\n",
      " |      print(foo[tf.newaxis, :, :].eval()) # => [[[1,2,3], [4,5,6], [7,8,9]]]\n",
      " |      print(foo[:, tf.newaxis, :].eval()) # => [[[1,2,3]], [[4,5,6]], [[7,8,9]]]\n",
      " |      print(foo[:, :, tf.newaxis].eval()) # => [[[1],[2],[3]], [[4],[5],[6]],\n",
      " |      [[7],[8],[9]]]\n",
      " |      \n",
      " |      # Ellipses (3 equivalent operations)\n",
      " |      foo = tf.constant([[1,2,3], [4,5,6], [7,8,9]])\n",
      " |      print(foo[tf.newaxis, :, :].eval())  # => [[[1,2,3], [4,5,6], [7,8,9]]]\n",
      " |      print(foo[tf.newaxis, ...].eval())  # => [[[1,2,3], [4,5,6], [7,8,9]]]\n",
      " |      print(foo[tf.newaxis].eval())  # => [[[1,2,3], [4,5,6], [7,8,9]]]\n",
      " |      \n",
      " |      # Masks\n",
      " |      foo = tf.constant([[1,2,3], [4,5,6], [7,8,9]])\n",
      " |      print(foo[foo > 2].eval())  # => [3, 4, 5, 6, 7, 8, 9]\n",
      " |      ```\n",
      " |      \n",
      " |      Notes:\n",
      " |        - `tf.newaxis` is `None` as in NumPy.\n",
      " |        - An implicit ellipsis is placed at the end of the `slice_spec`\n",
      " |        - NumPy advanced indexing is currently not supported.\n",
      " |      \n",
      " |      Args:\n",
      " |        tensor: An ops.Tensor object.\n",
      " |        slice_spec: The arguments to Tensor.__getitem__.\n",
      " |        var: In the case of variable slice assignment, the Variable object to slice\n",
      " |          (i.e. tensor is the read-only view of this variable).\n",
      " |      \n",
      " |      Returns:\n",
      " |        The appropriate slice of \"tensor\", based on \"slice_spec\".\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If a slice range is negative size.\n",
      " |        TypeError: If the slice indices aren't int, slice, ellipsis,\n",
      " |          tf.newaxis or scalar int32/int64 tensors.\n",
      " |  \n",
      " |  __gt__ = greater(x, y, name=None)\n",
      " |      Returns the truth value of (x > y) element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.greater` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      x = tf.constant([5, 4, 6])\n",
      " |      y = tf.constant([5, 2, 5])\n",
      " |      tf.math.greater(x, y) ==> [False, True, True]\n",
      " |      \n",
      " |      x = tf.constant([5, 4, 6])\n",
      " |      y = tf.constant([5])\n",
      " |      tf.math.greater(x, y) ==> [False, False, True]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __hash__(self)\n",
      " |      Return hash(self).\n",
      " |  \n",
      " |  __init__(self, op, value_index, dtype)\n",
      " |      Creates a new `Tensor`.\n",
      " |      \n",
      " |      Args:\n",
      " |        op: An `Operation`. `Operation` that computes this tensor.\n",
      " |        value_index: An `int`. Index of the operation's endpoint that produces\n",
      " |          this tensor.\n",
      " |        dtype: A `DType`. Type of elements stored in this tensor.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If the op is not an `Operation`.\n",
      " |  \n",
      " |  __invert__ = logical_not(x, name=None)\n",
      " |      Returns the truth value of `NOT x` element-wise.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      >>> tf.math.logical_not(tf.constant([True, False]))\n",
      " |      <tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])>\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `bool`. A `Tensor` of type `bool`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |  \n",
      " |  __le__ = less_equal(x, y, name=None)\n",
      " |      Returns the truth value of (x <= y) element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.less_equal` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      x = tf.constant([5, 4, 6])\n",
      " |      y = tf.constant([5])\n",
      " |      tf.math.less_equal(x, y) ==> [True, True, False]\n",
      " |      \n",
      " |      x = tf.constant([5, 4, 6])\n",
      " |      y = tf.constant([5, 6, 6])\n",
      " |      tf.math.less_equal(x, y) ==> [True, True, True]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |  \n",
      " |  __lt__ = less(x, y, name=None)\n",
      " |      Returns the truth value of (x < y) element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.less` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      x = tf.constant([5, 4, 6])\n",
      " |      y = tf.constant([5])\n",
      " |      tf.math.less(x, y) ==> [False, True, False]\n",
      " |      \n",
      " |      x = tf.constant([5, 4, 6])\n",
      " |      y = tf.constant([5, 6, 7])\n",
      " |      tf.math.less(x, y) ==> [False, True, True]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __matmul__ = binary_op_wrapper(x, y)\n",
      " |      Multiplies matrix `a` by matrix `b`, producing `a` * `b`.\n",
      " |      \n",
      " |      The inputs must, following any transpositions, be tensors of rank >= 2\n",
      " |      where the inner 2 dimensions specify valid matrix multiplication dimensions,\n",
      " |      and any further outer dimensions specify matching batch size.\n",
      " |      \n",
      " |      Both matrices must be of the same type. The supported types are:\n",
      " |      `float16`, `float32`, `float64`, `int32`, `complex64`, `complex128`.\n",
      " |      \n",
      " |      Either matrix can be transposed or adjointed (conjugated and transposed) on\n",
      " |      the fly by setting one of the corresponding flag to `True`. These are `False`\n",
      " |      by default.\n",
      " |      \n",
      " |      If one or both of the matrices contain a lot of zeros, a more efficient\n",
      " |      multiplication algorithm can be used by setting the corresponding\n",
      " |      `a_is_sparse` or `b_is_sparse` flag to `True`. These are `False` by default.\n",
      " |      This optimization is only available for plain matrices (rank-2 tensors) with\n",
      " |      datatypes `bfloat16` or `float32`.\n",
      " |      \n",
      " |      A simple 2-D tensor matrix multiplication:\n",
      " |      \n",
      " |      >>> a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])\n",
      " |      >>> a  # 2-D tensor\n",
      " |      <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
      " |      array([[1, 2, 3],\n",
      " |             [4, 5, 6]], dtype=int32)>\n",
      " |      >>> b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])\n",
      " |      >>> b  # 2-D tensor\n",
      " |      <tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
      " |      array([[ 7,  8],\n",
      " |             [ 9, 10],\n",
      " |             [11, 12]], dtype=int32)>\n",
      " |      >>> c = tf.matmul(a, b)\n",
      " |      >>> c  # `a` * `b`\n",
      " |      <tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
      " |      array([[ 58,  64],\n",
      " |             [139, 154]], dtype=int32)>\n",
      " |      \n",
      " |      A batch matrix multiplication with batch shape [2]:\n",
      " |      \n",
      " |      >>> a = tf.constant(np.arange(1, 13, dtype=np.int32), shape=[2, 2, 3])\n",
      " |      >>> a  # 3-D tensor\n",
      " |      <tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=\n",
      " |      array([[[ 1,  2,  3],\n",
      " |              [ 4,  5,  6]],\n",
      " |             [[ 7,  8,  9],\n",
      " |              [10, 11, 12]]], dtype=int32)>\n",
      " |      >>> b = tf.constant(np.arange(13, 25, dtype=np.int32), shape=[2, 3, 2])\n",
      " |      >>> b  # 3-D tensor\n",
      " |      <tf.Tensor: shape=(2, 3, 2), dtype=int32, numpy=\n",
      " |      array([[[13, 14],\n",
      " |              [15, 16],\n",
      " |              [17, 18]],\n",
      " |             [[19, 20],\n",
      " |              [21, 22],\n",
      " |              [23, 24]]], dtype=int32)>\n",
      " |      >>> c = tf.matmul(a, b)\n",
      " |      >>> c  # `a` * `b`\n",
      " |      <tf.Tensor: shape=(2, 2, 2), dtype=int32, numpy=\n",
      " |      array([[[ 94, 100],\n",
      " |              [229, 244]],\n",
      " |             [[508, 532],\n",
      " |              [697, 730]]], dtype=int32)>\n",
      " |      \n",
      " |      Since python >= 3.5 the @ operator is supported\n",
      " |      (see [PEP 465](https://www.python.org/dev/peps/pep-0465/)). In TensorFlow,\n",
      " |      it simply calls the `tf.matmul()` function, so the following lines are\n",
      " |      equivalent:\n",
      " |      \n",
      " |      >>> d = a @ b @ [[10], [11]]\n",
      " |      >>> d = tf.matmul(tf.matmul(a, b), [[10], [11]])\n",
      " |      \n",
      " |      Args:\n",
      " |        a: `tf.Tensor` of type `float16`, `float32`, `float64`, `int32`,\n",
      " |          `complex64`, `complex128` and rank > 1.\n",
      " |        b: `tf.Tensor` with same type and rank as `a`.\n",
      " |        transpose_a: If `True`, `a` is transposed before multiplication.\n",
      " |        transpose_b: If `True`, `b` is transposed before multiplication.\n",
      " |        adjoint_a: If `True`, `a` is conjugated and transposed before\n",
      " |          multiplication.\n",
      " |        adjoint_b: If `True`, `b` is conjugated and transposed before\n",
      " |          multiplication.\n",
      " |        a_is_sparse: If `True`, `a` is treated as a sparse matrix. Notice, this\n",
      " |          **does not support `tf.sparse.SparseTensor`**, it just makes optimizations\n",
      " |          that assume most values in `a` are zero.\n",
      " |          See `tf.sparse.sparse_dense_matmul`\n",
      " |          for some support for `tf.SparseTensor` multiplication.\n",
      " |        b_is_sparse: If `True`, `b` is treated as a sparse matrix. Notice, this\n",
      " |          **does not support `tf.sparse.SparseTensor`**, it just makes optimizations\n",
      " |          that assume most values in `a` are zero.\n",
      " |          See `tf.sparse.sparse_dense_matmul`\n",
      " |          for some support for `tf.SparseTensor` multiplication.\n",
      " |        name: Name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `tf.Tensor` of the same type as `a` and `b` where each inner-most matrix\n",
      " |        is the product of the corresponding matrices in `a` and `b`, e.g. if all\n",
      " |        transpose or adjoint attributes are `False`:\n",
      " |      \n",
      " |        `output[..., i, j] = sum_k (a[..., i, k] * b[..., k, j])`,\n",
      " |        for all indices `i`, `j`.\n",
      " |      \n",
      " |        Note: This is matrix product, not element-wise product.\n",
      " |      \n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If `transpose_a` and `adjoint_a`, or `transpose_b` and\n",
      " |          `adjoint_b` are both set to `True`.\n",
      " |  \n",
      " |  __mod__ = binary_op_wrapper(x, y)\n",
      " |      Returns element-wise remainder of division. When `x < 0` xor `y < 0` is\n",
      " |      \n",
      " |      true, this follows Python semantics in that the result here is consistent\n",
      " |      with a flooring divide. E.g. `floor(x / y) * y + mod(x, y) = x`.\n",
      " |      \n",
      " |      *NOTE*: `math.floormod` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `int32`, `int64`, `bfloat16`, `half`, `float32`, `float64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __mul__ = binary_op_wrapper(x, y)\n",
      " |      Dispatches cwise mul for \"Dense*Dense\" and \"Dense*Sparse\".\n",
      " |  \n",
      " |  __ne__ = tensor_not_equals(self, other)\n",
      " |      Compares two tensors element-wise for equality.\n",
      " |  \n",
      " |  __neg__ = neg(x, name=None)\n",
      " |      Computes numerical negative value element-wise.\n",
      " |      \n",
      " |      I.e., \\\\(y = -x\\\\).\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int32`, `int64`, `complex64`, `complex128`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |      \n",
      " |        If `x` is a `SparseTensor`, returns\n",
      " |        `SparseTensor(x.indices, tf.math.negative(x.values, ...), x.dense_shape)`\n",
      " |  \n",
      " |  __nonzero__(self)\n",
      " |      Dummy method to prevent a tensor from being used as a Python `bool`.\n",
      " |      \n",
      " |      This is the Python 2.x counterpart to `__bool__()` above.\n",
      " |      \n",
      " |      Raises:\n",
      " |        `TypeError`.\n",
      " |  \n",
      " |  __or__ = binary_op_wrapper(x, y)\n",
      " |      Returns the truth value of x OR y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.logical_or` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `bool`.\n",
      " |        y: A `Tensor` of type `bool`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __pow__ = binary_op_wrapper(x, y)\n",
      " |      Computes the power of one value to another.\n",
      " |      \n",
      " |      Given a tensor `x` and a tensor `y`, this operation computes \\\\(x^y\\\\) for\n",
      " |      corresponding elements in `x` and `y`. For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      x = tf.constant([[2, 2], [3, 3]])\n",
      " |      y = tf.constant([[8, 16], [2, 3]])\n",
      " |      tf.pow(x, y)  # [[256, 65536], [9, 27]]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,\n",
      " |          `complex64`, or `complex128`.\n",
      " |        y: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,\n",
      " |          `complex64`, or `complex128`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`.\n",
      " |  \n",
      " |  __radd__ = r_binary_op_wrapper(y, x)\n",
      " |      Dispatches to add for strings and add_v2 for all other types.\n",
      " |  \n",
      " |  __rand__ = r_binary_op_wrapper(y, x)\n",
      " |      Returns the truth value of x AND y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `LogicalAnd` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `bool`.\n",
      " |        y: A `Tensor` of type `bool`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __rdiv__ = r_binary_op_wrapper(y, x)\n",
      " |      Divide two values using Python 2 semantics.\n",
      " |      \n",
      " |      Used for Tensor.__div__.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: `Tensor` numerator of real numeric type.\n",
      " |        y: `Tensor` denominator of real numeric type.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        `x / y` returns the quotient of x and y.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __rfloordiv__ = r_binary_op_wrapper(y, x)\n",
      " |      Divides `x / y` elementwise, rounding toward the most negative integer.\n",
      " |      \n",
      " |      The same as `tf.compat.v1.div(x,y)` for integers, but uses\n",
      " |      `tf.floor(tf.compat.v1.div(x,y))` for\n",
      " |      floating point arguments so that the result is always an integer (though\n",
      " |      possibly an integer represented as floating point).  This op is generated by\n",
      " |      `x // y` floor division in Python 3 and in Python 2.7 with\n",
      " |      `from __future__ import division`.\n",
      " |      \n",
      " |      `x` and `y` must have the same type, and the result will have the same type\n",
      " |      as well.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: `Tensor` numerator of real numeric type.\n",
      " |        y: `Tensor` denominator of real numeric type.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        `x / y` rounded down.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If the inputs are complex.\n",
      " |  \n",
      " |  __rmatmul__ = r_binary_op_wrapper(y, x)\n",
      " |      Multiplies matrix `a` by matrix `b`, producing `a` * `b`.\n",
      " |      \n",
      " |      The inputs must, following any transpositions, be tensors of rank >= 2\n",
      " |      where the inner 2 dimensions specify valid matrix multiplication dimensions,\n",
      " |      and any further outer dimensions specify matching batch size.\n",
      " |      \n",
      " |      Both matrices must be of the same type. The supported types are:\n",
      " |      `float16`, `float32`, `float64`, `int32`, `complex64`, `complex128`.\n",
      " |      \n",
      " |      Either matrix can be transposed or adjointed (conjugated and transposed) on\n",
      " |      the fly by setting one of the corresponding flag to `True`. These are `False`\n",
      " |      by default.\n",
      " |      \n",
      " |      If one or both of the matrices contain a lot of zeros, a more efficient\n",
      " |      multiplication algorithm can be used by setting the corresponding\n",
      " |      `a_is_sparse` or `b_is_sparse` flag to `True`. These are `False` by default.\n",
      " |      This optimization is only available for plain matrices (rank-2 tensors) with\n",
      " |      datatypes `bfloat16` or `float32`.\n",
      " |      \n",
      " |      A simple 2-D tensor matrix multiplication:\n",
      " |      \n",
      " |      >>> a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])\n",
      " |      >>> a  # 2-D tensor\n",
      " |      <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
      " |      array([[1, 2, 3],\n",
      " |             [4, 5, 6]], dtype=int32)>\n",
      " |      >>> b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])\n",
      " |      >>> b  # 2-D tensor\n",
      " |      <tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
      " |      array([[ 7,  8],\n",
      " |             [ 9, 10],\n",
      " |             [11, 12]], dtype=int32)>\n",
      " |      >>> c = tf.matmul(a, b)\n",
      " |      >>> c  # `a` * `b`\n",
      " |      <tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
      " |      array([[ 58,  64],\n",
      " |             [139, 154]], dtype=int32)>\n",
      " |      \n",
      " |      A batch matrix multiplication with batch shape [2]:\n",
      " |      \n",
      " |      >>> a = tf.constant(np.arange(1, 13, dtype=np.int32), shape=[2, 2, 3])\n",
      " |      >>> a  # 3-D tensor\n",
      " |      <tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=\n",
      " |      array([[[ 1,  2,  3],\n",
      " |              [ 4,  5,  6]],\n",
      " |             [[ 7,  8,  9],\n",
      " |              [10, 11, 12]]], dtype=int32)>\n",
      " |      >>> b = tf.constant(np.arange(13, 25, dtype=np.int32), shape=[2, 3, 2])\n",
      " |      >>> b  # 3-D tensor\n",
      " |      <tf.Tensor: shape=(2, 3, 2), dtype=int32, numpy=\n",
      " |      array([[[13, 14],\n",
      " |              [15, 16],\n",
      " |              [17, 18]],\n",
      " |             [[19, 20],\n",
      " |              [21, 22],\n",
      " |              [23, 24]]], dtype=int32)>\n",
      " |      >>> c = tf.matmul(a, b)\n",
      " |      >>> c  # `a` * `b`\n",
      " |      <tf.Tensor: shape=(2, 2, 2), dtype=int32, numpy=\n",
      " |      array([[[ 94, 100],\n",
      " |              [229, 244]],\n",
      " |             [[508, 532],\n",
      " |              [697, 730]]], dtype=int32)>\n",
      " |      \n",
      " |      Since python >= 3.5 the @ operator is supported\n",
      " |      (see [PEP 465](https://www.python.org/dev/peps/pep-0465/)). In TensorFlow,\n",
      " |      it simply calls the `tf.matmul()` function, so the following lines are\n",
      " |      equivalent:\n",
      " |      \n",
      " |      >>> d = a @ b @ [[10], [11]]\n",
      " |      >>> d = tf.matmul(tf.matmul(a, b), [[10], [11]])\n",
      " |      \n",
      " |      Args:\n",
      " |        a: `tf.Tensor` of type `float16`, `float32`, `float64`, `int32`,\n",
      " |          `complex64`, `complex128` and rank > 1.\n",
      " |        b: `tf.Tensor` with same type and rank as `a`.\n",
      " |        transpose_a: If `True`, `a` is transposed before multiplication.\n",
      " |        transpose_b: If `True`, `b` is transposed before multiplication.\n",
      " |        adjoint_a: If `True`, `a` is conjugated and transposed before\n",
      " |          multiplication.\n",
      " |        adjoint_b: If `True`, `b` is conjugated and transposed before\n",
      " |          multiplication.\n",
      " |        a_is_sparse: If `True`, `a` is treated as a sparse matrix. Notice, this\n",
      " |          **does not support `tf.sparse.SparseTensor`**, it just makes optimizations\n",
      " |          that assume most values in `a` are zero.\n",
      " |          See `tf.sparse.sparse_dense_matmul`\n",
      " |          for some support for `tf.SparseTensor` multiplication.\n",
      " |        b_is_sparse: If `True`, `b` is treated as a sparse matrix. Notice, this\n",
      " |          **does not support `tf.sparse.SparseTensor`**, it just makes optimizations\n",
      " |          that assume most values in `a` are zero.\n",
      " |          See `tf.sparse.sparse_dense_matmul`\n",
      " |          for some support for `tf.SparseTensor` multiplication.\n",
      " |        name: Name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `tf.Tensor` of the same type as `a` and `b` where each inner-most matrix\n",
      " |        is the product of the corresponding matrices in `a` and `b`, e.g. if all\n",
      " |        transpose or adjoint attributes are `False`:\n",
      " |      \n",
      " |        `output[..., i, j] = sum_k (a[..., i, k] * b[..., k, j])`,\n",
      " |        for all indices `i`, `j`.\n",
      " |      \n",
      " |        Note: This is matrix product, not element-wise product.\n",
      " |      \n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If `transpose_a` and `adjoint_a`, or `transpose_b` and\n",
      " |          `adjoint_b` are both set to `True`.\n",
      " |  \n",
      " |  __rmod__ = r_binary_op_wrapper(y, x)\n",
      " |      Returns element-wise remainder of division. When `x < 0` xor `y < 0` is\n",
      " |      \n",
      " |      true, this follows Python semantics in that the result here is consistent\n",
      " |      with a flooring divide. E.g. `floor(x / y) * y + mod(x, y) = x`.\n",
      " |      \n",
      " |      *NOTE*: `math.floormod` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `int32`, `int64`, `bfloat16`, `half`, `float32`, `float64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __rmul__ = r_binary_op_wrapper(y, x)\n",
      " |      Dispatches cwise mul for \"Dense*Dense\" and \"Dense*Sparse\".\n",
      " |  \n",
      " |  __ror__ = r_binary_op_wrapper(y, x)\n",
      " |      Returns the truth value of x OR y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.logical_or` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `bool`.\n",
      " |        y: A `Tensor` of type `bool`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __rpow__ = r_binary_op_wrapper(y, x)\n",
      " |      Computes the power of one value to another.\n",
      " |      \n",
      " |      Given a tensor `x` and a tensor `y`, this operation computes \\\\(x^y\\\\) for\n",
      " |      corresponding elements in `x` and `y`. For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      x = tf.constant([[2, 2], [3, 3]])\n",
      " |      y = tf.constant([[8, 16], [2, 3]])\n",
      " |      tf.pow(x, y)  # [[256, 65536], [9, 27]]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,\n",
      " |          `complex64`, or `complex128`.\n",
      " |        y: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,\n",
      " |          `complex64`, or `complex128`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`.\n",
      " |  \n",
      " |  __rsub__ = r_binary_op_wrapper(y, x)\n",
      " |      Returns x - y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `Subtract` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `uint16`, `int16`, `int32`, `int64`, `complex64`, `complex128`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __rtruediv__ = r_binary_op_wrapper(y, x)\n",
      " |  \n",
      " |  __rxor__ = r_binary_op_wrapper(y, x)\n",
      " |      Logical XOR function.\n",
      " |      \n",
      " |      x ^ y = (x | y) & ~(x & y)\n",
      " |      \n",
      " |      The operation works for the following input types:\n",
      " |      \n",
      " |      - Two single elements of type `bool`\n",
      " |      - One `tf.Tensor` of type `bool` and one single `bool`, where the result will\n",
      " |        be calculated by applying logical XOR with the single element to each\n",
      " |        element in the larger Tensor.\n",
      " |      - Two `tf.Tensor` objects of type `bool` of the same shape. In this case,\n",
      " |        the result will be the element-wise logical XOR of the two input tensors.\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      >>> a = tf.constant([True])\n",
      " |      >>> b = tf.constant([False])\n",
      " |      >>> tf.math.logical_xor(a, b)\n",
      " |      <tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>\n",
      " |      \n",
      " |      >>> c = tf.constant([True])\n",
      " |      >>> x = tf.constant([False, True, True, False])\n",
      " |      >>> tf.math.logical_xor(c, x)\n",
      " |      <tf.Tensor: shape=(4,), dtype=bool, numpy=array([ True, False, False,  True])>\n",
      " |      \n",
      " |      >>> y = tf.constant([False, False, True, True])\n",
      " |      >>> z = tf.constant([False, True, False, True])\n",
      " |      >>> tf.math.logical_xor(y, z)\n",
      " |      <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False,  True,  True, False])>\n",
      " |      \n",
      " |      Args:\n",
      " |          x: A `tf.Tensor` type bool.\n",
      " |          y: A `tf.Tensor` of type bool.\n",
      " |          name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `tf.Tensor` of type bool with the same size as that of x or y.\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  __sub__ = binary_op_wrapper(x, y)\n",
      " |      Returns x - y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `Subtract` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `uint16`, `int16`, `int32`, `int64`, `complex64`, `complex128`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __truediv__ = binary_op_wrapper(x, y)\n",
      " |  \n",
      " |  __xor__ = binary_op_wrapper(x, y)\n",
      " |      Logical XOR function.\n",
      " |      \n",
      " |      x ^ y = (x | y) & ~(x & y)\n",
      " |      \n",
      " |      The operation works for the following input types:\n",
      " |      \n",
      " |      - Two single elements of type `bool`\n",
      " |      - One `tf.Tensor` of type `bool` and one single `bool`, where the result will\n",
      " |        be calculated by applying logical XOR with the single element to each\n",
      " |        element in the larger Tensor.\n",
      " |      - Two `tf.Tensor` objects of type `bool` of the same shape. In this case,\n",
      " |        the result will be the element-wise logical XOR of the two input tensors.\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      >>> a = tf.constant([True])\n",
      " |      >>> b = tf.constant([False])\n",
      " |      >>> tf.math.logical_xor(a, b)\n",
      " |      <tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>\n",
      " |      \n",
      " |      >>> c = tf.constant([True])\n",
      " |      >>> x = tf.constant([False, True, True, False])\n",
      " |      >>> tf.math.logical_xor(c, x)\n",
      " |      <tf.Tensor: shape=(4,), dtype=bool, numpy=array([ True, False, False,  True])>\n",
      " |      \n",
      " |      >>> y = tf.constant([False, False, True, True])\n",
      " |      >>> z = tf.constant([False, True, False, True])\n",
      " |      >>> tf.math.logical_xor(y, z)\n",
      " |      <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False,  True,  True, False])>\n",
      " |      \n",
      " |      Args:\n",
      " |          x: A `tf.Tensor` type bool.\n",
      " |          y: A `tf.Tensor` of type bool.\n",
      " |          name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `tf.Tensor` of type bool with the same size as that of x or y.\n",
      " |  \n",
      " |  consumers(self)\n",
      " |      Returns a list of `Operation`s that consume this tensor.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of `Operation`s.\n",
      " |  \n",
      " |  eval(self, feed_dict=None, session=None)\n",
      " |      Evaluates this tensor in a `Session`.\n",
      " |      \n",
      " |      Note: If you are not using `compat.v1` libraries, you should not need this,\n",
      " |      (or `feed_dict` or `Session`).  In eager execution (or within `tf.function`)\n",
      " |      you do not need to call `eval`.\n",
      " |      \n",
      " |      Calling this method will execute all preceding operations that\n",
      " |      produce the inputs needed for the operation that produces this\n",
      " |      tensor.\n",
      " |      \n",
      " |      *N.B.* Before invoking `Tensor.eval()`, its graph must have been\n",
      " |      launched in a session, and either a default session must be\n",
      " |      available, or `session` must be specified explicitly.\n",
      " |      \n",
      " |      Args:\n",
      " |        feed_dict: A dictionary that maps `Tensor` objects to feed values. See\n",
      " |          `tf.Session.run` for a description of the valid feed values.\n",
      " |        session: (Optional.) The `Session` to be used to evaluate this tensor. If\n",
      " |          none, the default session will be used.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A numpy array corresponding to the value of this tensor.\n",
      " |  \n",
      " |  experimental_ref(self)\n",
      " |      DEPRECATED FUNCTION\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Use ref() instead.\n",
      " |  \n",
      " |  get_shape(self)\n",
      " |      Alias of `tf.Tensor.shape`.\n",
      " |  \n",
      " |  ref(self)\n",
      " |      Returns a hashable reference object to this Tensor.\n",
      " |      \n",
      " |      The primary use case for this API is to put tensors in a set/dictionary.\n",
      " |      We can't put tensors in a set/dictionary as `tensor.__hash__()` is no longer\n",
      " |      available starting Tensorflow 2.0.\n",
      " |      \n",
      " |      The following will raise an exception starting 2.0\n",
      " |      \n",
      " |      >>> x = tf.constant(5)\n",
      " |      >>> y = tf.constant(10)\n",
      " |      >>> z = tf.constant(10)\n",
      " |      >>> tensor_set = {x, y, z}\n",
      " |      Traceback (most recent call last):\n",
      " |        ...\n",
      " |      TypeError: Tensor is unhashable. Instead, use tensor.ref() as the key.\n",
      " |      >>> tensor_dict = {x: 'five', y: 'ten'}\n",
      " |      Traceback (most recent call last):\n",
      " |        ...\n",
      " |      TypeError: Tensor is unhashable. Instead, use tensor.ref() as the key.\n",
      " |      \n",
      " |      Instead, we can use `tensor.ref()`.\n",
      " |      \n",
      " |      >>> tensor_set = {x.ref(), y.ref(), z.ref()}\n",
      " |      >>> x.ref() in tensor_set\n",
      " |      True\n",
      " |      >>> tensor_dict = {x.ref(): 'five', y.ref(): 'ten', z.ref(): 'ten'}\n",
      " |      >>> tensor_dict[y.ref()]\n",
      " |      'ten'\n",
      " |      \n",
      " |      Also, the reference object provides `.deref()` function that returns the\n",
      " |      original Tensor.\n",
      " |      \n",
      " |      >>> x = tf.constant(5)\n",
      " |      >>> x.ref().deref()\n",
      " |      <tf.Tensor: shape=(), dtype=int32, numpy=5>\n",
      " |  \n",
      " |  set_shape(self, shape)\n",
      " |      Updates the shape of this tensor.\n",
      " |      \n",
      " |      This method can be called multiple times, and will merge the given\n",
      " |      `shape` with the current shape of this tensor. It can be used to\n",
      " |      provide additional information about the shape of this tensor that\n",
      " |      cannot be inferred from the graph alone. For example, this can be used\n",
      " |      to provide additional information about the shapes of images:\n",
      " |      \n",
      " |      ```python\n",
      " |      _, image_data = tf.compat.v1.TFRecordReader(...).read(...)\n",
      " |      image = tf.image.decode_png(image_data, channels=3)\n",
      " |      \n",
      " |      # The height and width dimensions of `image` are data dependent, and\n",
      " |      # cannot be computed without executing the op.\n",
      " |      print(image.shape)\n",
      " |      ==> TensorShape([Dimension(None), Dimension(None), Dimension(3)])\n",
      " |      \n",
      " |      # We know that each image in this dataset is 28 x 28 pixels.\n",
      " |      image.set_shape([28, 28, 3])\n",
      " |      print(image.shape)\n",
      " |      ==> TensorShape([Dimension(28), Dimension(28), Dimension(3)])\n",
      " |      ```\n",
      " |      \n",
      " |      NOTE: This shape is not enforced at runtime. Setting incorrect shapes can\n",
      " |      result in inconsistencies between the statically-known graph and the runtime\n",
      " |      value of tensors. For runtime validation of the shape, use `tf.ensure_shape`\n",
      " |      instead.\n",
      " |      \n",
      " |      Args:\n",
      " |        shape: A `TensorShape` representing the shape of this tensor, a\n",
      " |          `TensorShapeProto`, a list, a tuple, or None.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If `shape` is not compatible with the current shape of\n",
      " |          this tensor.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  device\n",
      " |      The name of the device on which this tensor will be produced, or None.\n",
      " |  \n",
      " |  dtype\n",
      " |      The `DType` of elements in this tensor.\n",
      " |  \n",
      " |  graph\n",
      " |      The `Graph` that contains this tensor.\n",
      " |  \n",
      " |  name\n",
      " |      The string name of this tensor.\n",
      " |  \n",
      " |  op\n",
      " |      The `Operation` that produces this tensor as an output.\n",
      " |  \n",
      " |  shape\n",
      " |      Returns the `TensorShape` that represents the shape of this tensor.\n",
      " |      \n",
      " |      The shape is computed using shape inference functions that are\n",
      " |      registered in the Op for each `Operation`.  See\n",
      " |      `tf.TensorShape`\n",
      " |      for more details of what a shape represents.\n",
      " |      \n",
      " |      The inferred shape of a tensor is used to provide shape\n",
      " |      information without having to execute the underlying kernel. This\n",
      " |      can be used for debugging and providing early error messages. For\n",
      " |      example:\n",
      " |      \n",
      " |      ```python\n",
      " |      >>> c = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
      " |      >>> print(c.shape) # will be TensorShape([2, 3])\n",
      " |      (2, 3)\n",
      " |      \n",
      " |      >>> d = tf.constant([[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]])\n",
      " |      >>> print(d.shape)\n",
      " |      (4, 2)\n",
      " |      \n",
      " |      # Raises a ValueError, because `c` and `d` do not have compatible\n",
      " |      # inner dimensions.\n",
      " |      >>> e = tf.matmul(c, d)\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      tensorflow.python.framework.errors_impl.InvalidArgumentError: Matrix\n",
      " |      size-incompatible: In[0]: [2,3], In[1]: [4,2] [Op:MatMul] name: MatMul/\n",
      " |      \n",
      " |      # This works because we have compatible shapes.\n",
      " |      >>> f = tf.matmul(c, d, transpose_a=True, transpose_b=True)\n",
      " |      >>> print(f.shape)\n",
      " |      (3, 4)\n",
      " |      \n",
      " |      ```\n",
      " |      \n",
      " |      In some cases, the inferred shape may have unknown dimensions. If\n",
      " |      the caller has additional information about the values of these\n",
      " |      dimensions, `Tensor.set_shape()` can be used to augment the\n",
      " |      inferred shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `tf.TensorShape` representing the shape of this tensor.\n",
      " |  \n",
      " |  value_index\n",
      " |      The index of this tensor in the outputs of its `Operation`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  OVERLOADABLE_OPERATORS = {'__abs__', '__add__', '__and__', '__div__', ...\n",
      " |  \n",
      " |  __array_priority__ = 100\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.framework.tensor_like._TensorLike:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tf.function\n",
    "def eval(actor):\n",
    "    actor.lstm.stateful=False\n",
    "    context_outcome_actor = np.reshape(np.array([actor.pad_value]*40),(40,1,1)).astype(np.float32)\n",
    "    return actor(context_outcome_actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Input(shape=(None, 1,1)))\n",
    "# Add a LSTM layer with 128 internal units.\n",
    "model.add(layers.LSTM(128))\n",
    "\n",
    "# Add a Dense layer with 10 units.\n",
    "model.add(layers.Dense(10))\n",
    "\n",
    "model.summary()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
