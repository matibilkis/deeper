{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm as tqdm\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "import random\n",
    "import matplotlib\n",
    "\n",
    "from plots import *\n",
    "from misc import Prob, ps_maxlik, qval, record\n",
    "from nets import *\n",
    "from buffer import ReplayBuffer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(64, 2, 1), dtype=float32, numpy=\n",
       "array([[[0.48438182],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.48429245],\n",
       "        [1.        ]],\n",
       "\n",
       "       [[0.48446605],\n",
       "        [1.        ]],\n",
       "\n",
       "       [[0.4844515 ],\n",
       "        [1.        ]],\n",
       "\n",
       "       [[0.48444903],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.4843577 ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.48448274],\n",
       "        [1.        ]],\n",
       "\n",
       "       [[0.4842738 ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.48434904],\n",
       "        [1.        ]],\n",
       "\n",
       "       [[0.4843511 ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.48427483],\n",
       "        [1.        ]],\n",
       "\n",
       "       [[0.48421592],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.4842314 ],\n",
       "        [1.        ]],\n",
       "\n",
       "       [[0.48458126],\n",
       "        [1.        ]],\n",
       "\n",
       "       [[0.48445517],\n",
       "        [1.        ]],\n",
       "\n",
       "       [[0.4843235 ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.48431727],\n",
       "        [1.        ]],\n",
       "\n",
       "       [[0.4842989 ],\n",
       "        [1.        ]],\n",
       "\n",
       "       [[0.48438513],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.4844734 ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.48428857],\n",
       "        [1.        ]],\n",
       "\n",
       "       [[0.48432243],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.48440754],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.48441932],\n",
       "        [1.        ]],\n",
       "\n",
       "       [[0.48463944],\n",
       "        [1.        ]],\n",
       "\n",
       "       [[0.48425612],\n",
       "        [1.        ]],\n",
       "\n",
       "       [[0.4844243 ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.48432922],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.48452923],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.48425132],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.4843504 ],\n",
       "        [1.        ]],\n",
       "\n",
       "       [[0.48440623],\n",
       "        [1.        ]],\n",
       "\n",
       "       [[0.48430663],\n",
       "        [1.        ]],\n",
       "\n",
       "       [[0.48432887],\n",
       "        [1.        ]],\n",
       "\n",
       "       [[0.48453715],\n",
       "        [1.        ]],\n",
       "\n",
       "       [[0.4842749 ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.484409  ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.48435143],\n",
       "        [1.        ]],\n",
       "\n",
       "       [[0.48438278],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.48442784],\n",
       "        [1.        ]],\n",
       "\n",
       "       [[0.4844412 ],\n",
       "        [1.        ]],\n",
       "\n",
       "       [[0.4843627 ],\n",
       "        [1.        ]],\n",
       "\n",
       "       [[0.48432237],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.48427537],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.484101  ],\n",
       "        [1.        ]],\n",
       "\n",
       "       [[0.48440105],\n",
       "        [1.        ]],\n",
       "\n",
       "       [[0.48438644],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.48433888],\n",
       "        [1.        ]],\n",
       "\n",
       "       [[0.4842785 ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.48434916],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.48442113],\n",
       "        [1.        ]],\n",
       "\n",
       "       [[0.48445418],\n",
       "        [1.        ]],\n",
       "\n",
       "       [[0.48444057],\n",
       "        [1.        ]],\n",
       "\n",
       "       [[0.48443958],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.48434013],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.4843737 ],\n",
       "        [1.        ]],\n",
       "\n",
       "       [[0.4843972 ],\n",
       "        [1.        ]],\n",
       "\n",
       "       [[0.48420644],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.48442718],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.48456606],\n",
       "        [1.        ]],\n",
       "\n",
       "       [[0.48424926],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.4843204 ],\n",
       "        [1.        ]],\n",
       "\n",
       "       [[0.4842756 ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.48430315],\n",
       "        [0.        ]]], dtype=float32)>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "tf.executing_eagerly()\n",
    "\n",
    "class Critic(tf.keras.Model):\n",
    "    #input_dim: 1 if layer=0, 3 if layer= 2, for the Kennedy receiver ##\n",
    "    def __init__(self, valreg=0.01, seed_val=0.1, pad_value=-7.):\n",
    "        super(Critic,self).__init__()\n",
    "\n",
    "        self.pad_value = pad_value\n",
    "        self.mask = tf.keras.layers.Masking(mask_value=pad_value,\n",
    "                                  input_shape=(2, 2))\n",
    "        self.lstm = tf.keras.layers.LSTM(250, return_sequences=True)\n",
    "\n",
    "        self.l1 = Dense(50,kernel_initializer=tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val),\n",
    "        bias_initializer = tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val),\n",
    "        kernel_regularizer=tf.keras.regularizers.l1(valreg),\n",
    "    activity_regularizer=tf.keras.regularizers.l2(valreg), dtype=tf.float32)\n",
    "\n",
    "        self.l2 = Dense(50, kernel_regularizer=tf.keras.regularizers.l1(valreg),\n",
    "    activity_regularizer=tf.keras.regularizers.l2(valreg),\n",
    "    kernel_initializer=tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val),\n",
    "    bias_initializer = tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val), dtype=tf.float32)\n",
    "\n",
    "        self.l3 = Dense(1, kernel_regularizer=tf.keras.regularizers.l1(valreg),\n",
    "    activity_regularizer=tf.keras.regularizers.l2(valreg),\n",
    "    kernel_initializer=tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val),\n",
    "    bias_initializer = tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val), dtype=tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def update_target_parameters(self,primary_net, tau=0.01):\n",
    "        #### only\n",
    "        prim_weights = primary_net.get_weights()\n",
    "        targ_weights = self.get_weights()\n",
    "        weights = []\n",
    "        for i in tf.range(len(prim_weights)):\n",
    "            weights.append(tau * prim_weights[i] + (1 - tau) * targ_weights[i])\n",
    "        self.set_weights(weights)\n",
    "        return\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        feat = self.mask(inputs)\n",
    "\n",
    "        feat= self.lstm(feat)\n",
    "        feat = tf.nn.dropout(feat, rate=0.01)\n",
    "\n",
    "        feat = tf.nn.relu(self.l1(feat))\n",
    "        feat = tf.nn.dropout(feat, rate=0.01)\n",
    "\n",
    "        feat = tf.nn.relu(self.l2(feat))\n",
    "        feat = tf.nn.sigmoid(self.l3(feat))\n",
    "        return feat\n",
    "\n",
    "\n",
    "    def process_sequence(self,sample_buffer, pad_value = -4., LAYERS=1):\n",
    "        \"\"\"\" gets data obtained from N experiments: data.shape = (N, 2L+1),\n",
    "        where +1 accounts for the guess and 2L for (beta, outcome).\n",
    "\n",
    "        [[a0, o1, a1, o2, a2, o3, a4]\n",
    "         [same but other experiment]\n",
    "\n",
    "        ]\n",
    "\n",
    "        and returns an array of shape (experiments, queries_RNN, 2 ), as accepted by an RNN\n",
    "        The idea is that i input [\\beta, pad_value], and then [outcome, guess].\n",
    "\n",
    "        Or if I have two layers [\\beta, pa_value], [outcome, beta2], [outcome, guess],\n",
    "\n",
    "        so the number of \"queries\" to the RNN is layers+1,\n",
    "        and i'm always interested in putting 2 values more.\n",
    "\n",
    "        \"\"\"\n",
    "        batch_size = sample_buffer.shape[0]\n",
    "        data = sample_buffer[:,0:(LAYERS+1+1)]\n",
    "        pad_value = -4.\n",
    "        padded_data = np.ones((batch_size,LAYERS+1, 2))*pad_value\n",
    "        padded_data[:,0][:,0] = data[:,0]\n",
    "        for k in range(1,LAYERS+1):\n",
    "            padded_data[:,k] = data[:,[k,k+1]]\n",
    "\n",
    "        rewards_obtained = np.zeros((batch_size, LAYERS+1)).astype(np.float32)\n",
    "        rewards_obtained[:,-1] = sample_buffer[:,-1]\n",
    "        return padded_data, rewards_obtained\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def process_sequence_tf(self, sample_buffer, pad_value = -4., LAYERS=1):\n",
    "        sample_buffer = tf.convert_to_tensor(experiences.astype(np.float32))\n",
    "        first = tf.stack([sample_buffer[:,0], pad_value*tf.ones((64,))], axis=-1)\n",
    "        for k in range(1,LAYERS+1):\n",
    "            to_stack = tf.stack([sample_buffer[:,k], sample_buffer[:,k+1]], axis=-1)\n",
    "            first = tf.stack([first, to_stack], axis=1)\n",
    "\n",
    "        rewards = tf.zeros((sample_buffer.shape[0]))\n",
    "        rewards = tf.stack([rewards,sample_buffer[:,-1]], axis=-1)\n",
    "        rewards = tf.expand_dims(rewards, axis=2)\n",
    "        return first, rewards\n",
    "    \n",
    "    def pad_single_sequence(self, seq, pad_value = -4., LAYERS=1):\n",
    "        \"\"\"\"\n",
    "        input: [a0, o1, a1, o2, a2, o3, a4]\n",
    "\n",
    "        output: [[a0, pad], [o1, a1], [...]]\n",
    "\n",
    "        the cool thing is that then you can put this to predict the greedy guess/action.\n",
    "        \"\"\"\n",
    "        pad_value = -4.\n",
    "        padded_data = np.ones((1,LAYERS+1, 2))*pad_value\n",
    "        padded_data[0][0][0] = seq[0]\n",
    "        #padded_data[0][0] = data[0]\n",
    "        for k in range(1,LAYERS+1):\n",
    "            padded_data[0][k] = seq[k:(k+2)]\n",
    "        return padded_data\n",
    "\n",
    "\n",
    "    def give_td_error_Kennedy_guess(self,batched_input,sequential_rews_with_zeros):\n",
    "        '''\n",
    "        this function takes a batch with its corresponding labels\n",
    "        and retrieves what the true labels are according to network\n",
    "        prodection on next states.\n",
    "\n",
    "        For instance, my datapoint is [(\\beta, pad), (n, guess)]\n",
    "        and i want [Max_g Q(\\beta, n, guess), reward].\n",
    "\n",
    "\n",
    "        TO DO: extend this to more layers!!!\n",
    "\n",
    "        So what you want is\n",
    "        [Max_{a_1} Q(a0, o1, a_1),\n",
    "        Max_{a_2} Q(a0, o1, a_1, o2, a_2)\n",
    "        ,...,\n",
    "        Max_g Q(h, guess)]\n",
    "\n",
    "        But of course, we can't take the Max_g, so we replace by the target actor's choice !!!\n",
    "        '''\n",
    "        b = batched_input.copy()\n",
    "        ll = sequential_rews_with_zeros.copy()\n",
    "        preds1 = self(b)\n",
    "        b[:,1][:,1] = -b[:,1][:,1]\n",
    "        preds2 = self(b)\n",
    "        both = tf.concat([preds1,preds2],1)\n",
    "        maxs = np.squeeze(tf.math.reduce_max(both,axis=1).numpy())\n",
    "        ll[:,0] = maxs + ll[:,0]\n",
    "        ll = np.expand_dims(ll,axis=1)\n",
    "        return ll\n",
    "\n",
    "    \n",
    "    @tf.function\n",
    "    def give_td_error_Kennedy_guess_tf(self,batched_input,batched_zeroed_reward):\n",
    "        preds1 = self(batched_input)\n",
    "\n",
    "        Level1 = tf.unstack(b, axis=1)\n",
    "        pad, guess = tf.unstack(Level1[1], axis=1)\n",
    "        new_guess = tf.multiply(guess,-1)\n",
    "        flipped_guess = tf.stack([Level1[0],tf.stack([pad, new_guess], axis=1)], axis=2)\n",
    "\n",
    "        preds2 = self(flipped_guess)\n",
    "        both = tf.concat([preds1,preds2],1)\n",
    "        maxs = tf.math.reduce_max(both,axis=1)\n",
    "        batched_zeroed_reward = tf.stack([maxs, batched_zeroed_reward[:,1] ], axis=1)\n",
    "        return batched_zeroed_reward\n",
    "\n",
    "\n",
    "    def give_favourite_guess(self,sequence):\n",
    "        \"\"\"\"sequence should be [[beta, pad], [outcome, guess]] \"\"\"\n",
    "        pred_1 = self(sequence)\n",
    "        sequence[:,1][:,1] = -sequence[:,1][:,1]\n",
    "        pred_2 = self(sequence)\n",
    "        both = tf.concat([pred_1,pred_2],1)\n",
    "        maxs = tf.argmax(both,axis=1)\n",
    "        guess = (-1)**maxs.numpy()[0][0]\n",
    "        return guess\n",
    "\n",
    "experiences= np.load(\"experiences.npy\")\n",
    "critic = Critic()\n",
    "b, rews = critic.process_sequence_tf(experiences)\n",
    "critic.give_td_error_Kennedy_guess_tf(b, rews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -t td_errs.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guesses = np.array(tf.unstack(bb,axis=0))[:,1][:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.losses.MSE(ll,preds1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def optimization_step(experiences,critic, critic_target, actor, optimizer_critic, optimizer_actor, train_loss):\n",
    "    sequences, zeroed_rews = critic.process_sequence(experiences)\n",
    "    labels_critic = critic_target.give_td_error_Kennedy_guess( sequences, zeroed_rews)\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(critic.trainable_variables)\n",
    "        preds_critic = critic(sequences)\n",
    "        loss_critic = tf.keras.losses.MSE(labels_critic, preds_critic)\n",
    "        loss_critic = tf.reduce_mean(loss_critic)\n",
    "        grads = tape.gradient(loss_critic, critic.trainable_variables)\n",
    "        optimizer_critic.apply_gradients(zip(grads, critic.trainable_variables))\n",
    "        train_loss(loss_critic)\n",
    "\n",
    "    critic_target.update_target_parameters(critic, tau=0.05)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        ones = tf.ones(shape=(experiences.shape[0],1))\n",
    "        actions = tf.cast(actor(np.expand_dims(np.zeros(len(experiences)),axis=1)), tf.float32)   #This can be improved i think!! (the conversion... )\n",
    "\n",
    "        tape.watch(actions)\n",
    "        qvals = critic(tf.expand_dims(tf.concat([actions, ones], axis=1),axis=1))\n",
    "        dq_da = tape.gradient(qvals, actions)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        actionss = tf.cast(actor(np.expand_dims(np.zeros(len(experiences)),axis=1)), tf.float32)\n",
    "        da_dtheta = tape.gradient(actionss, actor.trainable_variables, output_gradients=-dq_da)\n",
    "\n",
    "    optimizer_actor.apply_gradients(zip(da_dtheta, actor.trainable_variables))\n",
    "    return\n",
    "        ###### END OF OPTIMIZATION STEP ######\n",
    "    ###### END OF OPTIMIZATION STEP ######\n",
    "\n",
    "\n",
    "def ddpgKennedy(special_name=\"\",total_episodes = 10**3,buffer_size=500, batch_size=64, ep_guess=0.01,\n",
    " noise_displacement=0.5,lr_actor=0.01, lr_critic=0.001, tau=0.005, repetitions=1, plots=True):\n",
    "\n",
    "    if not os.path.exists(\"results\"):\n",
    "        os.makedirs(\"results\")\n",
    "\n",
    "    amplitude = 0.4\n",
    "    buffer = ReplayBuffer(buffer_size=buffer_size)\n",
    "\n",
    "    critic = Critic()\n",
    "    critic_target = Critic()\n",
    "    actor = Actor(input_dim=1)\n",
    "    # actor_target = Actor(input_dim=1) THIS IS NOT REQUIRED FOR THE FIRST LAYER ONLY\n",
    "\n",
    "    actor(np.array([[0.]]).astype(np.float32)) #initialize the network 0, arbitrary inputs.\n",
    "    #\n",
    "    optimizer_critic = tf.keras.optimizers.Adam(lr=lr_critic)\n",
    "    optimizer_actor = tf.keras.optimizers.Adam(lr=lr_actor)\n",
    "\n",
    "\n",
    "    rt = []\n",
    "    pt = []\n",
    "\n",
    "    #define this global so i use them in a function defined above... optimizatin step and testing()\n",
    "    train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\n",
    "    test_loss = tf.keras.metrics.Mean('test_loss', dtype=tf.float32)\n",
    "\n",
    "\n",
    "    if special_name == \"\":\n",
    "        # current_run_and_time = \"results/{}\".format(datetime.now().strftime(\"%Y%m%d-%H%M\"))\n",
    "        numb = record()\n",
    "        current_run_and_time =\"results/run_\" + str(numb)\n",
    "    else:\n",
    "        current_run_and_time = \"results/\"+special_name\n",
    "\n",
    "    directory = current_run_and_time\n",
    "    train_log =  current_run_and_time + '/train_l0'\n",
    "    test_log =   current_run_and_time + '/test_l0'\n",
    "\n",
    "    train_summary_writer = tf.summary.create_file_writer(train_log)\n",
    "    test_summary_writer_0 = tf.summary.create_file_writer(test_log)\n",
    "\n",
    "    info_optimizers = \"optimizer_critic_guess: {} \\nOptimizer_actor_l0: {}\\n\".format(optimizer_critic.get_config(), optimizer_actor.get_config())\n",
    "    infor_buffer = \"Buffer_size: {}\\n Batch_size for sampling: {}\\n\".format(buffer.buffer_size, batch_size)\n",
    "    info_epsilons= \"epsilon-guess: {}\\nepsilon_displacement_noise: {}\".format(ep_guess,noise_displacement)\n",
    "\n",
    "    data = \"tau: {}, repetitions per optimization step (would be like epochs): {}\".format(tau,repetitions) + \"\\n \\n**** optimizers ***\\n\"+info_optimizers+\"\\n\\n\\n*** BUFFER ***\\n\"+infor_buffer+\"\\n\\n\\n *** NOISE PARAMETERS *** \\n\"+info_epsilons\n",
    "    with open(directory+\"/info.txt\", 'w') as f:\n",
    "        f.write(data)\n",
    "        f.close()\n",
    "\n",
    "    print(\"Beggining to train! \\n \\n\")\n",
    "    print(data)\n",
    "    print(\"starting time: {}\".format(datetime.now().strftime(\"%Y%m%d-%H%M%S\")))\n",
    "    print(\"saving results in \" + str(directory))\n",
    "    avg_train = []\n",
    "    avg_test = []\n",
    "\n",
    "    history_betas = [] #to put in histogram\n",
    "    history_betas_would_have_done=[] #to put in histogram\n",
    "    histo_preds = {\"layer0\":{}, \"layer1\":{}} #here i save the predictions to plot in a \"straightforward way\"\n",
    "\n",
    "    #######\n",
    "    for episode in tqdm(range(total_episodes)):\n",
    "\n",
    "        alice_phase = np.random.choice([-1.,1.],1)[0]\n",
    "        beta_would_do = actor(np.array([[0.]])).numpy()[0][0]\n",
    "        beta =  beta_would_do + np.random.uniform(-noise_displacement, noise_displacement)\n",
    "        proboutcome = Prob(alice_phase*amplitude,beta,0)\n",
    "        outcome = np.random.choice([0.,1.],1,p=[proboutcome, 1-proboutcome])[0]\n",
    "\n",
    "        history_betas.append(beta)\n",
    "        history_betas_would_have_done.append(beta_would_do)\n",
    "\n",
    "    #\n",
    "        if np.random.random()< ep_guess:\n",
    "            guess = np.random.choice([-1.,1.],1)[0]\n",
    "        else:\n",
    "            sequence = np.array([[ [beta, critic.pad_value], [outcome, -1.]]  ]).astype(np.float32)\n",
    "            guess = critic.give_favourite_guess(sequence)\n",
    "        if guess == alice_phase:\n",
    "            reward = 1.\n",
    "        else:\n",
    "            reward = 0.\n",
    "        buffer.add(beta, outcome, guess, reward)\n",
    "\n",
    "\n",
    "        ###### OPTIMIZATION STEP ######\n",
    "        ###### OPTIMIZATION STEP ######\n",
    "\n",
    "        experiences = buffer.sample(batch_size)\n",
    "        optimization_step(experiences,critic, critic_target, actor, optimizer_critic, optimizer_actor, train_loss)\n",
    "\n",
    "\n",
    "#####\n",
    "        avg_train.append(train_loss.result().numpy())\n",
    "        avg_test.append(test_loss.result().numpy())\n",
    "    #\n",
    "        rt.append(reward)\n",
    "    #\n",
    "\n",
    "        ########################################################################\n",
    "        ### calculate success probability if the agent went greedy ###########\n",
    "        p=0\n",
    "        for outcome in [0.,1.]:\n",
    "            p+=Prob(critic.give_favourite_guess(critic.pad_single_sequence([beta_would_do, outcome, -1.]))*amplitude, beta_would_do,outcome)\n",
    "        p/=2\n",
    "        pt.append(p)\n",
    "        ################\n",
    "\n",
    "        if episode%(total_episodes/10) == 0: #this is for showing 10 results in total.\n",
    "\n",
    "            template = 'Episode {}, \\Rt: {}, \\Pt: {}, Train loss: {}, Test loss: {}\\n\\n'\n",
    "            print(template.format(episode+1,\n",
    "                                np.sum(rt)/(episode+1),\n",
    "                                  pt[-1],\n",
    "                                 np.round(train_loss.result().numpy(),5),\n",
    "                                 np.round(test_loss.result().numpy(),5))\n",
    "                  )\n",
    "\n",
    "\n",
    "            for layer in [\"layer0\",\"layer1\"]: #net_0 will be critic_q0, net_1 will be critic_qguess\n",
    "\n",
    "                histo_preds[layer][str(episode)] ={}\n",
    "                histo_preds[layer][str(episode)][\"episode\"] = episode\n",
    "                histo_preds[layer][str(episode)][\"values\"] = {}\n",
    "\n",
    "            simp = np.random.randn(len(buffer.betas),4)\n",
    "            simp[:,0] =buffer.betas\n",
    "            qvals0 = np.squeeze(critic(critic.process_sequence(simp)[0]).numpy()[:,0])\n",
    "            histo_preds[\"layer0\"][str(episode)][\"values\"] = qvals0\n",
    "\n",
    "            index=0\n",
    "            for n1 in [0.,1.]:\n",
    "                for guess in [-1.,1.]:\n",
    "                    simp[:,1] = n1\n",
    "                    simp[:,2] = guess\n",
    "                    qvals1 = np.squeeze(critic(critic.process_sequence(simp)[0]).numpy()[:,1])\n",
    "                    histo_preds[\"layer1\"][str(episode)][\"values\"][str(index)] = qvals1\n",
    "                    index+=1\n",
    "\n",
    "\n",
    "\n",
    "    rt = [np.sum(rt[:k]) for k in range(len(rt))]\n",
    "    rt = rt/np.arange(1,len(rt)+1)\n",
    "\n",
    "    losses = [avg_train, avg_test]\n",
    "\n",
    "    BigPlot(buffer,rt, pt, history_betas, history_betas_would_have_done, histo_preds, losses, directory)\n",
    "    return\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    info_run = \"\"\n",
    "    to_csv=[]\n",
    "    for tau in [0.001]:\n",
    "        for lr_critic in [.001]:\n",
    "            for noise_displacement in [.2]:\n",
    "                for batch_size in [8., 16., 32. ,64.]:\n",
    "\n",
    "                    # name_run = datetime.now().strftime(\"%m-%d-%H-%-M%-S\")\n",
    "\n",
    "                    name_run = ddpgKennedy(total_episodes=500, noise_displacement=noise_displacement, tau=tau,\n",
    "                    buffer_size=10**3, batch_size=batch_size, lr_critic=lr_critic, lr_actor=0.001, plots=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
