{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm as tqdm\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "import random\n",
    "import matplotlib\n",
    "from environment import Environment\n",
    "from plots import just_plot\n",
    "from misc import *\n",
    "from nets import *\n",
    "from buffer import ReplayBuffer\n",
    "\n",
    "amplitude=0.4\n",
    "tau = .01\n",
    "lr_critic = 0.0001\n",
    "lr_actor=0.001\n",
    "noise_displacement = .1\n",
    "ep_guess=0.01\n",
    "dolinar_layers=2\n",
    "number_phases=2\n",
    "buffer_size = 5000\n",
    "batch_size = 8.\n",
    "\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def step_critic_tf(batched_input,labels_critic, critic, optimizer_critic):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(critic.trainable_variables)\n",
    "        preds_critic = critic(batched_input)\n",
    "        loss_critic = tf.keras.losses.MSE(tf.expand_dims(labels_critic, axis=2), preds_critic)\n",
    "        loss_critic = tf.reduce_mean(loss_critic)\n",
    "        grads = tape.gradient(loss_critic, critic.trainable_variables)\n",
    "        optimizer_critic.apply_gradients(zip(grads, critic.trainable_variables))\n",
    "        return tf.squeeze(loss_critic)\n",
    "\n",
    "@tf.function\n",
    "def critic_grad_tf(critic, experiences):\n",
    "    with tf.GradientTape() as tape:\n",
    "        unstacked_exp = tf.unstack(tf.convert_to_tensor(experiences), axis=1)\n",
    "        to_stack = []\n",
    "        actions_wathed_index = []\n",
    "        for index in range(0,experiences.shape[-1]-3,2): # I consider from first outcome to last one (but guess)\n",
    "            actions_wathed_index.append(index)\n",
    "            to_stack.append(tf.reshape(unstacked_exp[index],(experiences.shape[0],1,1)))\n",
    "\n",
    "        actions_indexed = tf.concat(to_stack,axis=1)\n",
    "        tape.watch(actions_indexed)\n",
    "\n",
    "        index_actions=0\n",
    "        watched_exps=[tf.ones((experiences.shape[0],1,1))*critic.pad_value]\n",
    "        watched_actions_unstacked = tf.unstack(actions_indexed, axis=1)\n",
    "        for index in range(0,experiences.shape[-1]-1):\n",
    "            if index in actions_wathed_index:\n",
    "                watched_exps.append(tf.expand_dims(watched_actions_unstacked[index_actions], axis=2))\n",
    "                index_actions+=1\n",
    "            else:\n",
    "                watched_exps.append(tf.reshape(unstacked_exp[index],(experiences.shape[0],1,1)))\n",
    "\n",
    "        qvals = critic(tf.reshape(tf.concat(watched_exps, axis=2), (experiences.shape[0],critic.dolinar_layers+1,2)))\n",
    "\n",
    "        dq_da = tape.gradient(qvals, actions_indexed)\n",
    "        return dq_da\n",
    "\n",
    "@tf.function\n",
    "def actor_grad_tf(actor, dq_da, experiences, optimizer_actor):\n",
    "    unstacked_exp = tf.unstack(experiences, axis=1)\n",
    "    actions_per_episode={}\n",
    "    context_outcome_actor = np.reshape(np.array([actor.pad_value]),(1,1,1)).astype(np.float32)\n",
    "    finns = [tf.multiply(actor(context_outcome_actor), tf.ones((experiences.shape[0],1,1)))]\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(actor.trainable_variables)\n",
    "        for index in range(1,2*actor.dolinar_layers-2,2):\n",
    "            actions_per_episode[str(index)] = []\n",
    "            for k in tf.unstack(unstacked_exp[index]):\n",
    "                actions_per_episode[str(index)].append(actor(tf.reshape(k, (1,1,1))))\n",
    "            finns.append(tf.concat(actions_per_episode[str(index)], axis=0))\n",
    "        final_preds = tf.concat(finns, axis=1)\n",
    "        da_dtheta=tape.gradient(final_preds, actor.trainable_variables, output_gradients=-dq_da)\n",
    "        optimizer_actor.apply_gradients(zip(da_dtheta, actor.trainable_variables))\n",
    "    return\n",
    "\n",
    "\n",
    "    # states_to_act=[tf.ones((experiences.shape[0],1,1))*actor.pad_value]\n",
    "    #\n",
    "    # to_stack = []\n",
    "    # actions_wathed_index = []\n",
    "    # for index in range(1,2*actor.dolinar_layers-2,2):\n",
    "    #     states_to_act.append(tf.reshape(unstacked_exp[index],(experiences.shape[0],1,1)))\n",
    "    # inps_actor = tf.concat(states_to_act, axis=1)\n",
    "    # actor.lstm.stateful=False\n",
    "    # actor_thinks = actor(inps_actor)\n",
    "    # actor.lstm.stateful=True\n",
    "    # da_dtheta = tape.gradient(actor_thinks, actor.trainable_variables, output_gradients=-dq_da)\n",
    "    # optimizer_actor.apply_gradients(zip(da_dtheta, actor.trainable_variables))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def optimization_step(experiences, critic, critic_target, actor, actor_target, optimizer_critic, optimizer_actor):\n",
    "    # actor.lstm.reset_states()\n",
    "    actor.lstm.stateful=False\n",
    "    # experiences = experiences.astype(np.float32)\n",
    "    targeted_experience = actor_target.process_sequence_of_experiences_tf(experiences)\n",
    "    sequences, zeroed_rews = critic_target.process_sequence_tf(targeted_experience)\n",
    "    labels_critic = critic_target.give_td_errors_tf( sequences, zeroed_rews)\n",
    "\n",
    "    loss_critic = step_critic_tf(sequences ,labels_critic, critic, optimizer_critic)\n",
    "\n",
    "    dq_da = critic_grad_tf(critic, experiences)\n",
    "\n",
    "    actor_grad_tf(actor, dq_da, experiences, optimizer_actor)\n",
    "\n",
    "    actor.lstm.stateful=True\n",
    "    return loss_critic\n",
    "\n",
    "\n",
    "env = Environment(amplitude=amplitude, dolinar_layers = dolinar_layers, number_phases=number_phases)\n",
    "buffer = ReplayBuffer(buffer_size=buffer_size)\n",
    "\n",
    "critic = Critic(nature=\"primary\",valreg=0.01, dolinar_layers = dolinar_layers, number_phases=number_phases)\n",
    "critic_target = Critic(nature=\"target\", dolinar_layers = dolinar_layers, number_phases=number_phases)\n",
    "actor = Actor(nature=\"primary\", dolinar_layers = dolinar_layers)\n",
    "actor_target = Actor(nature=\"target\", dolinar_layers = dolinar_layers)\n",
    "\n",
    "optimizer_critic = tf.keras.optimizers.Adam(lr=lr_critic)\n",
    "optimizer_actor = tf.keras.optimizers.Adam(lr=lr_actor)\n",
    "\n",
    "policy_evaluator = PolicyEvaluator(amplitude = amplitude, dolinar_layers=dolinar_layers, number_phases = number_phases)\n",
    "\n",
    "experiences = np.load(\"tutorials_functions/expe_2L.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor.lstm.stateful=False\n",
    "experiences = experiences.astype(np.float32)\n",
    "targeted_experience = actor_target.process_sequence_of_experiences_tf(experiences)\n",
    "sequences, zeroed_rews = critic_target.process_sequence_tf(targeted_experience)\n",
    "labels_critic = critic_target.give_td_errors_tf( sequences, zeroed_rews)\n",
    "dq_da = critic_grad_tf(critic, experiences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tf.function\n",
    "def actor_grad_tf_old(actor, dq_da, experiences, optimizer_actor):\n",
    "    unstacked_exp = tf.unstack(experiences, axis=1)\n",
    "    actions_per_episode={}\n",
    "    context_outcome_actor = np.reshape(np.array([actor.pad_value]),(1,1,1)).astype(np.float32)\n",
    "    finns = [tf.multiply(actor(context_outcome_actor), tf.ones((experiences.shape[0],1,1)))]\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(actor.trainable_variables)\n",
    "        for index in range(1,2*actor.dolinar_layers-2,2):\n",
    "            actions_per_episode[str(index)] = []\n",
    "            for k in tf.unstack(unstacked_exp[index]):\n",
    "                actions_per_episode[str(index)].append(actor(tf.reshape(k, (1,1,1))))\n",
    "            finns.append(tf.concat(actions_per_episode[str(index)], axis=0))\n",
    "        final_preds = tf.concat(finns, axis=1)\n",
    "        da_dtheta=tape.gradient(final_preds, actor.trainable_variables, output_gradients=-dq_da)\n",
    "        optimizer_actor.apply_gradients(zip(da_dtheta, actor.trainable_variables))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def act_v2(actor, dq_da, experiences, optimizer_actor):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(actor.trainable_variables)\n",
    "        finns = [actor(tf.ones((experiences.shape[0], 1,1))*actor.pad_value)]\n",
    "        unstacked_exp = tf.unstack(experiences, axis=1)\n",
    "        for index in range(1,2*actor.dolinar_layers-2,2):\n",
    "            finns.append(actor(tf.reshape(unstacked_exp[index], (experiences.shape[0], 1,1))))    \n",
    "        final_preds = tf.concat(finns, axis=1)\n",
    "        da_dtheta=tape.gradient(final_preds, actor.trainable_variables, output_gradients=-dq_da)\n",
    "        optimizer_actor.apply_gradients(zip(da_dtheta, actor.trainable_variables))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.8 ms ± 2.59 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit act_v2(actor, dq_da, experiences, optimizer_actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.5 ms ± 1.52 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit act_v2(actor, dq_da, tf.convert_to_tensor(experiences), optimizer_actor) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def act_v3(actor, dq_da, exper, optimizer_actor):\n",
    "    experiences = tf.convert_to_tensor(exper)\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(actor.trainable_variables)\n",
    "        finns = [actor(tf.ones((experiences.shape[0], 1,1))*actor.pad_value)]\n",
    "        unstacked_exp = tf.unstack(experiences, axis=1)\n",
    "        for index in tf.range(1,2*actor.dolinar_layers-2,2):\n",
    "            print(tf.gather(unstacked_exp, index))\n",
    "            finns.append(actor(tf.reshape(tf.gather(unstacked_exp, index), (experiences.shape[0], 1,1))))    \n",
    "        final_preds = tf.concat(finns, axis=1)\n",
    "        da_dtheta=tape.gradient(final_preds, actor.trainable_variables, output_gradients=-dq_da)\n",
    "        optimizer_actor.apply_gradients(zip(da_dtheta, actor.trainable_variables))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "in user code:\n\n    <ipython-input-40-de4204a849c6>:9 act_v3  *\n        finns.append(actor(tf.reshape(tf.gather(unstacked_exp, index), (experiences.shape[0], 1,1))))\n    /home/cooper-cooper/Desktop/deeper/nets.py:233 call  *\n        feat = tf.nn.relu(self.l1(feat))\n    /home/cooper-cooper/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py:955 __call__  **\n        self._handle_activity_regularization(inputs, outputs)\n    /home/cooper-cooper/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py:2215 _handle_activity_regularization\n        mean_activity_loss, method='activity_regularizer')\n    /home/cooper-cooper/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer_utils.py:557 check_graph_consistency\n        bad_example=bad_example, correct_example=correct_example))\n\n    RuntimeError: You are using a layer with `activity_regularizer` in a control flow branch, e.g.:\n    \n          class TestModel(tf.keras.Model):\n    \n            def __init__(self):\n              super(TestModel, self).__init__(name='test_model')\n              self.dense = tf.keras.layers.Dense(2, activity_regularizer='l2')\n    \n            def call(self, x, training=None):\n              if training:\n                return self.dense(x)\n              else:\n                return self.dense(x)\n          \n    This is currently not supported. Please move your call to the layer with `activity_regularizer` out of the control flow branch, e.g.:\n    \n          class TestModel(tf.keras.Model):\n    \n            def __init__(self):\n              super(TestModel, self).__init__(name='test_model')\n              self.dense = tf.keras.layers.Dense(2, activity_regularizer='l2')\n    \n            def call(self, x, training=None):\n              return self.dense(x)\n          \n    You can also resolve this by marking your outer model/layer dynamic (eager-only) by passing `dynamic=True` to the layer constructor. Any kind of control flow is supported with dynamic layers. Note that using `dynamic=True` requires you to implement static shape inference in the `compute_output_shape(input_shape)` method.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-26ed512f5598>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timeit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'act_v3(actor, dq_da, experiences, optimizer_actor)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2315\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2316\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2317\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2318\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1158\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m                 \u001b[0mnumber\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1160\u001b[0;31m                 \u001b[0mtime_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1161\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtime_number\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mtiming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<magic-timeit>\u001b[0m in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    625\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    504\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    505\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 506\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2444\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2445\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2446\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2447\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2777\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2778\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2665\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2666\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2667\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2668\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2669\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: in user code:\n\n    <ipython-input-40-de4204a849c6>:9 act_v3  *\n        finns.append(actor(tf.reshape(tf.gather(unstacked_exp, index), (experiences.shape[0], 1,1))))\n    /home/cooper-cooper/Desktop/deeper/nets.py:233 call  *\n        feat = tf.nn.relu(self.l1(feat))\n    /home/cooper-cooper/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py:955 __call__  **\n        self._handle_activity_regularization(inputs, outputs)\n    /home/cooper-cooper/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py:2215 _handle_activity_regularization\n        mean_activity_loss, method='activity_regularizer')\n    /home/cooper-cooper/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer_utils.py:557 check_graph_consistency\n        bad_example=bad_example, correct_example=correct_example))\n\n    RuntimeError: You are using a layer with `activity_regularizer` in a control flow branch, e.g.:\n    \n          class TestModel(tf.keras.Model):\n    \n            def __init__(self):\n              super(TestModel, self).__init__(name='test_model')\n              self.dense = tf.keras.layers.Dense(2, activity_regularizer='l2')\n    \n            def call(self, x, training=None):\n              if training:\n                return self.dense(x)\n              else:\n                return self.dense(x)\n          \n    This is currently not supported. Please move your call to the layer with `activity_regularizer` out of the control flow branch, e.g.:\n    \n          class TestModel(tf.keras.Model):\n    \n            def __init__(self):\n              super(TestModel, self).__init__(name='test_model')\n              self.dense = tf.keras.layers.Dense(2, activity_regularizer='l2')\n    \n            def call(self, x, training=None):\n              return self.dense(x)\n          \n    You can also resolve this by marking your outer model/layer dynamic (eager-only) by passing `dynamic=True` to the layer constructor. Any kind of control flow is supported with dynamic layers. Note that using `dynamic=True` requires you to implement static shape inference in the `compute_output_shape(input_shape)` method.\n"
     ]
    }
   ],
   "source": [
    "%timeit act_v3(actor, dq_da, experiences, optimizer_actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = [actor(tf.ones((experiences.shape[0], 1,1))*actor.pad_value)]\n",
    "tfexperiences = tf.convert_to_tensor(experiences)\n",
    "unsta = tf.unstack(tfexperiences, axis=1)\n",
    "for index in tf.range(1,2*actor.dolinar_layers-2,2):\n",
    "    ff.append(actor(tf.reshape(unstacked_exp[index], (experiences.shape[0], 1,1))))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(1,2*actor.dolinar_layers-2,2):\n",
    "        for k in tf.unstack(unstacked_exp[index]):\n",
    "            actions_per_episode[str(index)].append(actor(tf.reshape(k, (1,1,1))))\n",
    "        finns.append(tf.concat(actions_per_episode[str(index)], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(100, 2, 1), dtype=float32, numpy=\n",
       "array([[[0.00718073],\n",
       "        [0.00718174]],\n",
       "\n",
       "       [[0.00717851],\n",
       "        [0.00718064]],\n",
       "\n",
       "       [[0.00717866],\n",
       "        [0.00718212]],\n",
       "\n",
       "       [[0.00718067],\n",
       "        [0.00718326]],\n",
       "\n",
       "       [[0.00718077],\n",
       "        [0.00718088]],\n",
       "\n",
       "       [[0.00718001],\n",
       "        [0.00717943]],\n",
       "\n",
       "       [[0.00717958],\n",
       "        [0.00718186]],\n",
       "\n",
       "       [[0.00718099],\n",
       "        [0.00717923]],\n",
       "\n",
       "       [[0.00717997],\n",
       "        [0.00718219]],\n",
       "\n",
       "       [[0.00718046],\n",
       "        [0.00718246]],\n",
       "\n",
       "       [[0.00718216],\n",
       "        [0.00718108]],\n",
       "\n",
       "       [[0.00718047],\n",
       "        [0.00718226]],\n",
       "\n",
       "       [[0.00718331],\n",
       "        [0.00718   ]],\n",
       "\n",
       "       [[0.00718107],\n",
       "        [0.00718116]],\n",
       "\n",
       "       [[0.00718115],\n",
       "        [0.00718119]],\n",
       "\n",
       "       [[0.0071806 ],\n",
       "        [0.00718097]],\n",
       "\n",
       "       [[0.00718187],\n",
       "        [0.00718048]],\n",
       "\n",
       "       [[0.00718053],\n",
       "        [0.00718116]],\n",
       "\n",
       "       [[0.00718104],\n",
       "        [0.00718085]],\n",
       "\n",
       "       [[0.00718172],\n",
       "        [0.00718052]],\n",
       "\n",
       "       [[0.0071807 ],\n",
       "        [0.00718384]],\n",
       "\n",
       "       [[0.00718058],\n",
       "        [0.00718072]],\n",
       "\n",
       "       [[0.00717999],\n",
       "        [0.00718055]],\n",
       "\n",
       "       [[0.00718073],\n",
       "        [0.00717907]],\n",
       "\n",
       "       [[0.00717996],\n",
       "        [0.00717944]],\n",
       "\n",
       "       [[0.00718204],\n",
       "        [0.00718088]],\n",
       "\n",
       "       [[0.00717962],\n",
       "        [0.00717958]],\n",
       "\n",
       "       [[0.00718247],\n",
       "        [0.00718252]],\n",
       "\n",
       "       [[0.00718045],\n",
       "        [0.00717958]],\n",
       "\n",
       "       [[0.00718103],\n",
       "        [0.00718002]],\n",
       "\n",
       "       [[0.00718177],\n",
       "        [0.00718161]],\n",
       "\n",
       "       [[0.00718239],\n",
       "        [0.00718138]],\n",
       "\n",
       "       [[0.00718178],\n",
       "        [0.00718079]],\n",
       "\n",
       "       [[0.00718235],\n",
       "        [0.00718098]],\n",
       "\n",
       "       [[0.00718113],\n",
       "        [0.00718104]],\n",
       "\n",
       "       [[0.00718112],\n",
       "        [0.00718125]],\n",
       "\n",
       "       [[0.00718055],\n",
       "        [0.00718104]],\n",
       "\n",
       "       [[0.00718004],\n",
       "        [0.00718039]],\n",
       "\n",
       "       [[0.00718086],\n",
       "        [0.0071828 ]],\n",
       "\n",
       "       [[0.00718151],\n",
       "        [0.00718197]],\n",
       "\n",
       "       [[0.00718077],\n",
       "        [0.00718013]],\n",
       "\n",
       "       [[0.00718062],\n",
       "        [0.00717966]],\n",
       "\n",
       "       [[0.00718094],\n",
       "        [0.00718222]],\n",
       "\n",
       "       [[0.00718012],\n",
       "        [0.00718086]],\n",
       "\n",
       "       [[0.00718141],\n",
       "        [0.00718103]],\n",
       "\n",
       "       [[0.0071812 ],\n",
       "        [0.00717959]],\n",
       "\n",
       "       [[0.00718152],\n",
       "        [0.00718029]],\n",
       "\n",
       "       [[0.00718051],\n",
       "        [0.00718182]],\n",
       "\n",
       "       [[0.00718081],\n",
       "        [0.00717973]],\n",
       "\n",
       "       [[0.00718193],\n",
       "        [0.00718049]],\n",
       "\n",
       "       [[0.00718245],\n",
       "        [0.00718072]],\n",
       "\n",
       "       [[0.00718096],\n",
       "        [0.00717957]],\n",
       "\n",
       "       [[0.00718092],\n",
       "        [0.00718299]],\n",
       "\n",
       "       [[0.00718071],\n",
       "        [0.00718177]],\n",
       "\n",
       "       [[0.00718032],\n",
       "        [0.00718083]],\n",
       "\n",
       "       [[0.00718183],\n",
       "        [0.0071808 ]],\n",
       "\n",
       "       [[0.00717827],\n",
       "        [0.00718176]],\n",
       "\n",
       "       [[0.00718234],\n",
       "        [0.00717963]],\n",
       "\n",
       "       [[0.00718121],\n",
       "        [0.00718091]],\n",
       "\n",
       "       [[0.00718168],\n",
       "        [0.00718038]],\n",
       "\n",
       "       [[0.00718156],\n",
       "        [0.00718067]],\n",
       "\n",
       "       [[0.00718151],\n",
       "        [0.00718231]],\n",
       "\n",
       "       [[0.00718102],\n",
       "        [0.00718031]],\n",
       "\n",
       "       [[0.0071799 ],\n",
       "        [0.00718164]],\n",
       "\n",
       "       [[0.00718247],\n",
       "        [0.00717871]],\n",
       "\n",
       "       [[0.00718082],\n",
       "        [0.00717853]],\n",
       "\n",
       "       [[0.00718194],\n",
       "        [0.00718082]],\n",
       "\n",
       "       [[0.00717939],\n",
       "        [0.00718114]],\n",
       "\n",
       "       [[0.00718076],\n",
       "        [0.00718093]],\n",
       "\n",
       "       [[0.00718   ],\n",
       "        [0.0071813 ]],\n",
       "\n",
       "       [[0.00718105],\n",
       "        [0.00717982]],\n",
       "\n",
       "       [[0.00718204],\n",
       "        [0.00717889]],\n",
       "\n",
       "       [[0.00718114],\n",
       "        [0.00718086]],\n",
       "\n",
       "       [[0.00718176],\n",
       "        [0.00718191]],\n",
       "\n",
       "       [[0.00718066],\n",
       "        [0.00718236]],\n",
       "\n",
       "       [[0.00717884],\n",
       "        [0.00718225]],\n",
       "\n",
       "       [[0.00718093],\n",
       "        [0.00718155]],\n",
       "\n",
       "       [[0.00718188],\n",
       "        [0.0071808 ]],\n",
       "\n",
       "       [[0.00717998],\n",
       "        [0.00718087]],\n",
       "\n",
       "       [[0.00718153],\n",
       "        [0.00717982]],\n",
       "\n",
       "       [[0.00718253],\n",
       "        [0.00718084]],\n",
       "\n",
       "       [[0.00718344],\n",
       "        [0.00718081]],\n",
       "\n",
       "       [[0.00717939],\n",
       "        [0.00718134]],\n",
       "\n",
       "       [[0.00717978],\n",
       "        [0.00718087]],\n",
       "\n",
       "       [[0.00717927],\n",
       "        [0.00717998]],\n",
       "\n",
       "       [[0.00718086],\n",
       "        [0.00718179]],\n",
       "\n",
       "       [[0.0071813 ],\n",
       "        [0.00718025]],\n",
       "\n",
       "       [[0.00718   ],\n",
       "        [0.0071814 ]],\n",
       "\n",
       "       [[0.00717999],\n",
       "        [0.00718294]],\n",
       "\n",
       "       [[0.00718165],\n",
       "        [0.00718239]],\n",
       "\n",
       "       [[0.00718084],\n",
       "        [0.00718005]],\n",
       "\n",
       "       [[0.00718183],\n",
       "        [0.00718031]],\n",
       "\n",
       "       [[0.00718076],\n",
       "        [0.00718099]],\n",
       "\n",
       "       [[0.00718242],\n",
       "        [0.00718172]],\n",
       "\n",
       "       [[0.00718043],\n",
       "        [0.00718209]],\n",
       "\n",
       "       [[0.00718003],\n",
       "        [0.00717986]],\n",
       "\n",
       "       [[0.00718114],\n",
       "        [0.00717993]],\n",
       "\n",
       "       [[0.00717997],\n",
       "        [0.00718168]],\n",
       "\n",
       "       [[0.00718106],\n",
       "        [0.00718215]],\n",
       "\n",
       "       [[0.00718158],\n",
       "        [0.00718064]]], dtype=float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_per_episode={}\n",
    "context_outcome_actor = np.reshape(np.array([actor.pad_value]),(1,1,1)).astype(np.float32)\n",
    "finns = [tf.multiply(actor(context_outcome_actor), tf.ones((experiences.shape[0],1,1)))]\n",
    "\n",
    "\n",
    "    for index in range(1,2*actor.dolinar_layers-2,2):\n",
    "        actions_per_episode[str(index)] = []\n",
    "        for k in tf.unstack(unstacked_exp[index]):\n",
    "            actions_per_episode[str(index)].append(actor(tf.reshape(k, (1,1,1))))\n",
    "        finns.append(tf.concat(actions_per_episode[str(index)], axis=0))\n",
    "    final_preds = tf.concat(finns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        env.pick_phase()\n",
    "        experiences=[] #where the current history of the current episode is stored\n",
    "        context_outcome_actor = np.reshape(np.array([actor.pad_value]),(1,1,1)).astype(np.float32)\n",
    "        outcomes_so_far = []\n",
    "        for layer in range(actor.dolinar_layers):\n",
    "            beta_would_do = np.squeeze(actor(context_outcome_actor))\n",
    "            beta =  beta_would_do + np.random.uniform(-noise_displacement, noise_displacement)#np.clip(,-2*amplitude,2*amplitude)\n",
    "            policy_evaluator.recorded_trajectory_tree[str(layer)][str(np.array(outcomes_so_far))].append(beta)\n",
    "            policy_evaluator.recorded_trajectory_tree_would_do[str(layer)][str(np.array(outcomes_so_far))].append(beta_would_do)\n",
    "\n",
    "            outcome = env.give_outcome(beta,layer)\n",
    "            outcomes_so_far.append(int(outcome))\n",
    "            experiences.append(beta)\n",
    "            experiences.append(outcome)\n",
    "            context_outcome_actor = np.reshape(np.array([outcome]),(1,1,1)).astype(np.float32)\n",
    "\n",
    "        ### ep-gredy guessing of the phase###\n",
    "        ### ep-gredy guessing of the phase###\n",
    "        if np.random.random()<ep_guess:\n",
    "            val = np.random.choice(range(number_phases),1)[0]\n",
    "            guess_index, guess_input_network = val, val/critic.number_phases\n",
    "            # print(guess_input_network)\n",
    "        else:\n",
    "            guess_index, guess_input_network = critic.give_favourite_guess(experiences) #experiences is the branch of the current tree of actions + outcomes.\n",
    "        experiences.append(guess_input_network)\n",
    "\n",
    "        reward = env.give_reward(guess_index)\n",
    "        experiences.append(reward)\n",
    "        buffer.add(tuple(experiences))\n",
    "\n",
    "\n",
    "        rt.append(reward)\n",
    "        pt.append(policy_evaluator.greedy_strategy(actor = actor, critic = critic))\n",
    "\n",
    "        ###### OPTIMIZATION STEP ######\n",
    "        ###### OPTIMIZATION STEP ######\n",
    "        ###### OPTIMIZATION STEP ######\n",
    "        ###### OPTIMIZATION STEP ######\n",
    "        # if (buffer.count>1):#(episode%100==1):\n",
    "            # sampled_experiences = buffer.sample(batch_size)\n",
    "            # np.save(str(dolinar_layers)+\"_sample\", sampled_experiences)\n",
    "        if (buffer.count>batch_size):#(episode%100==1):\n",
    "            sampled_experiences = tf.convert_to_tensor(buffer.sample(batch_size), dtype=np.float32)\n",
    "\n",
    "            new_loss = optimization_step(sampled_experiences, critic, critic_target, actor, actor_target, optimizer_critic, optimizer_actor)\n",
    "            new_loss = new_loss.numpy()\n",
    "            critic_target.update_target_parameters(critic)\n",
    "            actor_target.update_target_parameters(actor)\n",
    "            # noise_displacement = max(0.1,0.999*noise_displacement)\n",
    "        ###### OPTIMIZATION STEP ######\n",
    "        ###### OPTIMIZATION STEP ######\n",
    "        ###### OPTIMIZATION STEP ######\n",
    "        ###### OPTIMIZATION STEP ######\n",
    "        avg_train.append(new_loss)\n",
    "        actor.lstm.reset_states()\n",
    "\n",
    "         #set again the states to zero, because when actor.lstm.stateful = True, it does not reset state along differnt batches !\n",
    "\n",
    "        # if episode%(total_episodes/100) == 0: #this is for showing 10 results in total.\n",
    "        #\n",
    "        #     template = 'Episode {}, \\Rt: {}, \\Pt: {}, Train loss: {}\\n\\n'\n",
    "        #     print(template.format(episode+1,\n",
    "        #                         np.sum(rt)/(episode+1),\n",
    "        #                           pt[-1],\n",
    "        #                          np.round(np.array(avg_train).mean(),5),\n",
    "        #                         )\n",
    "        #           )\n",
    "\n",
    "    cumre=0\n",
    "    rrt = []\n",
    "    for k in rt:\n",
    "        cumre+=k\n",
    "        rrt.append(cumre)\n",
    "    rrt = rrt/np.arange(1,len(rt)+1)\n",
    "\n",
    "    np.save(directory+\"/learning_curves/\", rrt)\n",
    "    np.save(directory+\"/learning_curves/\", pt)\n",
    "    policy_evaluator.save_hisory_tree(directory+\"/action_trees\")\n",
    "\n",
    "    for model, net_folder in zip([actor, actor_target, critic, critic_target],[\"actor_primary\", \"actor_target\", \"critic_primary\", \"critic_target\"]):\n",
    "        model.save_weights(directory+\"/networks/\"+net_folder+\"/\")\n",
    "    just_plot(rrt, pt, avg_train, env.helstrom(), policy_evaluator, directory)\n",
    "    # BigPlot(buffer,rt, pt, history_betas, history_betas_would_have_done, histo_preds, losses, directory)\n",
    "    return\n",
    "\n",
    "\n",
    "info_run = \"\"\n",
    "to_csv=[]\n",
    "\n",
    "RDPG(amplitude=amplitude, total_episodes=10**2, dolinar_layers=dolinar_layers, noise_displacement=noise_displacement, tau=tau,\n",
    "        buffer_size=buffer_size, batch_size=batch_size, lr_critic=lr_critic, lr_actor=lr_actor, ep_guess=ep_guess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
