{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beggining to train! \n",
      " \n",
      "\n",
      "tau: 0.005, repetitions per optimization step (would be like epochs): 1\n",
      " \n",
      "**** optimizers ***\n",
      "optimizer_critic_guess: {'name': 'Adam', 'learning_rate': 0.001, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False} \n",
      "Optimizer_actor_l0: {'name': 'Adam', 'learning_rate': 0.01, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
      "\n",
      "\n",
      "\n",
      "*** BUFFER ***\n",
      "Buffer_size: 500\n",
      " Batch_size for sampling: 64\n",
      "\n",
      "\n",
      "\n",
      " *** NOISE PARAMETERS *** \n",
      "epsilon-guess: 0.01\n",
      "epsilon_displacement_noise: 0.5\n",
      "starting time: 20200510-192739\n",
      "saving results in results/run_13\n",
      "WARNING:tensorflow:Layer critic_8 is casting an input tensor from dtype float32 to the layer's dtype of float64, which is new behavior in TensorFlow 2.  The layer has dtype float64 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float64, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float32 by default, call `tf.keras.backend.set_floatx('float32')`. To change just this layer, pass dtype='float32' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [02:03<00:00,  8.12it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm as tqdm\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "import random\n",
    "import matplotlib\n",
    "\n",
    "from plots import *\n",
    "from misc import Prob, ps_maxlik, qval, record\n",
    "from nets import *\n",
    "from buffer import ReplayBuffer\n",
    "\n",
    "\n",
    "special_name=\"\"\n",
    "total_episodes = 10**3\n",
    "buffer_size=500\n",
    "batch_size=64\n",
    "ep_guess=0.01\n",
    "noise_displacement=0.5\n",
    "lr_actor=0.01\n",
    "lr_critic=0.001\n",
    "tau=0.005\n",
    "repetitions=1\n",
    "plots=True\n",
    "\n",
    "if not os.path.exists(\"results\"):\n",
    "    os.makedirs(\"results\")\n",
    "\n",
    "amplitude = 0.4\n",
    "buffer = ReplayBuffer(buffer_size=buffer_size)\n",
    "\n",
    "critic = Critic()\n",
    "critic_target = Critic()\n",
    "actor = Actor(input_dim=1)\n",
    "\n",
    "actor(np.array([[0.],[1.]])) #initialize the network 0, arbitrary inputs.\n",
    "#\n",
    "optimizer_critic = tf.keras.optimizers.Adam(lr=lr_critic)\n",
    "optimizer_actor = tf.keras.optimizers.Adam(lr=lr_actor)\n",
    "\n",
    "\n",
    "rt = []\n",
    "pt = []\n",
    "\n",
    "#define this global so i use them in a function defined above... optimizatin step and testing()\n",
    "train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\n",
    "test_loss = tf.keras.metrics.Mean('test_loss', dtype=tf.float32)\n",
    "\n",
    "\n",
    "if special_name == \"\":\n",
    "    # current_run_and_time = \"results/{}\".format(datetime.now().strftime(\"%Y%m%d-%H%M\"))\n",
    "    numb = record()\n",
    "    current_run_and_time =\"results/run_\" + str(numb)\n",
    "else:\n",
    "    current_run_and_time = \"results/\"+special_name\n",
    "\n",
    "directory = current_run_and_time\n",
    "train_log =  current_run_and_time + '/train_l0'\n",
    "test_log =   current_run_and_time + '/test_l0'\n",
    "\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log)\n",
    "test_summary_writer_0 = tf.summary.create_file_writer(test_log)\n",
    "\n",
    "info_optimizers = \"optimizer_critic_guess: {} \\nOptimizer_actor_l0: {}\\n\".format(optimizer_critic.get_config(), optimizer_actor.get_config())\n",
    "infor_buffer = \"Buffer_size: {}\\n Batch_size for sampling: {}\\n\".format(buffer.buffer_size, batch_size)\n",
    "info_epsilons= \"epsilon-guess: {}\\nepsilon_displacement_noise: {}\".format(ep_guess,noise_displacement)\n",
    "\n",
    "data = \"tau: {}, repetitions per optimization step (would be like epochs): {}\".format(tau,repetitions) + \"\\n \\n**** optimizers ***\\n\"+info_optimizers+\"\\n\\n\\n*** BUFFER ***\\n\"+infor_buffer+\"\\n\\n\\n *** NOISE PARAMETERS *** \\n\"+info_epsilons\n",
    "with open(directory+\"/info.txt\", 'w') as f:\n",
    "    f.write(data)\n",
    "    f.close()\n",
    "\n",
    "print(\"Beggining to train! \\n \\n\")\n",
    "print(data)\n",
    "print(\"starting time: {}\".format(datetime.now().strftime(\"%Y%m%d-%H%M%S\")))\n",
    "print(\"saving results in \" + str(directory))\n",
    "avg_train = []\n",
    "avg_test = []\n",
    "\n",
    "history_betas = [] #to put in histogram\n",
    "history_betas_would_have_done=[] #to put in histogram\n",
    "histo_preds = {\"critic\":{}} #here i save the predictions to plot in a \"straightforward way\"\n",
    "\n",
    "#######\n",
    "for episode in tqdm(range(total_episodes)):\n",
    "\n",
    "    alice_phase = np.random.choice([-1.,1.],1)[0]\n",
    "    beta_would_do = actor(np.array([[0.]])).numpy()[0][0]\n",
    "    beta =  beta_would_do + np.random.uniform(-noise_displacement, noise_displacement)\n",
    "    proboutcome = Prob(alice_phase*amplitude,beta,0)\n",
    "    outcome = np.random.choice([0.,1.],1,p=[proboutcome, 1-proboutcome])[0]\n",
    "\n",
    "    history_betas.append(beta)\n",
    "    history_betas_would_have_done.append(beta_would_do)\n",
    "\n",
    "#\n",
    "    if np.random.random()< ep_guess:\n",
    "        guess = np.random.choice([-1.,1.],1)[0]\n",
    "    else:\n",
    "        sequence = np.array([[ [beta, critic.pad_value], [outcome, -1.]]  ]).astype(np.float32)\n",
    "        guess = critic.give_favourite_guess(sequence)\n",
    "    if guess == alice_phase:\n",
    "        reward = 1.\n",
    "    else:\n",
    "        reward = 0.\n",
    "    buffer.add(beta, outcome, guess, reward)\n",
    "\n",
    "    \n",
    "    ###### END OF OPTIMIZATION STEP ######\n",
    "    ###### END OF OPTIMIZATION STEP ######\n",
    "    \n",
    "    experiences = buffer.sample(batch_size)\n",
    "    sequences, zeroed_rews = process_sequence(experiences)\n",
    "    labels_critic = give_td_error_Kennedy_guess(critic_target, sequences, zeroed_rews)\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(critic.trainable_variables)\n",
    "        preds_critic = critic(sequences)\n",
    "        loss_critic = tf.keras.losses.MSE(labels_critic, preds_critic)\n",
    "        loss_critic = tf.reduce_mean(loss_critic)\n",
    "        grads = tape.gradient(loss_critic, critic.trainable_variables)\n",
    "        optimizer_critic.apply_gradients(zip(grads, critic.trainable_variables))\n",
    "        train_loss(loss_critic)\n",
    "    \n",
    "    critic_target.update_target_parameters(critic, tau=0.01)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        ones = tf.ones(shape=(experiences.shape[0],1))\n",
    "        actions = tf.cast(actor(np.expand_dims(np.zeros(len(experiences)),axis=1)), tf.float32)\n",
    "        tape.watch(actions)\n",
    "        qvals = critic(tf.expand_dims(tf.concat([actions, ones], axis=1),axis=1))\n",
    "        dq_da = tape.gradient(qvals, actions)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        actionss = tf.cast(actor(np.expand_dims(np.zeros(len(experiences)),axis=1)), tf.float32)\n",
    "        da_dtheta = tape.gradient(actionss, actor.trainable_variables, output_gradients=-dq_da)\n",
    "    \n",
    "    optimizer_actor.apply_gradients(zip(da_dtheta, actor.trainable_variables))\n",
    "    \n",
    "    ###### END OF OPTIMIZATION STEP ######\n",
    "    ###### END OF OPTIMIZATION STEP ######\n",
    "    \n",
    "    avg_train.append(train_loss.result().numpy())\n",
    "    avg_test.append(test_loss.result().numpy())\n",
    "#\n",
    "    rt.append(reward)\n",
    "#\n",
    "#         ### calculate success probability if the agent went greedy ###\n",
    "         p=0\n",
    "         for outcome in [0.,1.]:\n",
    "             p+=Prob(critic_guess.give_favourite_guess(beta_would_do, outcome)*amplitude, beta_would_do,outcome)\n",
    "         p/=2\n",
    "         pt.append(p)\n",
    "    #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sequence(sample_buffer, pad_value = -4., LAYERS=1):\n",
    "    \"\"\"\" gets data obtained from N experiments: data.shape = (N, 2L+1),\n",
    "    where +1 accounts for the guess and 2L for (beta, outcome).\n",
    "    \n",
    "    [[a0, o1, a1, o2, a2, o3, a4]\n",
    "     [same but other experiment]\n",
    "     \n",
    "    ]\n",
    "    \n",
    "    and returns an array of shape (experiments, queries_RNN, 2 ), as accepted by an RNN\n",
    "    The idea is that i input [\\beta, pad_value], and then [outcome, guess].\n",
    "    \n",
    "    Or if I have two layers [\\beta, pa_value], [outcome, beta2], [outcome, guess],\n",
    "    \n",
    "    so the number of \"queries\" to the RNN is layers+1,\n",
    "    and i'm always interested in putting 2 values more.\n",
    "    \n",
    "    \"\"\"\n",
    "    batch_size = sample_buffer.shape[0]\n",
    "    data = sample_buffer[:,0:(LAYERS+1+1)] \n",
    "    pad_value = -4.\n",
    "    padded_data = np.ones((batch_size,LAYERS+1, 2))*pad_value\n",
    "    padded_data[:,0][:,0] = data[:,0]\n",
    "    for k in range(1,LAYERS+1):\n",
    "        padded_data[:,k] = data[:,[k,k+1]]\n",
    "        \n",
    "    rewards_obtained = np.zeros((batch_size, LAYERS+1)).astype(np.float32)\n",
    "    rewards_obtained[:,-1] = sample_buffer[:,-1]\n",
    "    \n",
    "    return padded_data, rewards_obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_td_error_Kennedy_guess(net,batched_input,sequential_rews_with_zeros):\n",
    "    '''\n",
    "    this function takes a batch with its corresponding labels\n",
    "    and retrieves what the true labels are according to network\n",
    "    prodection on next states.\n",
    "\n",
    "    For instance, my datapoint is [(\\beta, pad), (n, guess)] \n",
    "    and i want [Max_g Q(\\beta, n, guess), reward].\n",
    "    \n",
    "    \n",
    "    TO DO: extend this to more layers!!! \n",
    "    \n",
    "    So what you want is \n",
    "    [Max_{a_1} Q(a0, o1, a_1), \n",
    "    Max_{a_2} Q(a0, o1, a_1, o2, a_2) \n",
    "    ,...,\n",
    "    Max_g Q(h, guess)]\n",
    "    \n",
    "    But of course, we can't take the Max_g, so we replace by the target actor's choice !!!\n",
    "    '''\n",
    "    b = batched_input.copy()\n",
    "    ll = sequential_rews_with_zeros.copy()\n",
    "    preds1 = net(b)\n",
    "    b[:,1][:,1] = -b[:,1][:,1]\n",
    "    preds2 = net(b)\n",
    "    both = tf.concat([preds1,preds2],1)\n",
    "    maxs = np.squeeze(tf.math.reduce_max(both,axis=1).numpy())\n",
    "    ll[:,0] = maxs + ll[:,0]\n",
    "    ll = np.expand_dims(ll,axis=1)\n",
    "    return ll\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
