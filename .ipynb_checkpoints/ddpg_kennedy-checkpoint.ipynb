{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this tutorial we'll show how to train a DDPG agent to learn optimal Kennedy receiver. This involve not only displacement optimization but also to learn a (trivial, yet demaning) guessing rule among the possible phases. We aim to clear the background so in the next tutorial we can consider complex displacements and thereby more phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "import random\n",
    "import matplotlib\n",
    "\n",
    "#### this is the outcome probability, given by the overlap <0|\\alpha - \\beta>|Â¨^{2}\n",
    "def Prob(alpha, beta, n):\n",
    "    p0 = np.exp(-(alpha-beta)**2)\n",
    "    if n == 0:\n",
    "        return p0\n",
    "    else:\n",
    "        return 1-p0\n",
    "\n",
    "### this is just p(R=1 | g, n; beta) = p((-1^{g} alpha | n)) = p(n|allpha) pr(alpha)(p(n))\n",
    "def qval(beta, n, guess):\n",
    "    #dolinar guessing rule (= max-likelihood for L=1, careful sign of \\beta)\n",
    "    alpha = 0.4\n",
    "    pn = np.sum([Prob(g*alpha, beta, n) for g in [-1,1]])\n",
    "    return Prob(guess*alpha, beta, n)/pn\n",
    "\n",
    "def ps_maxlik(beta):\n",
    "    #dolinar guessing rule (= max-likelihood for L=1, careful sign of \\beta)\n",
    "    alpha = 0.4\n",
    "    p=0\n",
    "    for n1 in [0,1]:\n",
    "       p+=Prob(np.sign(beta)*(-1)**(n1)*alpha, beta, n1)\n",
    "    return p/2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now we create the replay memory, in which we store the transitions with the corresponding rewards, and also we add some data to test how the agent is doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, buffer_size=10**3):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.count = 0\n",
    "        self.betas = np.arange(-1.5,1.5,0.01)\n",
    "        self.buffer = deque()\n",
    "        self.create_test_datasets()\n",
    "        \n",
    "    def create_test_datasets(self):\n",
    "        dt_l0 = []\n",
    "        dt_l1 = []\n",
    "        \n",
    "        for k in self.betas:\n",
    "            dt_l0.append([k, ps_maxlik(k)])\n",
    "            for n in [0.,1.]:\n",
    "                for g in [-1.,1.]:\n",
    "                    dt_l1.append([k, n, g, qval(k,n,g)])\n",
    "        self.test_l0 = np.array(dt_l0)\n",
    "        self.test_l1 = np.array(dt_l1)\n",
    "        return\n",
    "    \n",
    "    def add(self, beta, outcome, guess, reward):\n",
    "        experience = (beta, outcome, guess, reward)\n",
    "        if self.count < self.buffer_size:\n",
    "            self.buffer.append(experience)\n",
    "            self.count += 1\n",
    "        else:\n",
    "            self.buffer.popleft()\n",
    "            self.buffer.append(experience)\n",
    "\n",
    "    def size(self):\n",
    "        return self.count\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = []\n",
    "        if self.count < batch_size:\n",
    "            batch = random.sample(self.buffer, self.count)\n",
    "        else:\n",
    "            batch = random.sample(self.buffer, int(batch_size))\n",
    "        beta_batch, outcome_batch, guess_batch, r_batch= list(map(np.array, list(zip(*batch))))\n",
    "        return np.array([beta_batch, outcome_batch, guess_batch, r_batch]).transpose()\n",
    "\n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "        self.count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the networks! notice we add some features that may be unimportant for some objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9d5408b8aba3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mCritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;31m#input_dim: 1 if layer=0, 3 if layer= 2, for the Kennedy receiver ##\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalreg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCritic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "class Critic(tf.keras.Model):\n",
    "    #input_dim: 1 if layer=0, 3 if layer= 2, for the Kennedy receiver ##\n",
    "    def __init__(self, input_dim, valreg=0.01, seed_val=0.1):\n",
    "        super(Critic,self).__init__()\n",
    "\n",
    "        self.l1 = Dense(50, input_shape=(input_dim,),kernel_initializer=tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val),\n",
    "        bias_initializer = tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val),\n",
    "        kernel_regularizer=tf.keras.regularizers.l1(valreg),\n",
    "    activity_regularizer=tf.keras.regularizers.l2(valreg))\n",
    "\n",
    "        self.l2 = Dense(50, kernel_regularizer=tf.keras.regularizers.l1(valreg),\n",
    "    activity_regularizer=tf.keras.regularizers.l2(valreg),\n",
    "    kernel_initializer=tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val),\n",
    "    bias_initializer = tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val))\n",
    "        self.l3 = Dense(50, kernel_regularizer=tf.keras.regularizers.l1(valreg),\n",
    "    activity_regularizer=tf.keras.regularizers.l2(valreg),\n",
    "    kernel_initializer=tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val),\n",
    "    bias_initializer = tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val))\n",
    "\n",
    "        self.l4 = Dense(50, kernel_regularizer=tf.keras.regularizers.l1(valreg),\n",
    "    activity_regularizer=tf.keras.regularizers.l2(valreg),\n",
    "    kernel_initializer=tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val),\n",
    "    bias_initializer = tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val))\n",
    "\n",
    "        self.l5 = Dense(1, kernel_regularizer=tf.keras.regularizers.l1(valreg),\n",
    "    activity_regularizer=tf.keras.regularizers.l2(valreg),\n",
    "    kernel_initializer=tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val),\n",
    "    bias_initializer = tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val))\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    def update_target_parameters(self,primary_net, tau=0.01):\n",
    "        #### only \n",
    "        prim_weights = primary_net.get_weights()\n",
    "        targ_weights = self.get_weights()\n",
    "        weights = []\n",
    "        for i in tf.range(len(prim_weights)):\n",
    "            weights.append(tau * prim_weights[i] + (1 - tau) * targ_weights[i])\n",
    "        self.set_weights(weights)\n",
    "        return\n",
    "\n",
    "    def call(self, input):\n",
    "        feat = tf.nn.relu(self.l1(input))\n",
    "#        feat = tf.nn.dropout(feat, rate=0.01)\n",
    " #       feat = tf.nn.relu(self.l2(feat))\n",
    "  #      feat = tf.nn.dropout(feat, rate=0.01)\n",
    "   #     feat = tf.nn.relu(self.l3(feat))\n",
    "        feat = tf.nn.relu(self.l4(feat))\n",
    "        feat = tf.nn.sigmoid(self.l5(feat))\n",
    "        return feat\n",
    "\n",
    "    def calculate_greedy_from_batch(self, batch):\n",
    "        \"\"\" this function is only to intended for Q(n, beta, guess).\n",
    "        Assuming batch = np.array([[beta, n, guess], [beta1, n1, guess], ...])\n",
    "        \n",
    "        \"\"\"\n",
    "        a = batch.copy()\n",
    "        preds1 = self(a)\n",
    "        a[:,2] = -a[:,2]\n",
    "        preds2 = self(a)\n",
    "        both = tf.concat([preds1,preds2],1)\n",
    "        maxs = np.squeeze(tf.math.reduce_max(both,axis=1))\n",
    "        maxs = np.expand_dims(maxs, axis=1)\n",
    "        return maxs\n",
    "    \n",
    "    def give_favourite_guess(self, beta, outcome):\n",
    "        \"\"\"\"This funciton is only intended for Q(n, beta, guess)\"\"\"\n",
    "        h1a2 = np.array([[beta, outcome,-1.]])\n",
    "        pred_minus = self(h1a2)\n",
    "        h1a2[:,2] = 1.\n",
    "        pred_plus = self(h1a2)\n",
    "        both = tf.concat([pred_plus,pred_minus],1)\n",
    "        maxs = tf.argmax(both,axis=1)\n",
    "        guess = (-1)**maxs.numpy()[0]\n",
    "        return guess\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "    \n",
    "    \n",
    "##### ACTOR CLASSS ####    \n",
    "class Actor(tf.keras.Model):\n",
    "    #input_dim: 1 if layer=0, 3 if layer= 2, for the Kennedy receiver ##\n",
    "    def __init__(self, input_dim=1, valreg=0.01, seed_val=0.1):\n",
    "        super(Actor,self).__init__()\n",
    "\n",
    "        self.l1 = Dense(50, input_shape=(input_dim,),kernel_initializer=tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val),\n",
    "        bias_initializer = tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val),\n",
    "        kernel_regularizer=tf.keras.regularizers.l1(valreg),\n",
    "    activity_regularizer=tf.keras.regularizers.l2(valreg))\n",
    "\n",
    "        self.l2 = Dense(50, kernel_regularizer=tf.keras.regularizers.l1(valreg),\n",
    "    activity_regularizer=tf.keras.regularizers.l2(valreg),\n",
    "    kernel_initializer=tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val),\n",
    "    bias_initializer = tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val))\n",
    "        self.l3 = Dense(50, kernel_regularizer=tf.keras.regularizers.l1(valreg),\n",
    "    activity_regularizer=tf.keras.regularizers.l2(valreg),\n",
    "    kernel_initializer=tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val),\n",
    "    bias_initializer = tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val))\n",
    "\n",
    "        self.l4 = Dense(50, kernel_regularizer=tf.keras.regularizers.l1(valreg),\n",
    "    activity_regularizer=tf.keras.regularizers.l2(valreg),\n",
    "    kernel_initializer=tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val),\n",
    "    bias_initializer = tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val))\n",
    "\n",
    "        self.l5 = Dense(1, kernel_regularizer=tf.keras.regularizers.l1(valreg),\n",
    "    activity_regularizer=tf.keras.regularizers.l2(valreg),\n",
    "    kernel_initializer=tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val),\n",
    "    bias_initializer = tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val))\n",
    "\n",
    "        \n",
    "    def update_target_parameters(self,primary_net, tau=0.01):\n",
    "        #### only \n",
    "        prim_weights = primary_net.get_weights()\n",
    "        targ_weights = self.get_weights()\n",
    "        weights = []\n",
    "        for i in tf.range(len(prim_weights)):\n",
    "            weights.append(tau * prim_weights[i] + (1 - tau) * targ_weights[i])\n",
    "        self.set_weights(weights)\n",
    "        return\n",
    "\n",
    "    def call(self, input):\n",
    "        feat = tf.nn.relu(self.l1(input))\n",
    "#        feat = tf.nn.dropout(feat, rate=0.01)\n",
    " #       feat = tf.nn.relu(self.l2(feat))\n",
    "        feat = tf.nn.dropout(feat, rate=0.01)\n",
    "   #     feat = tf.nn.relu(self.l3(feat))\n",
    "        feat = tf.nn.relu(self.l4(feat))\n",
    "        feat = tf.nn.tanh(self.l5(feat))\n",
    "        return feat\n",
    "\n",
    "    def calculate_greedy_from_batch(self, batch):\n",
    "        \"\"\" this function is only to intended for Q(n, beta, guess)\"\"\"\n",
    "        a = batch[1].copy()\n",
    "        preds1 = self(a)\n",
    "        a[:,2] = -a[:,2]\n",
    "        preds2 = self(a)\n",
    "        both = tf.concat([preds1,preds2],1)\n",
    "        maxs = np.squeeze(tf.math.reduce_max(both,axis=1))\n",
    "        maxs = np.expand_dims(maxs, axis=1)\n",
    "        return maxs\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This are some functions for 1) compute the predictions and 2) optimization step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_data(buffer,networks):\n",
    "    actor_q0, critic_q0, critic_guess, target_guess = networks\n",
    "    ### this is the test data for the guess network, defined in Dataset() classs\n",
    "    predstest = critic_guess(buffer.test_l1[:,[0,1,2]])\n",
    "    targets_1 = np.expand_dims(buffer.test_l1[:,3], axis=1)\n",
    "    loss_test_l1 = tf.keras.losses.MSE(targets_1, predstest)\n",
    "    loss_test_l1 = tf.reduce_mean(loss_test_l1)\n",
    "    test_loss_l1(loss_test_l1)\n",
    "    \n",
    "    ### this is the test data for the \\hat{Q}('beta) #####\n",
    "    preds_test_l0 = critic_q0(np.expand_dims(buffer.test_l0[:,0], axis=1))\n",
    "    loss_y0 = tf.keras.losses.MSE(np.expand_dims(buffer.test_l0[:,1], axis=1), preds_test_l0)\n",
    "    loss_y0 = tf.reduce_mean(loss_y0)\n",
    "    test_loss_l0(loss_y0)\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def optimization_step(networks, optimizers, buffer, batch_size=500., tau=0.01, repetitions=1):\n",
    "    actor_q0, critic_q0, critic_guess, target_guess = networks\n",
    "    optimizer_critic_guess,  optimizer_actor_l0, optimizer_critic_l0 = optimizers\n",
    "    for thoughts in range(repetitions):\n",
    "        experiences = buffer.sample(batch_size)\n",
    "\n",
    "        ##### update the critic guess according to rewards obtained \n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(critic_guess.trainable_variables)\n",
    "            preds_cguess = critic_guess(experiences[:,[0,1,2]])\n",
    "            labels_cguess = np.expand_dims(experiences[:,3],axis=1)\n",
    "            loss_prim_guess = tf.keras.losses.MSE(labels_cguess, preds_cguess)\n",
    "            loss_prim_guess = tf.reduce_mean(loss_prim_guess)\n",
    "            grads = tape.gradient(loss_prim_guess, critic_guess.trainable_variables)\n",
    "            optimizer_critic_guess.apply_gradients(zip(grads, critic_guess.trainable_variables))\n",
    "            train_loss_l1(loss_prim_guess)\n",
    "\n",
    "        ##### update the target guess ######\n",
    "        target_guess.update_target_parameters(critic_guess, tau=0.01) #check this value !\n",
    "\n",
    "        #### obtain the labels for the update of Q(\\beta)\n",
    "        labels_critic_l0 = target_guess.calculate_greedy_from_batch(experiences[:,[0,1,2]]) #greedy from target; this is the label for net_0!!\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(critic_q0.trainable_variables)\n",
    "            preds0 = critic_q0(np.expand_dims(experiences[:,0],axis=1))\n",
    "            loss_0 = tf.keras.losses.MSE(labels_critic_l0,preds0)\n",
    "            loss_0 = tf.reduce_mean(loss_0)\n",
    "            grads0 = tape.gradient(loss_0, critic_q0.trainable_variables)\n",
    "            optimizer_critic_l0.apply_gradients(zip(grads0, critic_q0.trainable_variables))\n",
    "        train_loss_l0(loss_0)\n",
    "\n",
    "        #### obtain the components for the chain for the update of \\pi( h_0 = nada!) = \\beta\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = actor_q0(np.expand_dims(np.zeros(len(experiences)),axis=1))\n",
    "            tape.watch(actions)\n",
    "            qvals = critic_q0(actions)\n",
    "            dq_da = tape.gradient(qvals, actions)\n",
    "\n",
    "        ### update actor \\pi( h_0) = \\beta\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = actor_q0(np.expand_dims(np.zeros(len(experiences)),axis=1))\n",
    "            da_dtheta = tape.gradient(actions, actor_q0.trainable_variables, output_gradients=-dq_da)\n",
    "            optimizer_actor_l0.apply_gradients(zip(da_dtheta, actor_q0.trainable_variables))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BigPlot(buffer, rt, pt, history_betas, history_betas_would_have_done, histo_preds, losses, directory):\n",
    "\n",
    "    matplotlib.rc('font', serif='cm10')\n",
    "    plt.rcParams.update({'font.size': 40})\n",
    "\n",
    "    plt.figure(figsize=(60,60), dpi=100)\n",
    "    plt.subplots_adjust(wspace=0.5, hspace=0.3)\n",
    "\n",
    "    T=len(rt)\n",
    "    ax1=plt.subplot2grid((2,4),(0,0))\n",
    "    ax2=plt.subplot2grid((2,4),(1,0))\n",
    "    ax3=plt.subplot2grid((2,4),(0,1))\n",
    "    ax4=plt.subplot2grid((2,4),(1,1))\n",
    "    ax5=plt.subplot2grid((2,4),(0,2))\n",
    "    ax6=plt.subplot2grid((2,4),(1,2))\n",
    "    ax7=plt.subplot2grid((2,4),(0,3))\n",
    "    ax8=plt.subplot2grid((2,4),(1,3))\n",
    "\n",
    "    optimal = max([ps_maxlik(b) for b in buffer.betas])\n",
    "\n",
    "    ### ploting the \\Rt and \\Pt ###\n",
    "    ax1.plot(np.log10(np.arange(1,T+1)),rt, color=\"red\", linewidth=15, alpha=0.8, label=r'$R_t$')\n",
    "    ax1.plot(np.log10(np.arange(1,T+1)),optimal*np.ones(T), color=\"black\",  linewidth=15,alpha=0.5, label=\"optimal\")\n",
    "    ax1.plot(np.log10(np.arange(1,T+1)),pt, color=\"blue\", linewidth=8, alpha=0.3, label=r'$P_t (fluctuates!)$')\n",
    "\n",
    "    ## ploting the histogram for betas ##\n",
    "    optimal_beta = buffer.betas[np.where(ps_maxlik(buffer.betas) == max(ps_maxlik(buffer.betas)))[0][0]]\n",
    "    ax2.hist(history_betas,bins=100, facecolor='r', alpha=0.6, edgecolor='blue', label=\"done\")\n",
    "    ax2.hist(history_betas_would_have_done,bins=100, facecolor='g', alpha=0.4, edgecolor='black', label=\"would have done\")\n",
    "    ax2.text(optimal_beta, 0, \"*\", size=30)\n",
    "    ax2.text(-optimal_beta, 0, \"*\", size=30)\n",
    "\n",
    "    ## ploting the history of betas ##\n",
    "    ax3.plot(np.arange(1, len(history_betas)+1),history_betas, color=\"red\", linewidth=15, alpha=0.8, label=\"done\")\n",
    "    ax3.plot(np.arange(1, len(history_betas)+1),history_betas_would_have_done, color=\"green\", linewidth=15, alpha=0.8, label=\"would have done\")\n",
    "    ax3.plot(np.arange(1, len(history_betas)+1),np.ones(len(history_betas))*optimal_beta, color=\"black\", linewidth=15, alpha=0.8, label=\"optimal-beta\")\n",
    "    ax3.plot(np.arange(1, len(history_betas)+1),-np.ones(len(history_betas))*optimal_beta, color=\"black\", linewidth=15, alpha=0.8)#, label=\"optimal-beta\")\n",
    "\n",
    "\n",
    "    #### in here i plot the loss for the first Q(0), the test and the train. Notice they have different scale! I use different colors!\n",
    "    c=0\n",
    "    lab = [\"train\",\"test\"]\n",
    "    colors = [\"tab:red\",\"tab:blue\"]\n",
    "    for loss in losses[0]:\n",
    "        color = colors[c]\n",
    "        ax4.plot(np.arange(1,len(loss)+1),loss,'--',alpha=0.85,c=colors[c], linewidth=5)#, label=\"Preds Q(\" + r'$\\beta$'+\")-\"+lab[c])#, label=\"Q(n1=0,\"+r'$\\beta$'+\"; g=-1)\")\n",
    "        ax4.scatter(np.arange(1,len(loss)+1),loss,s=150,alpha=0.85,c=colors[c], linewidth=5)#,label=\"Preds Q(\\beta) - \"+lab[c])#, label=\"Q(n1=0,\"+r'$\\beta$'+\"; g=-1)\")\n",
    "        ax4.set_xlabel(\"epoch\", size=120)\n",
    "        ax4.set_ylabel(\"Loss Q(\"+r'$\\beta )$', size=100, color =colors[c])\n",
    "        ax4.tick_params(axis='y', labelcolor=colors[c])\n",
    "        ax4.legend()\n",
    "        ax4 = ax4.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "        c+=1\n",
    "\n",
    "    #### in here i plot the loss for the first Q(\\beta, n, guess ), the test and the train. Notice they have different scale! I use different colors!\n",
    "    c=0\n",
    "    for loss in losses[1]:\n",
    "        ax5.plot(np.arange(1,len(loss)+1),loss,'--',alpha=0.85,c=colors[c], linewidth=5)#, label=\"Preds Q(n, \"+r'$\\beta$'+\", guess) - \"+lab[c])#, label=\"Q(n1=0,\"+r'$\\beta$'+\"; g=-1)\")\n",
    "        ax5.scatter(np.arange(1,len(loss)+1),loss,s=150,alpha=0.85,c=colors[c], linewidth=5)#, label=\"Preds Q(n, \\beta, guess) - \"+lab[c])#, label=\"Q(n1=0,\"+r'$\\beta$'+\"; g=-1)\")\n",
    "        ax5.set_xlabel(\"epoch\", size=120)\n",
    "        ax5.set_ylabel(\"Loss Q(\" + r'$\\beta$'+\", n, guess)\",size=20, color =colors[c])\n",
    "        ax5.tick_params(axis='y', size=100, labelcolor=colors[c])\n",
    "        #ax5.legend()\n",
    "        ax5 = ax5.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "        c+=1\n",
    "        #plt.tight_layout()  # otherwise the right y-label is slightly clipped    \n",
    "\n",
    "\n",
    "    betas_train = buffer.betas\n",
    "    for predictions in histo_preds[\"net_1\"].values():\n",
    "        ax7.plot(betas_train,predictions[\"values\"][\"0\"],alpha=0.25, linewidth=5)#, label=\"epoch: \"+str(predictions[\"epoch_number\"])) #, label=r'$\\hat{Q}$'+\"(n1=0,\"+r'$\\beta$'+\"; g=-1)\")\n",
    "        ax7.plot(betas_train,predictions[\"values\"][\"1\"],alpha=0.25, linewidth=5)#, label=\"epoch: \"+str(predictions[\"epoch_number\"]))#,label=r'$\\hat{Q}$'+\"(n1=0,\"+r'$\\beta$'+\"; g=1)\")\n",
    "\n",
    "        ax8.plot(betas_train,predictions[\"values\"][\"2\"] ,alpha=0.25,  linewidth=5)#, label=\"epoch: \"+str(predictions[\"epoch_number\"]))#label=r'$\\hat{Q}$'+\"(n1=1,\"+r'$\\beta$'+\"; g=-1)\")\n",
    "        ax8.plot(betas_train,predictions[\"values\"][\"3\"] ,alpha=0.25,  linewidth=5)#, label=\"epoch: \"+str(predictions[\"epoch_number\"]))#,label=r'$\\hat{Q}$'+\"(n1=1,\"+r'$\\beta$'+\"; g=1)\")\n",
    "\n",
    "    #Now we take the last and plot it in bold!\n",
    "    ax7.plot(betas_train,predictions[\"values\"][\"0\"],alpha=0.85, c=\"black\",linewidth=8)#), label=\"epoch: \"+str(predictions[\"epoch_number\"])) #, label=r'$\\hat{Q}$'+\"(n1=0,\"+r'$\\beta$'+\"; g=-1)\")\n",
    "    ax7.plot(betas_train,predictions[\"values\"][\"1\"],alpha=0.85, c=\"purple\", linewidth=8)#, label=\"epoch: \"+str(predictions[\"epoch_number\"]))#,label=r'$\\hat{Q}$'+\"(n1=0,\"+r'$\\beta$'+\"; g=1)\")\n",
    "    ax7.scatter(betas_train,predictions[\"values\"][\"0\"],alpha=0.85, c=\"black\",s=150)\n",
    "    ax7.scatter(betas_train,predictions[\"values\"][\"1\"],alpha=0.85, c=\"purple\",s=150)\n",
    "\n",
    "    ax8.plot(betas_train,predictions[\"values\"][\"2\"] ,alpha=0.85, c=\"black\", linewidth=8)#, label=\"epoch: \"+str(predictions[\"epoch_number\"]))#label=r'$\\hat{Q}$'+\"(n1=1,\"+r'$\\beta$'+\"; g=-1)\")\n",
    "    ax8.plot(betas_train,predictions[\"values\"][\"3\"] ,alpha=0.85,  c=\"purple\",linewidth=8)#, label=\"epoch: \"+str(predictions[\"epoch_number\"]))#,label=r'$\\hat{Q}$'+\"(n1=1,\"+r'$\\beta$'+\"; g=1)\")\n",
    "    ax8.scatter(betas_train,predictions[\"values\"][\"2\"],alpha=0.85, c=\"black\",s=150)\n",
    "    ax8.scatter(betas_train,predictions[\"values\"][\"3\"],alpha=0.85, c=\"purple\",s=150)\n",
    "\n",
    "\n",
    "        ### we do the same for ax3:\n",
    "\n",
    "    for predictions in histo_preds[\"net_0\"].values():\n",
    "        ax6.plot(betas_train,predictions[\"values\"],alpha=0.15, linewidth=5)#, label=\"epoch: \"+str(predictions[\"epoch_number\"])) #, label=r'$\\hat{Q}$'+\"(n1=0,\"+r'$\\beta$'+\"; g=-1)\")\n",
    "\n",
    "    #The last one black and bigger!\n",
    "    ax6.plot(betas_train,predictions[\"values\"],alpha=0.85,c=\"black\", linewidth=5)#, label=\"epoch: \"+str(predictions[\"epoch_number\"])) #, label=r'$\\hat{Q}$'+\"(n1=0,\"+r'$\\beta$'+\"; g=-1)\")\n",
    "\n",
    "\n",
    "    ##### here we plot the true values (that we want to learn!!!) ###\n",
    "    ax7.plot(buffer.betas,[qval(b, 0, -1) for b in buffer.betas],'--',alpha=0.85,c=\"red\", linewidth=8, label=\"Q(n1=0,\"+r'$\\beta$'+\"; g=-1)\")\n",
    "    ax7.plot(buffer.betas,[qval(b, 0, 1) for b in buffer.betas],'--',alpha=0.85,c=\"blue\",  linewidth=8,label=\"Q(n1=0,\"+r'$\\beta$'+\"; g=1)\")\n",
    "\n",
    "    ax8.plot(buffer.betas,[qval(b, 1, -1) for b in buffer.betas],'--',alpha=0.85,c=\"red\",  linewidth=8,label=\"Q(n1=1,\"+r'$\\beta$'+\"; g=-1)\")\n",
    "    ax8.plot(buffer.betas,[qval(b, 1, 1) for b in buffer.betas],'--',alpha=0.85,c=\"blue\",  linewidth=8,label=\"Q(n1=1,\"+r'$\\beta$'+\"; g=1)\")\n",
    "\n",
    "    ax6.plot(buffer.betas,[ps_maxlik(b) for b in buffer.betas],'--',alpha=0.85,c=\"red\", linewidth=8)\n",
    "    ax6.set_ylabel(r'$P_s\\; ( \\beta )$', size=20)\n",
    "    ##### here we plot the true values (that we want to learn!!!) ###\n",
    "\n",
    "\n",
    "\n",
    "    for ax in [ax6, ax7, ax8]:\n",
    "        ax.set_xlabel(r'$\\beta$', size=20)\n",
    "\n",
    "    for ax in [ax1, ax2, ax3,ax4,ax5,ax6, ax7, ax8]:\n",
    "        ax.legend()\n",
    "    \n",
    "    #plt.tight_layout()\n",
    "    plt.savefig(directory+\"/big_plot.png\")\n",
    "    plt.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_inside_buffer(buffer, directory):\n",
    "    \n",
    "\n",
    "    plt.figure(figsize=(15,10))\n",
    "    ax1 =  plt.subplot2grid((1,2),(0,0))\n",
    "    ax2 =  plt.subplot2grid((1,2),(0,1))\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "\n",
    "    histo = {}\n",
    "    number = {}\n",
    "\n",
    "    data_collected = np.asarray(buffer.buffer)\n",
    "    for k in data_collected[:,0]:\n",
    "        for g in [-1.,1.]:\n",
    "            for outcome in [0.,1.]:\n",
    "\n",
    "                histo[str(np.round(k,2))+\"n\"+str(outcome)+\"g\"+str(g)] = 0\n",
    "                number[str(np.round(k,2))+\"n\"+str(outcome)+\"g\"+str(g)] = 1\n",
    "\n",
    "    for dato in data_collected:\n",
    "        histo[str(np.round(dato[0],2))+\"n\"+str(dato[1])+\"g\"+str(dato[2])] += dato[3]\n",
    "        number[str(np.round(dato[0],2))+\"n\"+str(dato[1])+\"g\"+str(dato[2])] += 1\n",
    "\n",
    "    for k in data_collected[:,0]:\n",
    "        for g in [-1.,1.]:\n",
    "            for outcome in [0.,1.]:\n",
    "                histo[str(np.round(k,2))+\"n\"+str(outcome)+\"g\"+str(g)] /=number[str(np.round(k,2))+\"n\"+str(outcome)+\"g\"+str(g)] \n",
    "\n",
    "\n",
    "\n",
    "    betas  = [np.round(b,2) for b in data_collected[:,0]]\n",
    "    ax1.plot(betas,[histo[str(np.round(b,2))+\"n0.0g-1.0\"] for b in data_collected[:,0]],alpha=0.5,c=\"red\", linewidth=5, label=\"Q(n1=0,\"+r'$\\beta$'+\"; g=-1)\")\n",
    "    ax1.plot(betas,[histo[str(np.round(b,2))+\"n0.0g1.0\"] for b in data_collected[:,0]],alpha=0.5,c=\"blue\", linewidth=5, label=\"Q(n1=0,\"+r'$\\beta$'+\"; g=-1)\")\n",
    "\n",
    "    ax2.plot(betas,[histo[str(np.round(b,2))+\"n1.0g-1.0\"] for b in data_collected[:,0]],alpha=0.5,c=\"red\", linewidth=5, label=\"Q(n1=1,\"+r'$\\beta$'+\"; g=-1)\")\n",
    "    ax2.plot(betas,[histo[str(np.round(b,2))+\"n1.0g1.0\"] for b in data_collected[:,0]],alpha=0.5,c=\"blue\", linewidth=5, label=\"Q(n1=1,\"+r'$\\beta$'+\"; g=-1)\")\n",
    "\n",
    "    betas = np.arange(-1.5,1.5,.01)\n",
    "    ax1.plot(betas,[qval(b, 0, -1) for b in betas],'--',alpha=0.5,c=\"red\", linewidth=5, label=\"True Q(n1=0,\"+r'$\\beta$'+\"; g=-1)\")\n",
    "    ax1.plot(betas,[qval(b, 0, 1) for b in betas],'--',alpha=0.5,c=\"blue\",  linewidth=5,label=\"True Q(n1=0,\"+r'$\\beta$'+\"; g=1)\")\n",
    "\n",
    "    ax2.plot(betas,[qval(b, 1, -1) for b in betas],'--',alpha=0.5,c=\"red\",  linewidth=5,label=\"True Q(n1=1,\"+r'$\\beta$'+\"; g=-1)\")\n",
    "    ax2.plot(betas,[qval(b, 1, 1) for b in betas],'--',alpha=0.5,c=\"blue\",  linewidth=5,label=\"True Q(n1=1,\"+r'$\\beta$'+\"; g=1)\")\n",
    "\n",
    "\n",
    "    for ax in [ax1, ax2]:\n",
    "        ax.set_xlabel(r'$\\beta$', size=20)\n",
    "        ax.legend(prop={\"size\":15})\n",
    "\n",
    "    plt.savefig(directory+\"/inside_buffer.png\")\n",
    "    plt.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### if you want to delete the folder with data generated by tensorflow (and the program), you can do it with this command .... !rm -rf \"logs/ddpg_results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf \"logs/ddpg_results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpgKennedy(total_episodes = 10**3,buffer_size=500, batch_size=64, ep_guess=0.01, noise_displacement=0.5,lr_actor=0.01, lr_critic=0.001, tau=0.005, repetitions=1):\n",
    "\n",
    "    amplitude = 0.4\n",
    "    buffer = ReplayBuffer(buffer_size=buffer_size)\n",
    "    \n",
    "    critic_q0 = Critic(input_dim=1)\n",
    "    actor_q0 = Actor(input_dim=1)\n",
    "    critic_guess = Critic(input_dim=3)\n",
    "    target_guess = Critic(input_dim=3)\n",
    "\n",
    "    critic_q0(np.array([[0.],[1.]])) #initialize the network 0, arbitrary inputs.\n",
    "    actor_q0(np.array([[0.],[1.]])) #initialize the network 0, arbitrary inputs.\n",
    "    critic_guess(np.array([[0.,1.,1.]]))\n",
    "    target_guess(np.array([[0.,1.,1.]]))\n",
    "    #\n",
    "    optimizer_critic_guess = tf.keras.optimizers.Adam(lr=lr_critic)\n",
    "    optimizer_actor_l0 = tf.keras.optimizers.Adam(lr=lr_actor)\n",
    "    optimizer_critic_l0 = tf.keras.optimizers.Adam(lr=lr_actor)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    rt = []\n",
    "    pt = [] \n",
    "\n",
    "    global train_loss_l0, train_loss_l1, test_loss_l0, test_loss_l1 #define this global so i use them in a function defined above... optimizatin step and testing()\n",
    "    train_loss_l0 = tf.keras.metrics.Mean('train_loss_l0', dtype=tf.float32)\n",
    "    test_loss_l0 = tf.keras.metrics.Mean('test_loss_l0', dtype=tf.float32)\n",
    "    train_loss_l1 = tf.keras.metrics.Mean('train_loss_l1', dtype=tf.float32)\n",
    "    test_loss_l1 = tf.keras.metrics.Mean('test_loss_l1', dtype=tf.float32)\n",
    "\n",
    "\n",
    "    current_run_and_time = \"time-{}\".format(datetime.now().strftime(\"%Y%m%d-%H%M\"))\n",
    "    directory = 'logs/ddpg_results/' + current_run_and_time \n",
    "    train_log_dir_0 = 'logs/ddpg_results/' + current_run_and_time + '/train_l0'\n",
    "    test_log_dir_0 = 'logs/ddpg_results/' +  current_run_and_time + '/test_l0'\n",
    "    train_log_dir_1 = 'logs/ddpg_results/' +  current_run_and_time + '/train_l1'\n",
    "    test_log_dir_1 = 'logs/ddpg_results/' +  current_run_and_time + '/test_l1'\n",
    "    train_summary_writer_0 = tf.summary.create_file_writer(train_log_dir_0)\n",
    "    train_summary_writer_1 = tf.summary.create_file_writer(train_log_dir_1)\n",
    "    test_summary_writer_0 = tf.summary.create_file_writer(test_log_dir_0)\n",
    "    test_summary_writer_1 = tf.summary.create_file_writer(test_log_dir_1)\n",
    "\n",
    "    info_optimizers = \"optimizer_critic_guess: {} \\nOptimizer_actor_l0: {}\\nOptimizer_critic_l0: {}\\n\".format(optimizer_critic_guess.get_config(), optimizer_actor_l0.get_config(), optimizer_critic_l0)\n",
    "    infor_buffer = \"Buffer_size: {}\\n Batch_size for sampling: {}\\n\".format(buffer.buffer_size, batch_size)\n",
    "    info_epsilons= \"epsilon-guess: {}\\nepsilon_displacement_noise: {}\".format(ep_guess,noise_displacement)\n",
    "    \n",
    "    data = \"tau: {}, repetitions per optimization step (would be like epochs): {}\".format(tau,repetitions) + \"\\n \\n**** optimizers ***\\n\"+info_optimizers+\"\\n\\n\\n*** BUFFER ***\\n\"+infor_buffer+\"\\n\\n\\n *** NOISE PARAMETERS *** \\n\"+info_epsilons\n",
    "    with open(directory+\"/info.txt\", 'w') as f:\n",
    "        f.write(data)\n",
    "        f.close()    \n",
    "    \n",
    "    print(\"Beggining to train! \\n \\n\")\n",
    "    print(data)\n",
    "    print(\"starting time: {}\".format(datetime.now().strftime(\"%Y%m%d-%H%M\")))\n",
    "    print(\"saving results in \" + str(directory))\n",
    "    avg_train_l0 = []\n",
    "    avg_train_l1 = []\n",
    "    avg_test_l0 = []\n",
    "    avg_test_l1 = []\n",
    "\n",
    "    history_betas = [] #to put in histogram\n",
    "    history_betas_would_have_done=[] #to put in histogram\n",
    "    histo_preds = {\"net_0\":{}, \"net_1\":{}} #here i save the predictions to plot in a \"straightforward way\"\n",
    "\n",
    "    #######\n",
    "    for episode in tqdm(range(total_episodes)):\n",
    "\n",
    "        alice_phase = np.random.choice([-1.,1.],1)[0]\n",
    "        beta_would_do = actor_q0(np.array([[0.]])).numpy()[0][0]\n",
    "        beta =  beta_would_do + np.random.uniform(-noise_displacement, noise_displacement)\n",
    "        proboutcome = Prob(alice_phase*amplitude,beta,0)\n",
    "        outcome = np.random.choice([0.,1.],1,p=[proboutcome, 1-proboutcome])[0]\n",
    "\n",
    "        history_betas.append(beta)\n",
    "        history_betas_would_have_done.append(beta_would_do)\n",
    "        #epsilon-greedy choice of the guessing! Do you imagine other way to do this? How would you apply UCB ? discretize?\n",
    "        if np.random.random()< ep_guess:\n",
    "            guess = np.random.choice([-1.,1.],1)[0]\n",
    "        else:\n",
    "            guess = critic_guess.give_favourite_guess(beta, outcome) \n",
    "        if guess == alice_phase:\n",
    "            reward = 1.\n",
    "        else:\n",
    "            reward = 0.\n",
    "        buffer.add(beta, outcome, guess, reward)\n",
    "\n",
    "\n",
    "        ### optimization step and testing the generalization performance ! ####\n",
    "        optimization_step(networks=[actor_q0, critic_q0, critic_guess, target_guess], \n",
    "                          optimizers = [optimizer_critic_guess,  optimizer_actor_l0, optimizer_critic_l0 ],buffer=buffer,\n",
    "                          batch_size=batch_size,repetitions=repetitions)\n",
    "        testing_data(buffer, networks=[actor_q0, critic_q0, critic_guess, target_guess])\n",
    "\n",
    "        ### i append the losses to plot them later ###\n",
    "        avg_train_l0.append(train_loss_l0.result().numpy())\n",
    "        avg_train_l1.append(train_loss_l1.result().numpy())\n",
    "        avg_test_l0.append(test_loss_l0.result().numpy())\n",
    "        avg_test_l1.append(test_loss_l1.result().numpy())\n",
    "\n",
    "\n",
    "        ### appending the reward to calculate cumulative! ###\n",
    "        rt.append(reward)\n",
    "\n",
    "        ### calculate success probability if the agent went greedy ###\n",
    "        p=0\n",
    "        for outcome in [0.,1.]:\n",
    "            p+=Prob(critic_guess.give_favourite_guess(beta_would_do, outcome)*amplitude, beta_would_do,outcome)\n",
    "        p/=2\n",
    "        pt.append(p)\n",
    "        \n",
    "\n",
    "        \n",
    "        with train_summary_writer_0.as_default():\n",
    "            tf.summary.scalar('loss', train_loss_l0.result(), step=episode)\n",
    "        with test_summary_writer_0.as_default():\n",
    "            tf.summary.scalar('loss', test_loss_l0.result(), step=episode)\n",
    "        with train_summary_writer_1.as_default():\n",
    "            tf.summary.scalar('loss', train_loss_l1.result(), step=episode)\n",
    "        with test_summary_writer_1.as_default():\n",
    "            tf.summary.scalar('loss', test_loss_l1.result(), step=episode)\n",
    "\n",
    "        if episode%(total_episodes/10) == 0: #this is for showing 10 results in total.\n",
    "\n",
    "            template = 'Episode {}, \\Rt: {}, \\Pt: {}, Train loss_l1: {}, Test loss_l1: {}, Train Loss_l0: {}, Test Loss_l0: {}\\n\\n'\n",
    "            print(template.format(episode+1,\n",
    "                                np.sum(rt)/(episode+1),\n",
    "                                  pt[-1],\n",
    "                                 np.round(train_loss_l1.result().numpy(),5), \n",
    "                                 test_loss_l1.result().numpy(), \n",
    "                                 np.round(train_loss_l0.result().numpy(),15),\n",
    "                                 np.round(test_loss_l0.result().numpy(),5))\n",
    "                  )\n",
    "\n",
    "            for nett in [\"net_0\",\"net_1\"]: #net_0 will be critic_q0, net_1 will be critic_qguess\n",
    "\n",
    "                histo_preds[nett][str(episode)] ={}\n",
    "                histo_preds[nett][str(episode)][\"episode\"] = episode\n",
    "                histo_preds[nett][str(episode)][\"values\"] = {}\n",
    "\n",
    "                histo_preds[\"net_0\"][str(episode)][\"values\"] = np.squeeze(critic_q0(np.expand_dims(buffer.betas,axis=1)))\n",
    "\n",
    "            index=0\n",
    "            for n1 in [0.,1.]:\n",
    "                for guess in [-1.,1.]:\n",
    "                    foo =np.array([[b,n1,guess] for b in buffer.betas]) #betas_train defined as global in create_dataset_l2()\n",
    "                    histo_preds[\"net_1\"][str(episode)][\"values\"][str(index)] = np.squeeze(critic_guess(foo))\n",
    "                    index+=1\n",
    "\n",
    "    \n",
    "    rt = [np.sum(rt[:k]) for k in range(len(rt))]\n",
    "    rt = rt/np.arange(1,len(rt)+1)\n",
    "    losses = [[avg_train_l0, avg_test_l0], [ avg_train_l1, avg_test_l1]]\n",
    "    BigPlot(buffer,rt, pt, history_betas, history_betas_would_have_done, histo_preds, losses, directory)\n",
    "    plot_inside_buffer(buffer, directory)\n",
    "    return #rt, pt, history_betas, history_betas_would_have_done, histo_preds, losses, name_directory-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beggining to train! \n",
      " \n",
      "\n",
      "tau: 0.1, repetitions per optimization step (would be like epochs): 1\n",
      " \n",
      "**** optimizers ***\n",
      "optimizer_critic_guess: {'name': 'Adam', 'learning_rate': 0.001, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False} \n",
      "Optimizer_actor_l0: {'name': 'Adam', 'learning_rate': 0.0001, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
      "Optimizer_critic_l0: <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f94e0679ef0>\n",
      "\n",
      "\n",
      "\n",
      "*** BUFFER ***\n",
      "Buffer_size: 100000\n",
      " Batch_size for sampling: 500\n",
      "\n",
      "\n",
      "\n",
      " *** NOISE PARAMETERS *** \n",
      "epsilon-guess: 0.01\n",
      "epsilon_displacement_noise: 0.1\n",
      "starting time: 20200406-1838\n",
      "saving results in logs/ddpg_results/time-20200406-1838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:67: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "237de415ad1d41fca2f7462a121e1d62"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, \\Rt: 0.0, \\Pt: 0.5, Train loss_l1: 0.2627300024032593, Test loss_l1: 0.09169825911521912, Train Loss_l0: 0.0001330637460341677, Test Loss_l0: 0.04749999940395355\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:65: DeprecationWarning: Both axis > a.ndim and axis < -a.ndim - 1 are deprecated and will raise an AxisError in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 101, \\Rt: 0.4158415841584158, \\Pt: 0.5061216628970331, Train loss_l1: 0.24192999303340912, Test loss_l1: 0.10184302181005478, Train Loss_l0: 0.00014388796989805996, Test Loss_l0: 0.05056000128388405\n",
      "\n",
      "\n",
      "Episode 201, \\Rt: 0.472636815920398, \\Pt: 0.4078792823212474, Train loss_l1: 0.24467000365257263, Test loss_l1: 0.09908901154994965, Train Loss_l0: 0.0002171872038161382, Test Loss_l0: 0.05592000111937523\n",
      "\n",
      "\n",
      "Episode 301, \\Rt: 0.4584717607973422, \\Pt: 0.5, Train loss_l1: 0.24553999304771423, Test loss_l1: 0.09966950118541718, Train Loss_l0: 0.00015219875785987824, Test Loss_l0: 0.06074000149965286\n",
      "\n",
      "\n",
      "Episode 401, \\Rt: 0.4763092269326683, \\Pt: 0.5, Train loss_l1: 0.24171000719070435, Test loss_l1: 0.11052192747592926, Train Loss_l0: 0.00021889724303036928, Test Loss_l0: 0.06328999996185303\n",
      "\n",
      "\n",
      "Episode 501, \\Rt: 0.4810379241516966, \\Pt: 0.5, Train loss_l1: 0.23513999581336975, Test loss_l1: 0.12081754952669144, Train Loss_l0: 0.0025726540479809046, Test Loss_l0: 0.06440000236034393\n",
      "\n",
      "\n",
      "Episode 601, \\Rt: 0.4908485856905158, \\Pt: 0.5, Train loss_l1: 0.22879000008106232, Test loss_l1: 0.12658807635307312, Train Loss_l0: 0.008017952553927898, Test Loss_l0: 0.06456000357866287\n",
      "\n",
      "\n",
      "Episode 701, \\Rt: 0.48787446504992865, \\Pt: 0.5, Train loss_l1: 0.22281000018119812, Test loss_l1: 0.13093920052051544, Train Loss_l0: 0.014362677931785583, Test Loss_l0: 0.0641700029373169\n",
      "\n",
      "\n",
      "Episode 801, \\Rt: 0.5131086142322098, \\Pt: 0.7836880799959133, Train loss_l1: 0.21785999834537506, Test loss_l1: 0.13272297382354736, Train Loss_l0: 0.02051587775349617, Test Loss_l0: 0.06341999769210815\n",
      "\n",
      "\n",
      "Episode 901, \\Rt: 0.5416204217536071, \\Pt: 0.7724727122211139, Train loss_l1: 0.213469997048378, Test loss_l1: 0.13173089921474457, Train Loss_l0: 0.022899065166711807, Test Loss_l0: 0.06162000074982643\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beggining to train! \n",
      " \n",
      "\n",
      "tau: 0.01, repetitions per optimization step (would be like epochs): 1\n",
      " \n",
      "**** optimizers ***\n",
      "optimizer_critic_guess: {'name': 'Adam', 'learning_rate': 0.001, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False} \n",
      "Optimizer_actor_l0: {'name': 'Adam', 'learning_rate': 0.0001, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
      "Optimizer_critic_l0: <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f94e05d01d0>\n",
      "\n",
      "\n",
      "\n",
      "*** BUFFER ***\n",
      "Buffer_size: 100000\n",
      " Batch_size for sampling: 500\n",
      "\n",
      "\n",
      "\n",
      " *** NOISE PARAMETERS *** \n",
      "epsilon-guess: 0.01\n",
      "epsilon_displacement_noise: 0.1\n",
      "starting time: 20200406-1840\n",
      "saving results in logs/ddpg_results/time-20200406-1840\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02470d1b651749759e5cb75cbefbd1ea"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, \\Rt: 0.0, \\Pt: 0.5, Train loss_l1: 0.24716000258922577, Test loss_l1: 0.09161005914211273, Train Loss_l0: 0.00038383121136575937, Test Loss_l0: 0.04757000133395195\n",
      "\n",
      "\n",
      "Episode 101, \\Rt: 0.45544554455445546, \\Pt: 0.5, Train loss_l1: 0.2442999929189682, Test loss_l1: 0.09544055163860321, Train Loss_l0: 0.0005426630959846079, Test Loss_l0: 0.0501599982380867\n",
      "\n",
      "\n",
      "Episode 201, \\Rt: 0.417910447761194, \\Pt: 0.40652006106741134, Train loss_l1: 0.24040000140666962, Test loss_l1: 0.11028531193733215, Train Loss_l0: 0.0005630968953482807, Test Loss_l0: 0.05349000170826912\n",
      "\n",
      "\n",
      "Episode 301, \\Rt: 0.40863787375415284, \\Pt: 0.5062032034118717, Train loss_l1: 0.23788000643253326, Test loss_l1: 0.13788443803787231, Train Loss_l0: 0.0007196592050604522, Test Loss_l0: 0.05781000107526779\n",
      "\n",
      "\n",
      "Episode 401, \\Rt: 0.4314214463840399, \\Pt: 0.608456965189683, Train loss_l1: 0.2363699972629547, Test loss_l1: 0.16316118836402893, Train Loss_l0: 0.0014543470460921526, Test Loss_l0: 0.06293000280857086\n",
      "\n",
      "\n",
      "Episode 501, \\Rt: 0.45708582834331335, \\Pt: 0.7177726363042038, Train loss_l1: 0.23492999374866486, Test loss_l1: 0.17499414086341858, Train Loss_l0: 0.003178367391228676, Test Loss_l0: 0.06589999794960022\n",
      "\n",
      "\n",
      "Episode 601, \\Rt: 0.5141430948419301, \\Pt: 0.7892041923903532, Train loss_l1: 0.23270000517368317, Test loss_l1: 0.1829470843076706, Train Loss_l0: 0.0055109369568526745, Test Loss_l0: 0.06689999997615814\n",
      "\n",
      "\n",
      "Episode 701, \\Rt: 0.5549215406562055, \\Pt: 0.8053069199117495, Train loss_l1: 0.22935999929904938, Test loss_l1: 0.18653982877731323, Train Loss_l0: 0.008417434059083462, Test Loss_l0: 0.06706999987363815\n",
      "\n",
      "\n",
      "Episode 801, \\Rt: 0.5842696629213483, \\Pt: 0.8089431694405198, Train loss_l1: 0.22606000304222107, Test loss_l1: 0.18694311380386353, Train Loss_l0: 0.011401091702282429, Test Loss_l0: 0.0675399973988533\n",
      "\n",
      "\n",
      "Episode 901, \\Rt: 0.6082130965593785, \\Pt: 0.8030670458941973, Train loss_l1: 0.22312000393867493, Test loss_l1: 0.1848354935646057, Train Loss_l0: 0.013545696623623371, Test Loss_l0: 0.06908000260591507\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n"
     ]
    }
   ],
   "source": [
    "for tau in [0.1, 0.01]:\n",
    "\n",
    "    ddpgKennedy(total_episodes=10**3, noise_displacement=0.1, tau=tau, \n",
    "                buffer_size=10**5, batch_size=500, lr_critic=0.001, lr_actor=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
